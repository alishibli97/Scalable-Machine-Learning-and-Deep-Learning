{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note to the TA:\n",
    "\n",
    "We implemented everything required for the assignment.\n",
    "\n",
    "Due to hardware limitation issues, we were unable to train and get results on more than 5000 data points, which even causes some trouble training since BERT is a very big network.\n",
    "\n",
    "Thus our results were affected a bit. We hope this doesn't affect our grade. We hope the grade of the assignment is per the work implemented and the resulting numbers of training.\n",
    "\n",
    "Thanks a lot for understanding :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Defining SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertModel, BertConfig, BertTokenizer\n",
    "from scipy import spatial\n",
    "\n",
    "\n",
    "class SBERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SBERT, self).__init__()\n",
    "        \n",
    "        # self.model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        configuration = BertConfig()\n",
    "        self.model = BertModel(configuration)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        self.pooling = nn.AvgPool1d(kernel_size=3,stride=1)\n",
    "        # 768 / 3 -> 256\n",
    "        self.linear = nn.Linear(in_features=2298, out_features=3) # 2298=(768-2)*3; 153 is the embedding dimension after pooling and stuff..\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, sent1, sent2=None, objective=\"embedding\"):\n",
    "        encoded_input1 = self.tokenizer(sent1, padding=True, truncation=True, return_tensors='pt')\n",
    "        output1 = self.model(**encoded_input1)\n",
    "        output1 = self.pooling(output1[\"pooler_output\"])\n",
    "        \n",
    "        if objective==\"embedding\":\n",
    "            return output1\n",
    "\n",
    "        encoded_input2 = self.tokenizer(sent2, padding=True, truncation=True, return_tensors='pt')\n",
    "        output2 = self.model(**encoded_input2)\n",
    "        output2 = self.pooling(output2[\"pooler_output\"])\n",
    "                        \n",
    "        if objective == \"regression\":\n",
    "            return torch.cosine_similarity(output1, output2)\n",
    "\n",
    "        if objective == \"classification\":\n",
    "            diff = abs(torch.subtract(output1,output2))\n",
    "            concat = torch.cat([output1,output2,diff],axis=1)            \n",
    "            result = self.linear(concat)\n",
    "            out = self.softmax(result)\n",
    "            return out\n",
    "\n",
    "sbert = SBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implement Regression Objective and Evaluate on STS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.6</td>\n",
       "      <td>A group of men play soccer on the beach.</td>\n",
       "      <td>A group of boys are playing soccer on the beach.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>One woman is measuring another woman's ankle.</td>\n",
       "      <td>A woman measures another woman's ankle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.2</td>\n",
       "      <td>A man is cutting up a cucumber.</td>\n",
       "      <td>A man is slicing a cucumber.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.5</td>\n",
       "      <td>A man is playing a harp.</td>\n",
       "      <td>A man is playing a keyboard.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.8</td>\n",
       "      <td>A woman is cutting onions.</td>\n",
       "      <td>A woman is cutting tofu.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                      sentence1  \\\n",
       "0    3.6       A group of men play soccer on the beach.   \n",
       "1    5.0  One woman is measuring another woman's ankle.   \n",
       "2    4.2                A man is cutting up a cucumber.   \n",
       "3    1.5                       A man is playing a harp.   \n",
       "4    1.8                     A woman is cutting onions.   \n",
       "\n",
       "                                          sentence2  \n",
       "0  A group of boys are playing soccer on the beach.  \n",
       "1           A woman measures another woman's ankle.  \n",
       "2                      A man is slicing a cucumber.  \n",
       "3                      A man is playing a keyboard.  \n",
       "4                          A woman is cutting tofu.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_csv(\"datasets/Stsbenchmark/sts-test.csv\",header=0,names=[\"main-caption\",\"genre\",\"filename\",\"year\",\"score\",\"sentence1\",\"sentence2\"])#,usecols=['score','sentence1','sentence2'])\n",
    "\n",
    "df_test = df_test[['score','sentence1','sentence2']]\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The min and max of the scores is: (0.0, 5.0)\n"
     ]
    }
   ],
   "source": [
    "ls = list(df_test.score)\n",
    "minn = min(ls)\n",
    "maxx = max(ls)\n",
    "print(\"The min and max of the scores is:\",(minn,maxx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_score(value, leftMin=0, leftMax=5, rightMin=-1, rightMax=1):\n",
    "    leftSpan = leftMax - leftMin\n",
    "    rightSpan = rightMax - rightMin\n",
    "    valueScaled = float(value - leftMin) / float(leftSpan)\n",
    "    return rightMin + (valueScaled * rightSpan)\n",
    "\n",
    "df_test['score'] = df_test['score'].apply(map_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After mapping the values the range of scores is: (-1.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "ls = list(df_test.score)\n",
    "minn = min(ls)\n",
    "maxx = max(ls)\n",
    "print(\"After mapping the values the range of scores is:\",(minn,maxx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nulls:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "score        0\n",
       "sentence1    0\n",
       "sentence2    5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The nulls:\")\n",
    "df_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.dropna(inplace=True)\n",
    "df_test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nulls after dropping:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "index        0\n",
       "score        0\n",
       "sentence1    0\n",
       "sentence2    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The nulls after dropping:\")\n",
    "df_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 974/975\r"
     ]
    }
   ],
   "source": [
    "sbert.eval()\n",
    "\n",
    "cosine_scores = []\n",
    "for i,row in df_test.iterrows():\n",
    "    print(f\"Finished {i}/{len(df_test)}\",end=\"\\r\")\n",
    "    score = sbert(row.sentence1,row.sentence2,\"regression\").detach().numpy()[0]\n",
    "    cosine_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.29374685036801934, pvalue=7.398166147771895e-21)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "spearmanr(df_test.score.values.tolist(),cosine_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implement Classification Objective and Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_list_train = list(open(\"datasets/snli_1.0/snli_1.0_train.jsonl\",\"r\"))\n",
    "json_list_val = list(open(\"datasets/snli_1.0/snli_1.0_dev.jsonl\",\"r\"))\n",
    "\n",
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 549367\n",
      "Testing data: 9842\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_train = {'sentence1': [], 'sentence2': [], 'gold_label': []}\n",
    "for json_str in json_list_train:\n",
    "    try:\n",
    "        result = json.loads(json_str)\n",
    "        result['gold_label']=label2int[result['gold_label']]\n",
    "        for key in data_train:\n",
    "            data_train[key].append(result[key])\n",
    "    except:\n",
    "        pass\n",
    "df_train = pd.DataFrame.from_dict(data_train)#.head()\n",
    "\n",
    "data_val = {'sentence1': [], 'sentence2': [], 'gold_label': []}\n",
    "for json_str in json_list_val:\n",
    "    try:\n",
    "        result = json.loads(json_str)\n",
    "        result['gold_label']=label2int[result['gold_label']]\n",
    "        for key in data_val:\n",
    "            data_val[key].append(result[key])\n",
    "    except:\n",
    "        pass\n",
    "df_val = pd.DataFrame.from_dict(data_val)#.head()\n",
    "\n",
    "print(\"Training data:\",len(df_train))\n",
    "print(\"Testing data:\",len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is training his horse for a competition.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is at a diner, ordering an omelette.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is outdoors, on a horse.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>They are smiling at their parents</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>There are children present</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  A person on a horse jumps over a broken down a...   \n",
       "1  A person on a horse jumps over a broken down a...   \n",
       "2  A person on a horse jumps over a broken down a...   \n",
       "3              Children smiling and waving at camera   \n",
       "4              Children smiling and waving at camera   \n",
       "\n",
       "                                           sentence2  gold_label  \n",
       "0  A person is training his horse for a competition.           2  \n",
       "1      A person is at a diner, ordering an omelette.           0  \n",
       "2                  A person is outdoors, on a horse.           1  \n",
       "3                  They are smiling at their parents           2  \n",
       "4                         There are children present           1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class SNLI_Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        sent1 = row['sentence1']\n",
    "        sent2 = row['sentence2']\n",
    "        label = row['gold_label']\n",
    "        return (sent1, sent2), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- outputs of shape (N,C) where N is the batch size C is the number of classes\n",
    "- target of shape (N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1/10 iteration=1/34336 train_loss=1.1059 took 0.7070 secs\n",
      "epoch=1/10 iteration=2/34336 train_loss=1.1709 took 0.4261 secs\n",
      "epoch=1/10 iteration=3/34336 train_loss=1.1881 took 0.3999 secs\n",
      "epoch=1/10 iteration=4/34336 train_loss=1.1084 took 0.4236 secs\n",
      "epoch=1/10 iteration=5/34336 train_loss=1.1597 took 0.6614 secs\n",
      "epoch=1/10 iteration=6/34336 train_loss=1.1511 took 0.7325 secs\n",
      "epoch=1/10 iteration=7/34336 train_loss=1.2218 took 0.5861 secs\n",
      "epoch=1/10 iteration=8/34336 train_loss=1.1032 took 0.3301 secs\n",
      "epoch=1/10 iteration=9/34336 train_loss=1.1626 took 0.4545 secs\n",
      "epoch=1/10 iteration=10/34336 train_loss=1.1426 took 0.3818 secs\n",
      "epoch=1/10 iteration=11/34336 train_loss=1.1036 took 0.3551 secs\n",
      "epoch=1/10 iteration=12/34336 train_loss=1.1771 took 0.3191 secs\n",
      "epoch=1/10 iteration=13/34336 train_loss=1.1839 took 0.3191 secs\n",
      "epoch=1/10 iteration=14/34336 train_loss=1.1054 took 0.4677 secs\n",
      "epoch=1/10 iteration=15/34336 train_loss=1.1008 took 0.4597 secs\n",
      "epoch=1/10 iteration=16/34336 train_loss=1.0938 took 0.4867 secs\n",
      "epoch=1/10 iteration=17/34336 train_loss=1.1163 took 0.5085 secs\n",
      "epoch=1/10 iteration=18/34336 train_loss=1.0948 took 0.7110 secs\n",
      "epoch=1/10 iteration=19/34336 train_loss=1.1251 took 0.3893 secs\n",
      "epoch=1/10 iteration=20/34336 train_loss=1.1237 took 0.4220 secs\n",
      "epoch=1/10 iteration=21/34336 train_loss=1.1063 took 0.6514 secs\n",
      "epoch=1/10 iteration=22/34336 train_loss=1.1172 took 0.6331 secs\n",
      "epoch=1/10 iteration=23/34336 train_loss=1.1047 took 0.5002 secs\n",
      "epoch=1/10 iteration=24/34336 train_loss=1.1029 took 0.3865 secs\n",
      "epoch=1/10 iteration=25/34336 train_loss=1.1065 took 0.3920 secs\n",
      "epoch=1/10 iteration=26/34336 train_loss=1.1068 took 0.4501 secs\n",
      "epoch=1/10 iteration=27/34336 train_loss=1.1154 took 0.4358 secs\n",
      "epoch=1/10 iteration=28/34336 train_loss=1.0941 took 0.6381 secs\n",
      "epoch=1/10 iteration=29/34336 train_loss=1.1040 took 0.4616 secs\n",
      "epoch=1/10 iteration=30/34336 train_loss=1.1006 took 0.5124 secs\n",
      "epoch=1/10 iteration=31/34336 train_loss=1.1027 took 0.4057 secs\n",
      "epoch=1/10 iteration=32/34336 train_loss=1.1053 took 0.4136 secs\n",
      "epoch=1/10 iteration=33/34336 train_loss=1.0924 took 0.3248 secs\n",
      "epoch=1/10 iteration=34/34336 train_loss=1.0997 took 0.4748 secs\n",
      "epoch=1/10 iteration=35/34336 train_loss=1.1010 took 0.3734 secs\n",
      "epoch=1/10 iteration=36/34336 train_loss=1.1055 took 0.4209 secs\n",
      "epoch=1/10 iteration=37/34336 train_loss=1.1101 took 0.6442 secs\n",
      "epoch=1/10 iteration=38/34336 train_loss=1.0979 took 0.3668 secs\n",
      "epoch=1/10 iteration=39/34336 train_loss=1.0990 took 0.3642 secs\n",
      "epoch=1/10 iteration=40/34336 train_loss=1.1113 took 0.5050 secs\n",
      "epoch=1/10 iteration=41/34336 train_loss=1.1032 took 0.3120 secs\n",
      "epoch=1/10 iteration=42/34336 train_loss=1.1036 took 0.4202 secs\n",
      "epoch=1/10 iteration=43/34336 train_loss=1.1005 took 0.5014 secs\n",
      "epoch=1/10 iteration=44/34336 train_loss=1.0983 took 0.4896 secs\n",
      "epoch=1/10 iteration=45/34336 train_loss=1.1009 took 0.3667 secs\n",
      "epoch=1/10 iteration=46/34336 train_loss=1.1072 took 0.4500 secs\n",
      "epoch=1/10 iteration=47/34336 train_loss=1.0990 took 0.4878 secs\n",
      "epoch=1/10 iteration=48/34336 train_loss=1.1043 took 0.4566 secs\n",
      "epoch=1/10 iteration=49/34336 train_loss=1.0981 took 0.3819 secs\n",
      "epoch=1/10 iteration=50/34336 train_loss=1.1015 took 0.3863 secs\n",
      "epoch=1/10 iteration=51/34336 train_loss=1.1025 took 0.3340 secs\n",
      "epoch=1/10 iteration=52/34336 train_loss=1.0991 took 0.3762 secs\n",
      "epoch=1/10 iteration=53/34336 train_loss=1.1000 took 0.3546 secs\n",
      "epoch=1/10 iteration=54/34336 train_loss=1.1082 took 0.6747 secs\n",
      "epoch=1/10 iteration=55/34336 train_loss=1.0993 took 0.3639 secs\n",
      "epoch=1/10 iteration=56/34336 train_loss=1.0905 took 0.3699 secs\n",
      "epoch=1/10 iteration=57/34336 train_loss=1.1058 took 0.4304 secs\n",
      "epoch=1/10 iteration=58/34336 train_loss=1.0945 took 0.4950 secs\n",
      "epoch=1/10 iteration=59/34336 train_loss=1.0968 took 0.4853 secs\n",
      "epoch=1/10 iteration=60/34336 train_loss=1.1027 took 0.4014 secs\n",
      "epoch=1/10 iteration=61/34336 train_loss=1.0972 took 0.4225 secs\n",
      "epoch=1/10 iteration=62/34336 train_loss=1.1053 took 0.5388 secs\n",
      "epoch=1/10 iteration=63/34336 train_loss=1.1085 took 0.4611 secs\n",
      "epoch=1/10 iteration=64/34336 train_loss=1.0944 took 0.3436 secs\n",
      "epoch=1/10 iteration=65/34336 train_loss=1.1159 took 0.4626 secs\n",
      "epoch=1/10 iteration=66/34336 train_loss=1.1014 took 0.4158 secs\n",
      "epoch=1/10 iteration=67/34336 train_loss=1.0933 took 0.4030 secs\n",
      "epoch=1/10 iteration=68/34336 train_loss=1.1066 took 0.5124 secs\n",
      "epoch=1/10 iteration=69/34336 train_loss=1.0908 took 0.5298 secs\n",
      "epoch=1/10 iteration=70/34336 train_loss=1.1038 took 0.4883 secs\n",
      "epoch=1/10 iteration=71/34336 train_loss=1.1097 took 0.5107 secs\n",
      "epoch=1/10 iteration=72/34336 train_loss=1.1045 took 0.4475 secs\n",
      "epoch=1/10 iteration=73/34336 train_loss=1.0949 took 0.4385 secs\n",
      "epoch=1/10 iteration=74/34336 train_loss=1.0886 took 0.5849 secs\n",
      "epoch=1/10 iteration=75/34336 train_loss=1.0991 took 0.4794 secs\n",
      "epoch=1/10 iteration=76/34336 train_loss=1.0986 took 0.6103 secs\n",
      "epoch=1/10 iteration=77/34336 train_loss=1.1042 took 0.3700 secs\n",
      "epoch=1/10 iteration=78/34336 train_loss=1.1067 took 0.4443 secs\n",
      "epoch=1/10 iteration=79/34336 train_loss=1.1054 took 0.6291 secs\n",
      "epoch=1/10 iteration=80/34336 train_loss=1.0968 took 0.4599 secs\n",
      "epoch=1/10 iteration=81/34336 train_loss=1.1045 took 0.4696 secs\n",
      "epoch=1/10 iteration=82/34336 train_loss=1.0912 took 0.4214 secs\n",
      "epoch=1/10 iteration=83/34336 train_loss=1.1119 took 0.3695 secs\n",
      "epoch=1/10 iteration=84/34336 train_loss=1.0995 took 0.4036 secs\n",
      "epoch=1/10 iteration=85/34336 train_loss=1.0984 took 0.3989 secs\n",
      "epoch=1/10 iteration=86/34336 train_loss=1.0922 took 0.4490 secs\n",
      "epoch=1/10 iteration=87/34336 train_loss=1.0952 took 0.4418 secs\n",
      "epoch=1/10 iteration=88/34336 train_loss=1.0997 took 0.3757 secs\n",
      "epoch=1/10 iteration=89/34336 train_loss=1.0954 took 0.3798 secs\n",
      "epoch=1/10 iteration=90/34336 train_loss=1.1003 took 0.4401 secs\n",
      "epoch=1/10 iteration=91/34336 train_loss=1.0931 took 0.5018 secs\n",
      "epoch=1/10 iteration=92/34336 train_loss=1.0906 took 0.3563 secs\n",
      "epoch=1/10 iteration=93/34336 train_loss=1.0947 took 0.3196 secs\n",
      "epoch=1/10 iteration=94/34336 train_loss=1.1033 took 0.3599 secs\n",
      "epoch=1/10 iteration=95/34336 train_loss=1.1160 took 0.4037 secs\n",
      "epoch=1/10 iteration=96/34336 train_loss=1.1000 took 0.4550 secs\n",
      "epoch=1/10 iteration=97/34336 train_loss=1.0998 took 0.5040 secs\n",
      "epoch=1/10 iteration=98/34336 train_loss=1.0979 took 0.3623 secs\n",
      "epoch=1/10 iteration=99/34336 train_loss=1.1038 took 0.4206 secs\n",
      "epoch=1/10 iteration=100/34336 train_loss=1.1016 took 0.4629 secs\n",
      "epoch=1/10 iteration=101/34336 train_loss=1.0977 took 0.4713 secs\n",
      "epoch=1/10 iteration=102/34336 train_loss=1.1036 took 0.4134 secs\n",
      "epoch=1/10 iteration=103/34336 train_loss=1.0957 took 0.3956 secs\n",
      "epoch=1/10 iteration=104/34336 train_loss=1.1114 took 0.4902 secs\n",
      "epoch=1/10 iteration=105/34336 train_loss=1.0993 took 0.4695 secs\n",
      "epoch=1/10 iteration=106/34336 train_loss=1.1048 took 0.4299 secs\n",
      "epoch=1/10 iteration=107/34336 train_loss=1.1028 took 0.4076 secs\n",
      "epoch=1/10 iteration=108/34336 train_loss=1.1123 took 0.4411 secs\n",
      "epoch=1/10 iteration=109/34336 train_loss=1.0881 took 0.4639 secs\n",
      "epoch=1/10 iteration=110/34336 train_loss=1.1073 took 0.4018 secs\n",
      "epoch=1/10 iteration=111/34336 train_loss=1.1112 took 0.5643 secs\n",
      "epoch=1/10 iteration=112/34336 train_loss=1.0874 took 0.4503 secs\n",
      "epoch=1/10 iteration=113/34336 train_loss=1.0989 took 0.3435 secs\n",
      "epoch=1/10 iteration=114/34336 train_loss=1.1064 took 0.5895 secs\n",
      "epoch=1/10 iteration=115/34336 train_loss=1.1050 took 0.5200 secs\n",
      "epoch=1/10 iteration=116/34336 train_loss=1.1037 took 0.4278 secs\n",
      "epoch=1/10 iteration=117/34336 train_loss=1.1020 took 0.3685 secs\n",
      "epoch=1/10 iteration=118/34336 train_loss=1.1135 took 0.4686 secs\n",
      "epoch=1/10 iteration=119/34336 train_loss=1.1049 took 0.5075 secs\n",
      "epoch=1/10 iteration=120/34336 train_loss=1.0938 took 0.3890 secs\n",
      "epoch=1/10 iteration=121/34336 train_loss=1.1019 took 0.4015 secs\n",
      "epoch=1/10 iteration=122/34336 train_loss=1.0926 took 0.3592 secs\n",
      "epoch=1/10 iteration=123/34336 train_loss=1.0897 took 0.3707 secs\n",
      "epoch=1/10 iteration=124/34336 train_loss=1.0992 took 0.3160 secs\n",
      "epoch=1/10 iteration=125/34336 train_loss=1.1052 took 0.4196 secs\n",
      "epoch=1/10 iteration=126/34336 train_loss=1.1022 took 0.3489 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1/10 iteration=127/34336 train_loss=1.0977 took 0.3690 secs\n",
      "epoch=1/10 iteration=128/34336 train_loss=1.1001 took 0.3332 secs\n",
      "epoch=1/10 iteration=129/34336 train_loss=1.1000 took 0.3713 secs\n",
      "epoch=1/10 iteration=130/34336 train_loss=1.1171 took 0.4331 secs\n",
      "epoch=1/10 iteration=131/34336 train_loss=1.1060 took 0.3523 secs\n",
      "epoch=1/10 iteration=132/34336 train_loss=1.0820 took 0.3680 secs\n",
      "epoch=1/10 iteration=133/34336 train_loss=1.1085 took 0.3665 secs\n",
      "epoch=1/10 iteration=134/34336 train_loss=1.0986 took 0.4781 secs\n",
      "epoch=1/10 iteration=135/34336 train_loss=1.1000 took 0.4283 secs\n",
      "epoch=1/10 iteration=136/34336 train_loss=1.1067 took 0.3226 secs\n",
      "epoch=1/10 iteration=137/34336 train_loss=1.1103 took 0.3258 secs\n",
      "epoch=1/10 iteration=138/34336 train_loss=1.0962 took 0.4284 secs\n",
      "epoch=1/10 iteration=139/34336 train_loss=1.1038 took 0.3075 secs\n",
      "epoch=1/10 iteration=140/34336 train_loss=1.0924 took 0.5742 secs\n",
      "epoch=1/10 iteration=141/34336 train_loss=1.0968 took 0.3720 secs\n",
      "epoch=1/10 iteration=142/34336 train_loss=1.1088 took 0.4559 secs\n",
      "epoch=1/10 iteration=143/34336 train_loss=1.1121 took 0.4446 secs\n",
      "epoch=1/10 iteration=144/34336 train_loss=1.1004 took 0.4191 secs\n",
      "epoch=1/10 iteration=145/34336 train_loss=1.0921 took 0.3738 secs\n",
      "epoch=1/10 iteration=146/34336 train_loss=1.0988 took 0.4490 secs\n",
      "epoch=1/10 iteration=147/34336 train_loss=1.1148 took 0.4731 secs\n",
      "epoch=1/10 iteration=148/34336 train_loss=1.0956 took 0.4257 secs\n",
      "epoch=1/10 iteration=149/34336 train_loss=1.0939 took 0.3619 secs\n",
      "epoch=1/10 iteration=150/34336 train_loss=1.1070 took 0.4864 secs\n",
      "epoch=1/10 iteration=151/34336 train_loss=1.1120 took 0.8314 secs\n",
      "epoch=1/10 iteration=152/34336 train_loss=1.0891 took 1.0219 secs\n",
      "epoch=1/10 iteration=153/34336 train_loss=1.1096 took 0.5619 secs\n",
      "epoch=1/10 iteration=154/34336 train_loss=1.0957 took 0.5976 secs\n",
      "epoch=1/10 iteration=155/34336 train_loss=1.1203 took 0.4299 secs\n",
      "epoch=1/10 iteration=156/34336 train_loss=1.0929 took 0.3459 secs\n",
      "epoch=1/10 iteration=157/34336 train_loss=1.0953 took 0.3814 secs\n",
      "epoch=1/10 iteration=158/34336 train_loss=1.1002 took 0.3871 secs\n",
      "epoch=1/10 iteration=159/34336 train_loss=1.1134 took 0.3645 secs\n",
      "epoch=1/10 iteration=160/34336 train_loss=1.0944 took 0.4631 secs\n",
      "epoch=1/10 iteration=161/34336 train_loss=1.1052 took 0.4739 secs\n",
      "epoch=1/10 iteration=162/34336 train_loss=1.0946 took 0.3923 secs\n",
      "epoch=1/10 iteration=163/34336 train_loss=1.0988 took 0.3150 secs\n",
      "epoch=1/10 iteration=164/34336 train_loss=1.1074 took 0.3969 secs\n",
      "epoch=1/10 iteration=165/34336 train_loss=1.1000 took 0.4340 secs\n",
      "epoch=1/10 iteration=166/34336 train_loss=1.1008 took 0.4090 secs\n",
      "epoch=1/10 iteration=167/34336 train_loss=1.0944 took 0.4698 secs\n",
      "epoch=1/10 iteration=168/34336 train_loss=1.1082 took 0.4224 secs\n",
      "epoch=1/10 iteration=169/34336 train_loss=1.0952 took 0.3698 secs\n",
      "epoch=1/10 iteration=170/34336 train_loss=1.1138 took 0.3590 secs\n",
      "epoch=1/10 iteration=171/34336 train_loss=1.0862 took 0.3599 secs\n",
      "epoch=1/10 iteration=172/34336 train_loss=1.1041 took 0.4292 secs\n",
      "epoch=1/10 iteration=173/34336 train_loss=1.0962 took 0.3958 secs\n",
      "epoch=1/10 iteration=174/34336 train_loss=1.0934 took 0.3984 secs\n",
      "epoch=1/10 iteration=175/34336 train_loss=1.1000 took 0.5009 secs\n",
      "epoch=1/10 iteration=176/34336 train_loss=1.1099 took 0.7023 secs\n",
      "epoch=1/10 iteration=177/34336 train_loss=1.0933 took 0.3957 secs\n",
      "epoch=1/10 iteration=178/34336 train_loss=1.0846 took 0.4439 secs\n",
      "epoch=1/10 iteration=179/34336 train_loss=1.1050 took 0.4772 secs\n",
      "epoch=1/10 iteration=180/34336 train_loss=1.0939 took 0.4112 secs\n",
      "epoch=1/10 iteration=181/34336 train_loss=1.1041 took 0.4534 secs\n",
      "epoch=1/10 iteration=182/34336 train_loss=1.1081 took 0.2977 secs\n",
      "epoch=1/10 iteration=183/34336 train_loss=1.1030 took 0.3391 secs\n",
      "epoch=1/10 iteration=184/34336 train_loss=1.1020 took 0.3900 secs\n",
      "epoch=1/10 iteration=185/34336 train_loss=1.1050 took 0.5116 secs\n",
      "epoch=1/10 iteration=186/34336 train_loss=1.0910 took 0.4526 secs\n",
      "epoch=1/10 iteration=187/34336 train_loss=1.1065 took 0.3490 secs\n",
      "epoch=1/10 iteration=188/34336 train_loss=1.1072 took 0.3207 secs\n",
      "epoch=1/10 iteration=189/34336 train_loss=1.1047 took 0.7703 secs\n",
      "epoch=1/10 iteration=190/34336 train_loss=1.1049 took 0.3632 secs\n",
      "epoch=1/10 iteration=191/34336 train_loss=1.1032 took 0.4395 secs\n",
      "epoch=1/10 iteration=192/34336 train_loss=1.0998 took 0.3319 secs\n",
      "epoch=1/10 iteration=193/34336 train_loss=1.1026 took 0.3687 secs\n",
      "epoch=1/10 iteration=194/34336 train_loss=1.0998 took 0.5562 secs\n",
      "epoch=1/10 iteration=195/34336 train_loss=1.1010 took 0.4586 secs\n",
      "epoch=1/10 iteration=196/34336 train_loss=1.1006 took 0.3886 secs\n",
      "epoch=1/10 iteration=197/34336 train_loss=1.0947 took 0.4005 secs\n",
      "epoch=1/10 iteration=198/34336 train_loss=1.0961 took 0.3665 secs\n",
      "epoch=1/10 iteration=199/34336 train_loss=1.1053 took 0.5345 secs\n",
      "epoch=1/10 iteration=200/34336 train_loss=1.0978 took 0.3852 secs\n",
      "epoch=1/10 iteration=201/34336 train_loss=1.1022 took 0.4984 secs\n",
      "epoch=1/10 iteration=202/34336 train_loss=1.0953 took 0.4563 secs\n",
      "epoch=1/10 iteration=203/34336 train_loss=1.0933 took 0.3481 secs\n",
      "epoch=1/10 iteration=204/34336 train_loss=1.1018 took 0.3470 secs\n",
      "epoch=1/10 iteration=205/34336 train_loss=1.0992 took 0.3983 secs\n",
      "epoch=1/10 iteration=206/34336 train_loss=1.1008 took 0.5261 secs\n",
      "epoch=1/10 iteration=207/34336 train_loss=1.1128 took 0.4260 secs\n",
      "epoch=1/10 iteration=208/34336 train_loss=1.0930 took 0.5135 secs\n",
      "epoch=1/10 iteration=209/34336 train_loss=1.1007 took 0.5676 secs\n",
      "epoch=1/10 iteration=210/34336 train_loss=1.1023 took 0.4180 secs\n",
      "epoch=1/10 iteration=211/34336 train_loss=1.0989 took 0.4150 secs\n",
      "epoch=1/10 iteration=212/34336 train_loss=1.0977 took 0.4599 secs\n",
      "epoch=1/10 iteration=213/34336 train_loss=1.1075 took 0.3030 secs\n",
      "epoch=1/10 iteration=214/34336 train_loss=1.0953 took 0.4299 secs\n",
      "epoch=1/10 iteration=215/34336 train_loss=1.0957 took 0.3507 secs\n",
      "epoch=1/10 iteration=216/34336 train_loss=1.1287 took 0.3557 secs\n",
      "epoch=1/10 iteration=217/34336 train_loss=1.0866 took 0.2658 secs\n",
      "epoch=1/10 iteration=218/34336 train_loss=1.1046 took 0.3103 secs\n",
      "epoch=1/10 iteration=219/34336 train_loss=1.0922 took 0.3199 secs\n",
      "epoch=1/10 iteration=220/34336 train_loss=1.0989 took 0.4503 secs\n",
      "epoch=1/10 iteration=221/34336 train_loss=1.0984 took 0.3124 secs\n",
      "epoch=1/10 iteration=222/34336 train_loss=1.1026 took 0.3042 secs\n",
      "epoch=1/10 iteration=223/34336 train_loss=1.0947 took 0.3735 secs\n",
      "epoch=1/10 iteration=224/34336 train_loss=1.1020 took 0.3546 secs\n",
      "epoch=1/10 iteration=225/34336 train_loss=1.0973 took 0.4304 secs\n",
      "epoch=1/10 iteration=226/34336 train_loss=1.0913 took 0.3574 secs\n",
      "epoch=1/10 iteration=227/34336 train_loss=1.1158 took 0.3495 secs\n",
      "epoch=1/10 iteration=228/34336 train_loss=1.1026 took 0.4499 secs\n",
      "epoch=1/10 iteration=229/34336 train_loss=1.1040 took 0.4554 secs\n",
      "epoch=1/10 iteration=230/34336 train_loss=1.0993 took 0.3436 secs\n",
      "epoch=1/10 iteration=231/34336 train_loss=1.1014 took 0.3038 secs\n",
      "epoch=1/10 iteration=232/34336 train_loss=1.0984 took 0.3752 secs\n",
      "epoch=1/10 iteration=233/34336 train_loss=1.1080 took 0.3630 secs\n",
      "epoch=1/10 iteration=234/34336 train_loss=1.0993 took 0.3805 secs\n",
      "epoch=1/10 iteration=235/34336 train_loss=1.0998 took 0.3895 secs\n",
      "epoch=1/10 iteration=236/34336 train_loss=1.0978 took 0.3260 secs\n",
      "epoch=1/10 iteration=237/34336 train_loss=1.0960 took 0.5323 secs\n",
      "epoch=1/10 iteration=238/34336 train_loss=1.0982 took 0.4673 secs\n",
      "epoch=1/10 iteration=239/34336 train_loss=1.0866 took 0.4086 secs\n",
      "epoch=1/10 iteration=240/34336 train_loss=1.1131 took 0.3623 secs\n",
      "epoch=1/10 iteration=241/34336 train_loss=1.0943 took 0.3509 secs\n",
      "epoch=1/10 iteration=242/34336 train_loss=1.1069 took 0.3771 secs\n",
      "epoch=1/10 iteration=243/34336 train_loss=1.0951 took 0.3883 secs\n",
      "epoch=1/10 iteration=244/34336 train_loss=1.1041 took 0.3703 secs\n",
      "epoch=1/10 iteration=245/34336 train_loss=1.0957 took 0.5340 secs\n",
      "epoch=1/10 iteration=246/34336 train_loss=1.1023 took 0.3159 secs\n",
      "epoch=1/10 iteration=247/34336 train_loss=1.0979 took 0.3706 secs\n",
      "epoch=1/10 iteration=248/34336 train_loss=1.0918 took 0.5184 secs\n",
      "epoch=1/10 iteration=249/34336 train_loss=1.1046 took 0.3661 secs\n",
      "epoch=1/10 iteration=250/34336 train_loss=1.1029 took 0.3499 secs\n",
      "epoch=1/10 iteration=251/34336 train_loss=1.0984 took 0.3130 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1/10 iteration=252/34336 train_loss=1.0936 took 0.3439 secs\n",
      "epoch=1/10 iteration=253/34336 train_loss=1.1012 took 0.3577 secs\n",
      "epoch=1/10 iteration=254/34336 train_loss=1.0926 took 0.3493 secs\n",
      "epoch=1/10 iteration=255/34336 train_loss=1.1052 took 0.4273 secs\n",
      "epoch=1/10 iteration=256/34336 train_loss=1.1002 took 0.4254 secs\n",
      "epoch=1/10 iteration=257/34336 train_loss=1.1050 took 0.5002 secs\n",
      "epoch=1/10 iteration=258/34336 train_loss=1.0996 took 0.5125 secs\n",
      "epoch=1/10 iteration=259/34336 train_loss=1.0950 took 0.4493 secs\n",
      "epoch=1/10 iteration=260/34336 train_loss=1.1405 took 0.4415 secs\n",
      "epoch=1/10 iteration=261/34336 train_loss=1.0980 took 0.4050 secs\n",
      "epoch=1/10 iteration=262/34336 train_loss=1.0996 took 0.4618 secs\n",
      "epoch=1/10 iteration=263/34336 train_loss=1.1056 took 0.4704 secs\n",
      "epoch=1/10 iteration=264/34336 train_loss=1.0934 took 0.3020 secs\n",
      "epoch=1/10 iteration=265/34336 train_loss=1.0994 took 0.4040 secs\n",
      "epoch=1/10 iteration=266/34336 train_loss=1.0997 took 0.3608 secs\n",
      "epoch=1/10 iteration=267/34336 train_loss=1.1128 took 0.4224 secs\n",
      "epoch=1/10 iteration=268/34336 train_loss=1.0976 took 0.3924 secs\n",
      "epoch=1/10 iteration=269/34336 train_loss=1.0966 took 0.3681 secs\n",
      "epoch=1/10 iteration=270/34336 train_loss=1.1062 took 0.3367 secs\n",
      "epoch=1/10 iteration=271/34336 train_loss=1.1017 took 0.3535 secs\n",
      "epoch=1/10 iteration=272/34336 train_loss=1.0965 took 0.4700 secs\n",
      "epoch=1/10 iteration=273/34336 train_loss=1.0992 took 0.3503 secs\n",
      "epoch=1/10 iteration=274/34336 train_loss=1.0864 took 0.4993 secs\n",
      "epoch=1/10 iteration=275/34336 train_loss=1.1041 took 0.4562 secs\n",
      "epoch=1/10 iteration=276/34336 train_loss=1.1073 took 0.4206 secs\n",
      "epoch=1/10 iteration=277/34336 train_loss=1.0925 took 0.3904 secs\n",
      "epoch=1/10 iteration=278/34336 train_loss=1.0967 took 0.3874 secs\n",
      "epoch=1/10 iteration=279/34336 train_loss=1.0988 took 0.3829 secs\n",
      "epoch=1/10 iteration=280/34336 train_loss=1.1008 took 0.3378 secs\n",
      "epoch=1/10 iteration=281/34336 train_loss=1.0979 took 0.4060 secs\n",
      "epoch=1/10 iteration=282/34336 train_loss=1.1077 took 0.3828 secs\n",
      "epoch=1/10 iteration=283/34336 train_loss=1.1054 took 0.4190 secs\n",
      "epoch=1/10 iteration=284/34336 train_loss=1.0980 took 0.5008 secs\n",
      "epoch=1/10 iteration=285/34336 train_loss=1.0951 took 0.4260 secs\n",
      "epoch=1/10 iteration=286/34336 train_loss=1.0916 took 0.3594 secs\n",
      "epoch=1/10 iteration=287/34336 train_loss=1.1120 took 0.5055 secs\n",
      "epoch=1/10 iteration=288/34336 train_loss=1.0909 took 0.3568 secs\n",
      "epoch=1/10 iteration=289/34336 train_loss=1.1043 took 0.3338 secs\n",
      "epoch=1/10 iteration=290/34336 train_loss=1.0959 took 0.3740 secs\n",
      "epoch=1/10 iteration=291/34336 train_loss=1.1007 took 0.3683 secs\n",
      "epoch=1/10 iteration=292/34336 train_loss=1.0963 took 0.3682 secs\n",
      "epoch=1/10 iteration=293/34336 train_loss=1.1103 took 0.3129 secs\n",
      "epoch=1/10 iteration=294/34336 train_loss=1.0918 took 0.3645 secs\n",
      "epoch=1/10 iteration=295/34336 train_loss=1.1074 took 0.3697 secs\n",
      "epoch=1/10 iteration=296/34336 train_loss=1.0977 took 0.3803 secs\n",
      "epoch=1/10 iteration=297/34336 train_loss=1.1016 took 0.3671 secs\n",
      "epoch=1/10 iteration=298/34336 train_loss=1.0981 took 0.4237 secs\n",
      "epoch=1/10 iteration=299/34336 train_loss=1.1030 took 0.3897 secs\n",
      "epoch=1/10 iteration=300/34336 train_loss=1.0983 took 0.3843 secs\n",
      "epoch=1/10 iteration=301/34336 train_loss=1.0955 took 0.3971 secs\n",
      "epoch=1/10 iteration=302/34336 train_loss=1.1045 took 0.3267 secs\n",
      "epoch=1/10 iteration=303/34336 train_loss=1.0947 took 0.4018 secs\n",
      "epoch=1/10 iteration=304/34336 train_loss=1.0978 took 0.3368 secs\n",
      "epoch=1/10 iteration=305/34336 train_loss=1.1009 took 0.4548 secs\n",
      "epoch=1/10 iteration=306/34336 train_loss=1.1002 took 0.3660 secs\n",
      "epoch=1/10 iteration=307/34336 train_loss=1.0977 took 0.3753 secs\n",
      "epoch=1/10 iteration=308/34336 train_loss=1.0938 took 0.3875 secs\n",
      "epoch=1/10 iteration=309/34336 train_loss=1.1001 took 0.3664 secs\n",
      "epoch=1/10 iteration=310/34336 train_loss=1.0957 took 0.3607 secs\n",
      "epoch=1/10 iteration=311/34336 train_loss=1.1071 took 0.3198 secs\n",
      "epoch=1/10 iteration=312/34336 train_loss=1.1007 took 0.3729 secs\n",
      "epoch=1/10 iteration=313/34336 train_loss=1.0949 took 0.3756 secs\n",
      "epoch=1/10 iteration=314/34336 train_loss=1.1008 took 0.4537 secs\n",
      "epoch=1/10 iteration=315/34336 train_loss=1.0997 took 0.4560 secs\n",
      "epoch=1/10 iteration=316/34336 train_loss=1.0973 took 0.4527 secs\n",
      "epoch=1/10 iteration=317/34336 train_loss=1.1096 took 0.4195 secs\n",
      "epoch=1/10 iteration=318/34336 train_loss=1.0912 took 0.4163 secs\n",
      "epoch=1/10 iteration=319/34336 train_loss=1.1359 took 0.3643 secs\n",
      "epoch=1/10 iteration=320/34336 train_loss=1.0955 took 0.3380 secs\n",
      "epoch=1/10 iteration=321/34336 train_loss=1.0996 took 0.3491 secs\n",
      "epoch=1/10 iteration=322/34336 train_loss=1.1046 took 0.3673 secs\n",
      "epoch=1/10 iteration=323/34336 train_loss=1.0993 took 0.2845 secs\n",
      "epoch=1/10 iteration=324/34336 train_loss=1.0964 took 0.4458 secs\n",
      "epoch=1/10 iteration=325/34336 train_loss=1.1039 took 0.3738 secs\n",
      "epoch=1/10 iteration=326/34336 train_loss=1.1010 took 0.4134 secs\n",
      "epoch=1/10 iteration=327/34336 train_loss=1.0997 took 0.4640 secs\n",
      "epoch=1/10 iteration=328/34336 train_loss=1.0924 took 0.4937 secs\n",
      "epoch=1/10 iteration=329/34336 train_loss=1.0990 took 0.3923 secs\n",
      "epoch=1/10 iteration=330/34336 train_loss=1.1056 took 0.4370 secs\n",
      "epoch=1/10 iteration=331/34336 train_loss=1.0965 took 0.3895 secs\n",
      "epoch=1/10 iteration=332/34336 train_loss=1.1022 took 0.5347 secs\n",
      "epoch=1/10 iteration=333/34336 train_loss=1.1195 took 0.4322 secs\n",
      "epoch=1/10 iteration=334/34336 train_loss=1.0993 took 0.3765 secs\n",
      "epoch=1/10 iteration=335/34336 train_loss=1.1035 took 0.7427 secs\n",
      "epoch=1/10 iteration=336/34336 train_loss=1.1000 took 0.3269 secs\n",
      "epoch=1/10 iteration=337/34336 train_loss=1.1034 took 0.4051 secs\n",
      "epoch=1/10 iteration=338/34336 train_loss=1.0927 took 0.5662 secs\n",
      "epoch=1/10 iteration=339/34336 train_loss=1.1293 took 0.7235 secs\n",
      "epoch=1/10 iteration=340/34336 train_loss=1.1029 took 0.7111 secs\n",
      "epoch=1/10 iteration=341/34336 train_loss=1.1031 took 0.4998 secs\n",
      "epoch=1/10 iteration=342/34336 train_loss=1.0996 took 0.4171 secs\n",
      "epoch=1/10 iteration=343/34336 train_loss=1.0979 took 0.4213 secs\n",
      "epoch=1/10 iteration=344/34336 train_loss=1.0959 took 0.4932 secs\n",
      "epoch=1/10 iteration=345/34336 train_loss=1.1061 took 0.3927 secs\n",
      "epoch=1/10 iteration=346/34336 train_loss=1.1016 took 0.3749 secs\n",
      "epoch=1/10 iteration=347/34336 train_loss=1.0960 took 0.3126 secs\n",
      "epoch=1/10 iteration=348/34336 train_loss=1.0923 took 0.3156 secs\n",
      "epoch=1/10 iteration=349/34336 train_loss=1.0986 took 0.3321 secs\n",
      "epoch=1/10 iteration=350/34336 train_loss=1.1043 took 0.5057 secs\n",
      "epoch=1/10 iteration=351/34336 train_loss=1.0923 took 0.3792 secs\n",
      "epoch=1/10 iteration=352/34336 train_loss=1.1068 took 0.3320 secs\n",
      "epoch=1/10 iteration=353/34336 train_loss=1.0857 took 0.3433 secs\n",
      "epoch=1/10 iteration=354/34336 train_loss=1.1144 took 0.5717 secs\n",
      "epoch=1/10 iteration=355/34336 train_loss=1.1091 took 0.4482 secs\n",
      "epoch=1/10 iteration=356/34336 train_loss=1.0883 took 0.4316 secs\n",
      "epoch=1/10 iteration=357/34336 train_loss=1.1136 took 0.4911 secs\n",
      "epoch=1/10 iteration=358/34336 train_loss=1.0997 took 0.6598 secs\n",
      "epoch=1/10 iteration=359/34336 train_loss=1.1048 took 0.5163 secs\n",
      "epoch=1/10 iteration=360/34336 train_loss=1.0966 took 0.5501 secs\n",
      "epoch=1/10 iteration=361/34336 train_loss=1.0977 took 0.6178 secs\n",
      "epoch=1/10 iteration=362/34336 train_loss=1.0972 took 0.6692 secs\n",
      "epoch=1/10 iteration=363/34336 train_loss=1.0954 took 0.4525 secs\n",
      "epoch=1/10 iteration=364/34336 train_loss=1.1147 took 0.8454 secs\n",
      "epoch=1/10 iteration=365/34336 train_loss=1.0980 took 0.5202 secs\n",
      "epoch=1/10 iteration=366/34336 train_loss=1.1066 took 0.4197 secs\n",
      "epoch=1/10 iteration=367/34336 train_loss=1.1159 took 0.4018 secs\n",
      "epoch=1/10 iteration=368/34336 train_loss=1.1117 took 0.3405 secs\n",
      "epoch=1/10 iteration=369/34336 train_loss=1.0970 took 0.2932 secs\n",
      "epoch=1/10 iteration=370/34336 train_loss=1.0979 took 0.3242 secs\n",
      "epoch=1/10 iteration=371/34336 train_loss=1.0988 took 0.4353 secs\n",
      "epoch=1/10 iteration=372/34336 train_loss=1.0986 took 0.4344 secs\n",
      "epoch=1/10 iteration=373/34336 train_loss=1.1012 took 0.3885 secs\n",
      "epoch=1/10 iteration=374/34336 train_loss=1.0940 took 0.4305 secs\n",
      "epoch=1/10 iteration=375/34336 train_loss=1.1131 took 0.3557 secs\n",
      "epoch=1/10 iteration=376/34336 train_loss=1.1002 took 0.3867 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1/10 iteration=377/34336 train_loss=1.1098 took 0.4717 secs\n",
      "epoch=1/10 iteration=378/34336 train_loss=1.0992 took 0.3339 secs\n",
      "epoch=1/10 iteration=379/34336 train_loss=1.1069 took 0.4697 secs\n",
      "epoch=1/10 iteration=380/34336 train_loss=1.0999 took 0.3949 secs\n",
      "epoch=1/10 iteration=381/34336 train_loss=1.0997 took 0.3452 secs\n",
      "epoch=1/10 iteration=382/34336 train_loss=1.0939 took 0.3781 secs\n",
      "epoch=1/10 iteration=383/34336 train_loss=1.1031 took 0.3377 secs\n",
      "epoch=1/10 iteration=384/34336 train_loss=1.1020 took 0.3799 secs\n",
      "epoch=1/10 iteration=385/34336 train_loss=1.1027 took 0.2858 secs\n",
      "epoch=1/10 iteration=386/34336 train_loss=1.1038 took 0.3774 secs\n",
      "epoch=1/10 iteration=387/34336 train_loss=1.1078 took 0.3635 secs\n",
      "epoch=1/10 iteration=388/34336 train_loss=1.0969 took 0.3824 secs\n",
      "epoch=1/10 iteration=389/34336 train_loss=1.1022 took 0.4305 secs\n",
      "epoch=1/10 iteration=390/34336 train_loss=1.1000 took 0.5288 secs\n",
      "epoch=1/10 iteration=391/34336 train_loss=1.1014 took 0.4507 secs\n",
      "epoch=1/10 iteration=392/34336 train_loss=1.1030 took 0.4375 secs\n",
      "epoch=1/10 iteration=393/34336 train_loss=1.0964 took 0.5660 secs\n",
      "epoch=1/10 iteration=394/34336 train_loss=1.1043 took 0.8007 secs\n",
      "epoch=1/10 iteration=395/34336 train_loss=1.1106 took 0.4205 secs\n",
      "epoch=1/10 iteration=396/34336 train_loss=1.0959 took 0.4303 secs\n",
      "epoch=1/10 iteration=397/34336 train_loss=1.1019 took 0.4737 secs\n",
      "epoch=1/10 iteration=398/34336 train_loss=1.0968 took 0.4551 secs\n",
      "epoch=1/10 iteration=399/34336 train_loss=1.0966 took 0.4295 secs\n",
      "epoch=1/10 iteration=400/34336 train_loss=1.1031 took 0.3727 secs\n",
      "epoch=1/10 iteration=401/34336 train_loss=1.0941 took 0.4060 secs\n",
      "epoch=1/10 iteration=402/34336 train_loss=1.1108 took 0.3301 secs\n",
      "epoch=1/10 iteration=403/34336 train_loss=1.0981 took 0.4253 secs\n",
      "epoch=1/10 iteration=404/34336 train_loss=1.0985 took 0.3493 secs\n",
      "epoch=1/10 iteration=405/34336 train_loss=1.1029 took 0.5357 secs\n",
      "epoch=1/10 iteration=406/34336 train_loss=1.0998 took 0.5299 secs\n",
      "epoch=1/10 iteration=407/34336 train_loss=1.1013 took 0.4156 secs\n",
      "epoch=1/10 iteration=408/34336 train_loss=1.0955 took 0.4299 secs\n",
      "epoch=1/10 iteration=409/34336 train_loss=1.1011 took 0.4389 secs\n",
      "epoch=1/10 iteration=410/34336 train_loss=1.1047 took 0.3640 secs\n",
      "epoch=1/10 iteration=411/34336 train_loss=1.1030 took 0.5361 secs\n",
      "epoch=1/10 iteration=412/34336 train_loss=1.1029 took 0.3663 secs\n",
      "epoch=1/10 iteration=413/34336 train_loss=1.1000 took 0.4458 secs\n",
      "epoch=1/10 iteration=414/34336 train_loss=1.1007 took 0.3741 secs\n",
      "epoch=1/10 iteration=415/34336 train_loss=1.1076 took 0.3493 secs\n",
      "epoch=1/10 iteration=416/34336 train_loss=1.1000 took 0.6370 secs\n",
      "epoch=1/10 iteration=417/34336 train_loss=1.0959 took 0.4293 secs\n",
      "epoch=1/10 iteration=418/34336 train_loss=1.1003 took 0.3280 secs\n",
      "epoch=1/10 iteration=419/34336 train_loss=1.1003 took 0.3254 secs\n",
      "epoch=1/10 iteration=420/34336 train_loss=1.1064 took 0.7562 secs\n",
      "epoch=1/10 iteration=421/34336 train_loss=1.1014 took 0.3258 secs\n",
      "epoch=1/10 iteration=422/34336 train_loss=1.1046 took 0.3771 secs\n",
      "epoch=1/10 iteration=423/34336 train_loss=1.1018 took 0.5114 secs\n",
      "epoch=1/10 iteration=424/34336 train_loss=1.0997 took 0.4093 secs\n",
      "epoch=1/10 iteration=425/34336 train_loss=1.0991 took 0.2785 secs\n",
      "epoch=1/10 iteration=426/34336 train_loss=1.1028 took 0.3252 secs\n",
      "epoch=1/10 iteration=427/34336 train_loss=1.0990 took 0.3787 secs\n",
      "epoch=1/10 iteration=428/34336 train_loss=1.0962 took 0.4409 secs\n",
      "epoch=1/10 iteration=429/34336 train_loss=1.1001 took 0.3220 secs\n",
      "epoch=1/10 iteration=430/34336 train_loss=1.1024 took 0.4457 secs\n",
      "epoch=1/10 iteration=431/34336 train_loss=1.0978 took 0.3431 secs\n",
      "epoch=1/10 iteration=432/34336 train_loss=1.0989 took 0.4972 secs\n",
      "epoch=1/10 iteration=433/34336 train_loss=1.0993 took 0.5310 secs\n",
      "epoch=1/10 iteration=434/34336 train_loss=1.1031 took 0.6418 secs\n",
      "epoch=1/10 iteration=435/34336 train_loss=1.0982 took 0.4264 secs\n",
      "epoch=1/10 iteration=436/34336 train_loss=1.0986 took 0.4585 secs\n",
      "epoch=1/10 iteration=437/34336 train_loss=1.1190 took 0.6567 secs\n",
      "epoch=1/10 iteration=438/34336 train_loss=1.1036 took 0.7256 secs\n",
      "epoch=1/10 iteration=439/34336 train_loss=1.0967 took 0.5054 secs\n",
      "epoch=1/10 iteration=440/34336 train_loss=1.1032 took 0.5009 secs\n",
      "epoch=1/10 iteration=441/34336 train_loss=1.0995 took 0.6535 secs\n",
      "epoch=1/10 iteration=442/34336 train_loss=1.0978 took 0.5762 secs\n",
      "epoch=1/10 iteration=443/34336 train_loss=1.1041 took 0.3965 secs\n",
      "epoch=1/10 iteration=444/34336 train_loss=1.0972 took 0.4002 secs\n",
      "epoch=1/10 iteration=445/34336 train_loss=1.1030 took 0.3830 secs\n",
      "epoch=1/10 iteration=446/34336 train_loss=1.0970 took 0.3815 secs\n",
      "epoch=1/10 iteration=447/34336 train_loss=1.0992 took 0.4526 secs\n",
      "epoch=1/10 iteration=448/34336 train_loss=1.1071 took 0.4418 secs\n",
      "epoch=1/10 iteration=449/34336 train_loss=1.1008 took 0.4069 secs\n",
      "epoch=1/10 iteration=450/34336 train_loss=1.1060 took 0.3892 secs\n",
      "epoch=1/10 iteration=451/34336 train_loss=1.0990 took 0.4351 secs\n",
      "epoch=1/10 iteration=452/34336 train_loss=1.0976 took 0.4161 secs\n",
      "epoch=1/10 iteration=453/34336 train_loss=1.1074 took 0.4988 secs\n",
      "epoch=1/10 iteration=454/34336 train_loss=1.0987 took 0.3899 secs\n",
      "epoch=1/10 iteration=455/34336 train_loss=1.0963 took 0.2858 secs\n",
      "epoch=1/10 iteration=456/34336 train_loss=1.1074 took 0.4572 secs\n",
      "epoch=1/10 iteration=457/34336 train_loss=1.1024 took 0.3466 secs\n",
      "epoch=1/10 iteration=458/34336 train_loss=1.0998 took 0.5279 secs\n",
      "epoch=1/10 iteration=459/34336 train_loss=1.1132 took 0.4840 secs\n",
      "epoch=1/10 iteration=460/34336 train_loss=1.0961 took 0.7385 secs\n",
      "epoch=1/10 iteration=461/34336 train_loss=1.1038 took 0.4187 secs\n",
      "epoch=1/10 iteration=462/34336 train_loss=1.0964 took 0.3231 secs\n",
      "epoch=1/10 iteration=463/34336 train_loss=1.0946 took 0.4429 secs\n",
      "epoch=1/10 iteration=464/34336 train_loss=1.1035 took 0.8130 secs\n",
      "epoch=1/10 iteration=465/34336 train_loss=1.1034 took 0.5787 secs\n",
      "epoch=1/10 iteration=466/34336 train_loss=1.0995 took 0.4562 secs\n",
      "epoch=1/10 iteration=467/34336 train_loss=1.0983 took 0.2873 secs\n",
      "epoch=1/10 iteration=468/34336 train_loss=1.0988 took 0.5454 secs\n",
      "epoch=1/10 iteration=469/34336 train_loss=1.1016 took 0.5204 secs\n",
      "epoch=1/10 iteration=470/34336 train_loss=1.0958 took 0.3731 secs\n",
      "epoch=1/10 iteration=471/34336 train_loss=1.1001 took 0.3458 secs\n",
      "epoch=1/10 iteration=472/34336 train_loss=1.0966 took 0.4669 secs\n",
      "epoch=1/10 iteration=473/34336 train_loss=1.0971 took 0.3302 secs\n",
      "epoch=1/10 iteration=474/34336 train_loss=1.1000 took 0.3960 secs\n",
      "epoch=1/10 iteration=475/34336 train_loss=1.1119 took 0.3650 secs\n",
      "epoch=1/10 iteration=476/34336 train_loss=1.0960 took 0.4360 secs\n",
      "epoch=1/10 iteration=477/34336 train_loss=1.0974 took 0.4585 secs\n",
      "epoch=1/10 iteration=478/34336 train_loss=1.1032 took 0.3417 secs\n",
      "epoch=1/10 iteration=479/34336 train_loss=1.1075 took 0.3194 secs\n",
      "epoch=1/10 iteration=480/34336 train_loss=1.1040 took 0.5890 secs\n",
      "epoch=1/10 iteration=481/34336 train_loss=1.0994 took 0.4850 secs\n",
      "epoch=1/10 iteration=482/34336 train_loss=1.0980 took 0.6583 secs\n",
      "epoch=1/10 iteration=483/34336 train_loss=1.0998 took 0.3350 secs\n",
      "epoch=1/10 iteration=484/34336 train_loss=1.1072 took 0.3400 secs\n",
      "epoch=1/10 iteration=485/34336 train_loss=1.1087 took 0.3263 secs\n",
      "epoch=1/10 iteration=486/34336 train_loss=1.0953 took 0.4727 secs\n",
      "epoch=1/10 iteration=487/34336 train_loss=1.0953 took 0.3884 secs\n",
      "epoch=1/10 iteration=488/34336 train_loss=1.0985 took 0.4280 secs\n",
      "epoch=1/10 iteration=489/34336 train_loss=1.0981 took 0.4472 secs\n",
      "epoch=1/10 iteration=490/34336 train_loss=1.1006 took 0.3597 secs\n",
      "epoch=1/10 iteration=491/34336 train_loss=1.1001 took 0.3749 secs\n",
      "epoch=1/10 iteration=492/34336 train_loss=1.0986 took 0.3339 secs\n",
      "epoch=1/10 iteration=493/34336 train_loss=1.1202 took 0.4421 secs\n",
      "epoch=1/10 iteration=494/34336 train_loss=1.0977 took 0.4115 secs\n",
      "epoch=1/10 iteration=495/34336 train_loss=1.0970 took 0.3340 secs\n",
      "epoch=1/10 iteration=496/34336 train_loss=1.1057 took 0.4078 secs\n",
      "epoch=1/10 iteration=497/34336 train_loss=1.1001 took 0.6424 secs\n",
      "epoch=1/10 iteration=498/34336 train_loss=1.0983 took 0.3123 secs\n",
      "epoch=1/10 iteration=499/34336 train_loss=1.1103 took 0.4373 secs\n",
      "epoch=1/10 iteration=500/34336 train_loss=1.0971 took 0.4405 secs\n",
      "epoch=1/10 iteration=501/34336 train_loss=1.0963 took 0.3663 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1/10 iteration=502/34336 train_loss=1.1015 took 0.3570 secs\n",
      "epoch=1/10 iteration=503/34336 train_loss=1.1027 took 0.3654 secs\n",
      "epoch=1/10 iteration=504/34336 train_loss=1.0992 took 0.3958 secs\n",
      "epoch=1/10 iteration=505/34336 train_loss=1.1005 took 0.5075 secs\n",
      "epoch=1/10 iteration=506/34336 train_loss=1.0999 took 0.4457 secs\n",
      "epoch=1/10 iteration=507/34336 train_loss=1.1068 took 0.4602 secs\n",
      "epoch=1/10 iteration=508/34336 train_loss=1.1041 took 0.3417 secs\n",
      "epoch=1/10 iteration=509/34336 train_loss=1.0911 took 0.5211 secs\n",
      "epoch=1/10 iteration=510/34336 train_loss=1.1035 took 0.3986 secs\n",
      "epoch=1/10 iteration=511/34336 train_loss=1.1097 took 0.3953 secs\n",
      "epoch=1/10 iteration=512/34336 train_loss=1.0980 took 0.3953 secs\n",
      "epoch=1/10 iteration=513/34336 train_loss=1.0989 took 0.3885 secs\n",
      "epoch=1/10 iteration=514/34336 train_loss=1.1009 took 0.4470 secs\n",
      "epoch=1/10 iteration=515/34336 train_loss=1.1113 took 0.4314 secs\n",
      "epoch=1/10 iteration=516/34336 train_loss=1.0976 took 0.4414 secs\n",
      "epoch=1/10 iteration=517/34336 train_loss=1.1036 took 0.3273 secs\n",
      "epoch=1/10 iteration=518/34336 train_loss=1.0982 took 0.3395 secs\n",
      "epoch=1/10 iteration=519/34336 train_loss=1.0984 took 0.4062 secs\n",
      "epoch=1/10 iteration=520/34336 train_loss=1.1007 took 0.3645 secs\n",
      "epoch=1/10 iteration=521/34336 train_loss=1.0998 took 0.4106 secs\n",
      "epoch=1/10 iteration=522/34336 train_loss=1.0994 took 0.3639 secs\n",
      "epoch=1/10 iteration=523/34336 train_loss=1.0941 took 0.5408 secs\n",
      "epoch=1/10 iteration=524/34336 train_loss=1.1068 took 0.4731 secs\n",
      "epoch=1/10 iteration=525/34336 train_loss=1.0944 took 0.3946 secs\n",
      "epoch=1/10 iteration=526/34336 train_loss=1.1138 took 0.4205 secs\n",
      "epoch=1/10 iteration=527/34336 train_loss=1.0975 took 0.4687 secs\n",
      "epoch=1/10 iteration=528/34336 train_loss=1.1057 took 0.4913 secs\n",
      "epoch=1/10 iteration=529/34336 train_loss=1.0965 took 0.4588 secs\n",
      "epoch=1/10 iteration=530/34336 train_loss=1.1010 took 0.4737 secs\n",
      "epoch=1/10 iteration=531/34336 train_loss=1.0995 took 0.4425 secs\n",
      "epoch=1/10 iteration=532/34336 train_loss=1.0997 took 0.5505 secs\n",
      "epoch=1/10 iteration=533/34336 train_loss=1.0942 took 0.3260 secs\n",
      "epoch=1/10 iteration=534/34336 train_loss=1.1035 took 0.3720 secs\n",
      "epoch=1/10 iteration=535/34336 train_loss=1.0955 took 0.5190 secs\n",
      "epoch=1/10 iteration=536/34336 train_loss=1.1050 took 0.4495 secs\n",
      "epoch=1/10 iteration=537/34336 train_loss=1.1117 took 0.4049 secs\n",
      "epoch=1/10 iteration=538/34336 train_loss=1.0986 took 0.4977 secs\n",
      "epoch=1/10 iteration=539/34336 train_loss=1.0989 took 0.3855 secs\n",
      "epoch=1/10 iteration=540/34336 train_loss=1.0977 took 0.3719 secs\n",
      "epoch=1/10 iteration=541/34336 train_loss=1.1070 took 0.3333 secs\n",
      "epoch=1/10 iteration=542/34336 train_loss=1.0997 took 0.3350 secs\n",
      "epoch=1/10 iteration=543/34336 train_loss=1.0994 took 0.3496 secs\n",
      "epoch=1/10 iteration=544/34336 train_loss=1.0991 took 0.3325 secs\n",
      "epoch=1/10 iteration=545/34336 train_loss=1.0916 took 0.4358 secs\n",
      "epoch=1/10 iteration=546/34336 train_loss=1.1084 took 0.3746 secs\n",
      "epoch=1/10 iteration=547/34336 train_loss=1.1063 took 0.4438 secs\n",
      "epoch=1/10 iteration=548/34336 train_loss=1.0965 took 0.3349 secs\n",
      "epoch=1/10 iteration=549/34336 train_loss=1.1079 took 0.3574 secs\n",
      "epoch=1/10 iteration=550/34336 train_loss=1.0995 took 0.4210 secs\n",
      "epoch=1/10 iteration=551/34336 train_loss=1.1046 took 0.4340 secs\n",
      "epoch=1/10 iteration=552/34336 train_loss=1.1041 took 0.4615 secs\n",
      "epoch=1/10 iteration=553/34336 train_loss=1.0934 took 0.5373 secs\n",
      "epoch=1/10 iteration=554/34336 train_loss=1.1049 took 0.4864 secs\n",
      "epoch=1/10 iteration=555/34336 train_loss=1.1037 took 0.4679 secs\n",
      "epoch=1/10 iteration=556/34336 train_loss=1.0951 took 0.4571 secs\n",
      "epoch=1/10 iteration=557/34336 train_loss=1.1024 took 0.4765 secs\n",
      "epoch=1/10 iteration=558/34336 train_loss=1.1019 took 0.3267 secs\n",
      "epoch=1/10 iteration=559/34336 train_loss=1.0995 took 0.3105 secs\n",
      "epoch=1/10 iteration=560/34336 train_loss=1.1088 took 0.3959 secs\n",
      "epoch=1/10 iteration=561/34336 train_loss=1.0973 took 0.3468 secs\n",
      "epoch=1/10 iteration=562/34336 train_loss=1.0964 took 0.4048 secs\n",
      "epoch=1/10 iteration=563/34336 train_loss=1.1051 took 0.4258 secs\n",
      "epoch=1/10 iteration=564/34336 train_loss=1.0953 took 0.3968 secs\n",
      "epoch=1/10 iteration=565/34336 train_loss=1.1016 took 0.5119 secs\n",
      "epoch=1/10 iteration=566/34336 train_loss=1.1008 took 0.5074 secs\n",
      "epoch=1/10 iteration=567/34336 train_loss=1.0991 took 0.4110 secs\n",
      "epoch=1/10 iteration=568/34336 train_loss=1.1010 took 0.4349 secs\n",
      "epoch=1/10 iteration=569/34336 train_loss=1.0989 took 0.3721 secs\n",
      "epoch=1/10 iteration=570/34336 train_loss=1.1005 took 0.4115 secs\n",
      "epoch=1/10 iteration=571/34336 train_loss=1.0960 took 0.3899 secs\n",
      "epoch=1/10 iteration=572/34336 train_loss=1.0974 took 0.3884 secs\n",
      "epoch=1/10 iteration=573/34336 train_loss=1.0986 took 0.4096 secs\n",
      "epoch=1/10 iteration=574/34336 train_loss=1.1030 took 0.4606 secs\n",
      "epoch=1/10 iteration=575/34336 train_loss=1.1014 took 0.4207 secs\n",
      "epoch=1/10 iteration=576/34336 train_loss=1.0981 took 0.3619 secs\n",
      "epoch=1/10 iteration=577/34336 train_loss=1.0997 took 0.3557 secs\n",
      "epoch=1/10 iteration=578/34336 train_loss=1.1032 took 0.3544 secs\n",
      "epoch=1/10 iteration=579/34336 train_loss=1.1005 took 0.3512 secs\n",
      "epoch=1/10 iteration=580/34336 train_loss=1.1035 took 0.3564 secs\n",
      "epoch=1/10 iteration=581/34336 train_loss=1.0981 took 0.3946 secs\n",
      "epoch=1/10 iteration=582/34336 train_loss=1.0997 took 0.4615 secs\n",
      "epoch=1/10 iteration=583/34336 train_loss=1.1011 took 0.4829 secs\n",
      "epoch=1/10 iteration=584/34336 train_loss=1.1007 took 0.4510 secs\n",
      "epoch=1/10 iteration=585/34336 train_loss=1.0977 took 0.4411 secs\n",
      "epoch=1/10 iteration=586/34336 train_loss=1.1001 took 0.3786 secs\n",
      "epoch=1/10 iteration=587/34336 train_loss=1.0987 took 0.3638 secs\n",
      "epoch=1/10 iteration=588/34336 train_loss=1.0996 took 0.3511 secs\n",
      "epoch=1/10 iteration=589/34336 train_loss=1.0991 took 0.4184 secs\n",
      "epoch=1/10 iteration=590/34336 train_loss=1.0997 took 0.3705 secs\n",
      "epoch=1/10 iteration=591/34336 train_loss=1.0993 took 0.4423 secs\n",
      "epoch=1/10 iteration=592/34336 train_loss=1.1013 took 0.3342 secs\n",
      "epoch=1/10 iteration=593/34336 train_loss=1.1015 took 0.2972 secs\n",
      "epoch=1/10 iteration=594/34336 train_loss=1.0978 took 0.3143 secs\n",
      "epoch=1/10 iteration=595/34336 train_loss=1.0962 took 0.3962 secs\n",
      "epoch=1/10 iteration=596/34336 train_loss=1.1121 took 0.3407 secs\n",
      "epoch=1/10 iteration=597/34336 train_loss=1.1010 took 0.4154 secs\n",
      "epoch=1/10 iteration=598/34336 train_loss=1.1005 took 0.3872 secs\n",
      "epoch=1/10 iteration=599/34336 train_loss=1.1033 took 0.4927 secs\n",
      "epoch=1/10 iteration=600/34336 train_loss=1.0935 took 0.3881 secs\n",
      "epoch=1/10 iteration=601/34336 train_loss=1.0996 took 0.3262 secs\n",
      "epoch=1/10 iteration=602/34336 train_loss=1.0981 took 0.3503 secs\n",
      "epoch=1/10 iteration=603/34336 train_loss=1.0960 took 0.5605 secs\n",
      "epoch=1/10 iteration=604/34336 train_loss=1.0992 took 0.7132 secs\n",
      "epoch=1/10 iteration=605/34336 train_loss=1.1019 took 0.3769 secs\n",
      "epoch=1/10 iteration=606/34336 train_loss=1.1024 took 0.3574 secs\n",
      "epoch=1/10 iteration=607/34336 train_loss=1.0958 took 0.3561 secs\n",
      "epoch=1/10 iteration=608/34336 train_loss=1.1008 took 0.4818 secs\n",
      "epoch=1/10 iteration=609/34336 train_loss=1.1009 took 0.3563 secs\n",
      "epoch=1/10 iteration=610/34336 train_loss=1.0970 took 0.4634 secs\n",
      "epoch=1/10 iteration=611/34336 train_loss=1.1070 took 0.4577 secs\n",
      "epoch=1/10 iteration=612/34336 train_loss=1.1089 took 0.4857 secs\n",
      "epoch=1/10 iteration=613/34336 train_loss=1.0989 took 0.4321 secs\n",
      "epoch=1/10 iteration=614/34336 train_loss=1.1099 took 0.3496 secs\n",
      "epoch=1/10 iteration=615/34336 train_loss=1.0978 took 0.6922 secs\n",
      "epoch=1/10 iteration=616/34336 train_loss=1.0991 took 0.4140 secs\n",
      "epoch=1/10 iteration=617/34336 train_loss=1.0989 took 0.3508 secs\n",
      "epoch=1/10 iteration=618/34336 train_loss=1.1035 took 0.4165 secs\n",
      "epoch=1/10 iteration=619/34336 train_loss=1.1058 took 0.4152 secs\n",
      "epoch=1/10 iteration=620/34336 train_loss=1.0992 took 0.3510 secs\n",
      "epoch=1/10 iteration=621/34336 train_loss=1.0959 took 0.3195 secs\n",
      "epoch=1/10 iteration=622/34336 train_loss=1.0982 took 0.3829 secs\n",
      "epoch=1/10 iteration=623/34336 train_loss=1.1050 took 0.6140 secs\n",
      "epoch=1/10 iteration=624/34336 train_loss=1.0987 took 0.6260 secs\n",
      "epoch=1/10 iteration=625/34336 train_loss=1.0966 took 0.3622 secs\n",
      "epoch=1/10 iteration=626/34336 train_loss=1.0983 took 0.2620 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1/10 iteration=627/34336 train_loss=1.1021 took 0.3714 secs\n",
      "epoch=1/10 iteration=628/34336 train_loss=1.0923 took 0.3818 secs\n",
      "epoch=1/10 iteration=629/34336 train_loss=1.1048 took 0.3221 secs\n",
      "epoch=1/10 iteration=630/34336 train_loss=1.1056 took 0.3449 secs\n",
      "epoch=1/10 iteration=631/34336 train_loss=1.1040 took 0.3434 secs\n",
      "epoch=1/10 iteration=632/34336 train_loss=1.1053 took 0.3193 secs\n",
      "epoch=1/10 iteration=633/34336 train_loss=1.0978 took 0.4865 secs\n",
      "epoch=1/10 iteration=634/34336 train_loss=1.0998 took 0.5117 secs\n",
      "epoch=1/10 iteration=635/34336 train_loss=1.0987 took 0.6334 secs\n",
      "epoch=1/10 iteration=636/34336 train_loss=1.0984 took 0.4702 secs\n",
      "epoch=1/10 iteration=637/34336 train_loss=1.1033 took 0.4426 secs\n",
      "epoch=1/10 iteration=638/34336 train_loss=1.0990 took 0.4426 secs\n",
      "epoch=1/10 iteration=639/34336 train_loss=1.0990 took 0.4420 secs\n",
      "epoch=1/10 iteration=640/34336 train_loss=1.0969 took 0.3982 secs\n",
      "epoch=1/10 iteration=641/34336 train_loss=1.1014 took 0.4450 secs\n",
      "epoch=1/10 iteration=642/34336 train_loss=1.0993 took 0.3504 secs\n",
      "epoch=1/10 iteration=643/34336 train_loss=1.0991 took 0.3256 secs\n",
      "epoch=1/10 iteration=644/34336 train_loss=1.1036 took 0.4988 secs\n",
      "epoch=1/10 iteration=645/34336 train_loss=1.0975 took 0.5179 secs\n",
      "epoch=1/10 iteration=646/34336 train_loss=1.1012 took 0.3558 secs\n",
      "epoch=1/10 iteration=647/34336 train_loss=1.0994 took 0.4346 secs\n",
      "epoch=1/10 iteration=648/34336 train_loss=1.1008 took 0.3622 secs\n",
      "epoch=1/10 iteration=649/34336 train_loss=1.0932 took 0.4319 secs\n",
      "epoch=1/10 iteration=650/34336 train_loss=1.1186 took 0.4479 secs\n",
      "epoch=1/10 iteration=651/34336 train_loss=1.1117 took 0.4104 secs\n",
      "epoch=1/10 iteration=652/34336 train_loss=1.0905 took 0.4293 secs\n",
      "epoch=1/10 iteration=653/34336 train_loss=1.1059 took 0.4514 secs\n",
      "epoch=1/10 iteration=654/34336 train_loss=1.1038 took 0.3720 secs\n",
      "epoch=1/10 iteration=655/34336 train_loss=1.1070 took 0.3573 secs\n",
      "epoch=1/10 iteration=656/34336 train_loss=1.1015 took 0.4538 secs\n",
      "epoch=1/10 iteration=657/34336 train_loss=1.0953 took 0.4529 secs\n",
      "epoch=1/10 iteration=658/34336 train_loss=1.1140 took 0.4927 secs\n",
      "epoch=1/10 iteration=659/34336 train_loss=1.1046 took 0.3879 secs\n",
      "epoch=1/10 iteration=660/34336 train_loss=1.1020 took 0.3329 secs\n",
      "epoch=1/10 iteration=661/34336 train_loss=1.1000 took 0.3200 secs\n",
      "epoch=1/10 iteration=662/34336 train_loss=1.1014 took 0.3039 secs\n",
      "epoch=1/10 iteration=663/34336 train_loss=1.0957 took 0.3517 secs\n",
      "epoch=1/10 iteration=664/34336 train_loss=1.0961 took 0.3564 secs\n",
      "epoch=1/10 iteration=665/34336 train_loss=1.1071 took 0.4163 secs\n",
      "epoch=1/10 iteration=666/34336 train_loss=1.1050 took 0.4208 secs\n",
      "epoch=1/10 iteration=667/34336 train_loss=1.0950 took 0.4943 secs\n",
      "epoch=1/10 iteration=668/34336 train_loss=1.1022 took 0.3687 secs\n",
      "epoch=1/10 iteration=669/34336 train_loss=1.1006 took 0.3804 secs\n",
      "epoch=1/10 iteration=670/34336 train_loss=1.0990 took 0.3792 secs\n",
      "epoch=1/10 iteration=671/34336 train_loss=1.1005 took 0.6116 secs\n",
      "epoch=1/10 iteration=672/34336 train_loss=1.0970 took 0.3878 secs\n",
      "epoch=1/10 iteration=673/34336 train_loss=1.1011 took 0.3173 secs\n",
      "epoch=1/10 iteration=674/34336 train_loss=1.1051 took 0.3427 secs\n",
      "epoch=1/10 iteration=675/34336 train_loss=1.1041 took 0.4132 secs\n",
      "epoch=1/10 iteration=676/34336 train_loss=1.0932 took 0.5229 secs\n",
      "epoch=1/10 iteration=677/34336 train_loss=1.1005 took 0.4582 secs\n",
      "epoch=1/10 iteration=678/34336 train_loss=1.0950 took 0.4681 secs\n",
      "epoch=1/10 iteration=679/34336 train_loss=1.1214 took 0.3810 secs\n",
      "epoch=1/10 iteration=680/34336 train_loss=1.0906 took 0.4177 secs\n",
      "epoch=1/10 iteration=681/34336 train_loss=1.0995 took 0.3969 secs\n",
      "epoch=1/10 iteration=682/34336 train_loss=1.0974 took 0.3977 secs\n",
      "epoch=1/10 iteration=683/34336 train_loss=1.1187 took 0.4051 secs\n",
      "epoch=1/10 iteration=684/34336 train_loss=1.1194 took 0.3722 secs\n",
      "epoch=1/10 iteration=685/34336 train_loss=1.0897 took 0.3303 secs\n",
      "epoch=1/10 iteration=686/34336 train_loss=1.0976 took 0.3659 secs\n",
      "epoch=1/10 iteration=687/34336 train_loss=1.1084 took 0.3801 secs\n",
      "epoch=1/10 iteration=688/34336 train_loss=1.1045 took 0.3014 secs\n",
      "epoch=1/10 iteration=689/34336 train_loss=1.0953 took 0.4475 secs\n",
      "epoch=1/10 iteration=690/34336 train_loss=1.0938 took 0.4608 secs\n",
      "epoch=1/10 iteration=691/34336 train_loss=1.1117 took 0.4048 secs\n",
      "epoch=1/10 iteration=692/34336 train_loss=1.1020 took 0.2513 secs\n",
      "epoch=1/10 iteration=693/34336 train_loss=1.1011 took 0.2944 secs\n",
      "epoch=1/10 iteration=694/34336 train_loss=1.1087 took 0.3484 secs\n",
      "epoch=1/10 iteration=695/34336 train_loss=1.1056 took 0.6423 secs\n",
      "epoch=1/10 iteration=696/34336 train_loss=1.0996 took 0.3141 secs\n",
      "epoch=1/10 iteration=697/34336 train_loss=1.0999 took 0.3661 secs\n",
      "epoch=1/10 iteration=698/34336 train_loss=1.0985 took 0.3711 secs\n",
      "epoch=1/10 iteration=699/34336 train_loss=1.0982 took 0.6282 secs\n",
      "epoch=1/10 iteration=700/34336 train_loss=1.1042 took 0.4361 secs\n",
      "epoch=1/10 iteration=701/34336 train_loss=1.0980 took 0.4667 secs\n",
      "epoch=1/10 iteration=702/34336 train_loss=1.1062 took 0.4213 secs\n",
      "epoch=1/10 iteration=703/34336 train_loss=1.0975 took 0.4250 secs\n",
      "epoch=1/10 iteration=704/34336 train_loss=1.1018 took 0.3665 secs\n",
      "epoch=1/10 iteration=705/34336 train_loss=1.0972 took 0.4937 secs\n",
      "epoch=1/10 iteration=706/34336 train_loss=1.1047 took 0.5279 secs\n",
      "epoch=1/10 iteration=707/34336 train_loss=1.1003 took 0.3354 secs\n",
      "epoch=1/10 iteration=708/34336 train_loss=1.0971 took 0.4209 secs\n",
      "epoch=1/10 iteration=709/34336 train_loss=1.1108 took 0.4712 secs\n",
      "epoch=1/10 iteration=710/34336 train_loss=1.1052 took 0.3675 secs\n",
      "epoch=1/10 iteration=711/34336 train_loss=1.0957 took 0.4193 secs\n",
      "epoch=1/10 iteration=712/34336 train_loss=1.0991 took 0.4684 secs\n",
      "epoch=1/10 iteration=713/34336 train_loss=1.0967 took 0.3508 secs\n",
      "epoch=1/10 iteration=714/34336 train_loss=1.0993 took 0.3179 secs\n",
      "epoch=1/10 iteration=715/34336 train_loss=1.1001 took 0.3027 secs\n",
      "epoch=1/10 iteration=716/34336 train_loss=1.0990 took 0.5066 secs\n",
      "epoch=1/10 iteration=717/34336 train_loss=1.1036 took 0.3808 secs\n",
      "epoch=1/10 iteration=718/34336 train_loss=1.0980 took 0.4055 secs\n",
      "epoch=1/10 iteration=719/34336 train_loss=1.0985 took 0.6562 secs\n",
      "epoch=1/10 iteration=720/34336 train_loss=1.1020 took 0.4515 secs\n",
      "epoch=1/10 iteration=721/34336 train_loss=1.0986 took 0.3795 secs\n",
      "epoch=1/10 iteration=722/34336 train_loss=1.1008 took 0.4874 secs\n",
      "epoch=1/10 iteration=723/34336 train_loss=1.1017 took 0.5036 secs\n",
      "epoch=1/10 iteration=724/34336 train_loss=1.1002 took 0.4560 secs\n",
      "epoch=1/10 iteration=725/34336 train_loss=1.1021 took 0.4219 secs\n",
      "epoch=1/10 iteration=726/34336 train_loss=1.0987 took 0.3750 secs\n",
      "epoch=1/10 iteration=727/34336 train_loss=1.0996 took 0.3736 secs\n",
      "epoch=1/10 iteration=728/34336 train_loss=1.0965 took 0.3908 secs\n",
      "epoch=1/10 iteration=729/34336 train_loss=1.1114 took 0.4376 secs\n",
      "epoch=1/10 iteration=730/34336 train_loss=1.0916 took 0.4198 secs\n",
      "epoch=1/10 iteration=731/34336 train_loss=1.1128 took 0.4459 secs\n",
      "epoch=1/10 iteration=732/34336 train_loss=1.1065 took 0.3708 secs\n",
      "epoch=1/10 iteration=733/34336 train_loss=1.1006 took 0.3748 secs\n",
      "epoch=1/10 iteration=734/34336 train_loss=1.0968 took 0.4949 secs\n",
      "epoch=1/10 iteration=735/34336 train_loss=1.1032 took 0.4032 secs\n",
      "epoch=1/10 iteration=736/34336 train_loss=1.1025 took 0.3999 secs\n",
      "epoch=1/10 iteration=737/34336 train_loss=1.0987 took 0.3849 secs\n",
      "epoch=1/10 iteration=738/34336 train_loss=1.1144 took 0.3455 secs\n",
      "epoch=1/10 iteration=739/34336 train_loss=1.0944 took 0.5225 secs\n",
      "epoch=1/10 iteration=740/34336 train_loss=1.0959 took 0.6361 secs\n",
      "epoch=1/10 iteration=741/34336 train_loss=1.1043 took 0.4584 secs\n",
      "epoch=1/10 iteration=742/34336 train_loss=1.0974 took 0.3659 secs\n",
      "epoch=1/10 iteration=743/34336 train_loss=1.0996 took 0.3639 secs\n",
      "epoch=1/10 iteration=744/34336 train_loss=1.1104 took 0.4160 secs\n",
      "epoch=1/10 iteration=745/34336 train_loss=1.1098 took 0.5174 secs\n",
      "epoch=1/10 iteration=746/34336 train_loss=1.1109 took 0.5297 secs\n",
      "epoch=1/10 iteration=747/34336 train_loss=1.0982 took 0.5329 secs\n",
      "epoch=1/10 iteration=748/34336 train_loss=1.0990 took 0.4920 secs\n",
      "epoch=1/10 iteration=749/34336 train_loss=1.1011 took 0.4573 secs\n",
      "epoch=1/10 iteration=750/34336 train_loss=1.1020 took 0.3964 secs\n",
      "epoch=1/10 iteration=751/34336 train_loss=1.0955 took 0.4326 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1/10 iteration=752/34336 train_loss=1.1046 took 0.4105 secs\n",
      "epoch=1/10 iteration=753/34336 train_loss=1.1029 took 0.3185 secs\n",
      "epoch=1/10 iteration=754/34336 train_loss=1.0971 took 0.3870 secs\n",
      "epoch=1/10 iteration=755/34336 train_loss=1.0991 took 0.4435 secs\n",
      "epoch=1/10 iteration=756/34336 train_loss=1.0963 took 0.4850 secs\n",
      "epoch=1/10 iteration=757/34336 train_loss=1.0956 took 0.4180 secs\n",
      "epoch=1/10 iteration=758/34336 train_loss=1.1058 took 0.6471 secs\n",
      "epoch=1/10 iteration=759/34336 train_loss=1.1098 took 0.4610 secs\n",
      "epoch=1/10 iteration=760/34336 train_loss=1.0951 took 0.4518 secs\n",
      "epoch=1/10 iteration=761/34336 train_loss=1.1016 took 0.3948 secs\n",
      "epoch=1/10 iteration=762/34336 train_loss=1.1044 took 0.4513 secs\n",
      "epoch=1/10 iteration=763/34336 train_loss=1.1009 took 0.3583 secs\n",
      "epoch=1/10 iteration=764/34336 train_loss=1.1001 took 0.3250 secs\n",
      "epoch=1/10 iteration=765/34336 train_loss=1.1039 took 0.4266 secs\n",
      "epoch=1/10 iteration=766/34336 train_loss=1.1027 took 0.3304 secs\n",
      "epoch=1/10 iteration=767/34336 train_loss=1.1013 took 0.3555 secs\n",
      "epoch=1/10 iteration=768/34336 train_loss=1.1019 took 0.4835 secs\n",
      "epoch=1/10 iteration=769/34336 train_loss=1.0976 took 0.3339 secs\n",
      "epoch=1/10 iteration=770/34336 train_loss=1.1039 took 0.2796 secs\n",
      "epoch=1/10 iteration=771/34336 train_loss=1.0984 took 0.4883 secs\n",
      "epoch=1/10 iteration=772/34336 train_loss=1.0994 took 0.3775 secs\n",
      "epoch=1/10 iteration=773/34336 train_loss=1.1006 took 0.5632 secs\n",
      "epoch=1/10 iteration=774/34336 train_loss=1.0987 took 0.4357 secs\n",
      "epoch=1/10 iteration=775/34336 train_loss=1.0999 took 0.3664 secs\n",
      "epoch=1/10 iteration=776/34336 train_loss=1.0985 took 0.5600 secs\n",
      "epoch=1/10 iteration=777/34336 train_loss=1.1017 took 0.5920 secs\n",
      "epoch=1/10 iteration=778/34336 train_loss=1.1160 took 0.5340 secs\n",
      "epoch=1/10 iteration=779/34336 train_loss=1.0903 took 0.4022 secs\n",
      "epoch=1/10 iteration=780/34336 train_loss=1.1061 took 0.5104 secs\n",
      "epoch=1/10 iteration=781/34336 train_loss=1.1061 took 0.3635 secs\n",
      "epoch=1/10 iteration=782/34336 train_loss=1.1003 took 0.2926 secs\n",
      "epoch=1/10 iteration=783/34336 train_loss=1.1033 took 0.4426 secs\n",
      "epoch=1/10 iteration=784/34336 train_loss=1.1045 took 0.6031 secs\n",
      "epoch=1/10 iteration=785/34336 train_loss=1.1002 took 0.3462 secs\n",
      "epoch=1/10 iteration=786/34336 train_loss=1.0958 took 0.3075 secs\n",
      "epoch=1/10 iteration=787/34336 train_loss=1.1049 took 0.3535 secs\n",
      "epoch=1/10 iteration=788/34336 train_loss=1.0948 took 0.3964 secs\n",
      "epoch=1/10 iteration=789/34336 train_loss=1.1026 took 0.3035 secs\n",
      "epoch=1/10 iteration=790/34336 train_loss=1.0997 took 0.3532 secs\n",
      "epoch=1/10 iteration=791/34336 train_loss=1.0991 took 0.4662 secs\n",
      "epoch=1/10 iteration=792/34336 train_loss=1.1014 took 0.4391 secs\n",
      "epoch=1/10 iteration=793/34336 train_loss=1.0959 took 0.5977 secs\n",
      "epoch=1/10 iteration=794/34336 train_loss=1.1029 took 0.3495 secs\n",
      "epoch=1/10 iteration=795/34336 train_loss=1.0985 took 0.4127 secs\n",
      "epoch=1/10 iteration=796/34336 train_loss=1.1030 took 0.3424 secs\n",
      "epoch=1/10 iteration=797/34336 train_loss=1.1010 took 0.3604 secs\n",
      "epoch=1/10 iteration=798/34336 train_loss=1.1028 took 0.3662 secs\n",
      "epoch=1/10 iteration=799/34336 train_loss=1.0954 took 0.3914 secs\n",
      "epoch=1/10 iteration=800/34336 train_loss=1.1016 took 0.3669 secs\n",
      "epoch=1/10 iteration=801/34336 train_loss=1.0982 took 0.3133 secs\n",
      "epoch=1/10 iteration=802/34336 train_loss=1.1027 took 0.4699 secs\n",
      "epoch=1/10 iteration=803/34336 train_loss=1.1009 took 0.3808 secs\n",
      "epoch=1/10 iteration=804/34336 train_loss=1.0994 took 0.4174 secs\n",
      "epoch=1/10 iteration=805/34336 train_loss=1.1054 took 0.3603 secs\n",
      "epoch=1/10 iteration=806/34336 train_loss=1.0984 took 0.4215 secs\n",
      "epoch=1/10 iteration=807/34336 train_loss=1.0999 took 0.4105 secs\n",
      "epoch=1/10 iteration=808/34336 train_loss=1.0987 took 0.4264 secs\n",
      "epoch=1/10 iteration=809/34336 train_loss=1.1023 took 0.4116 secs\n",
      "epoch=1/10 iteration=810/34336 train_loss=1.1077 took 0.7411 secs\n",
      "epoch=1/10 iteration=811/34336 train_loss=1.0989 took 0.6933 secs\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# from torchsample.callbacks import EarlyStopping\n",
    "# callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "# model.set_callbacks(callbacks)\n",
    "\n",
    "\n",
    "n_epochs = 10\n",
    "n_batch = 16\n",
    "lr=0.001\n",
    "\n",
    "\n",
    "training_data = SNLI_Dataset(df_train)\n",
    "train_dataloader = DataLoader(training_data, batch_size=n_batch, shuffle=False)\n",
    "\n",
    "validation_data = SNLI_Dataset(df_val)\n",
    "val_dataloader = DataLoader(validation_data, batch_size=n_batch, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(sbert.parameters(), lr=lr, momentum=0.9)\n",
    "optimizer = optim.Adam(sbert.parameters(), lr=lr)\n",
    "\n",
    "sbert.train()\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for i,((sent1,sent2),label) in enumerate(train_dataloader):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = sbert(sent1,sent2,objective=\"classification\")\n",
    "        loss = criterion(output, label)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "#         (sent1_val,sent2_val),label_val = next(iter(val_dataloader))\n",
    "#         output_val = sbert(sent1_val,sent2_val,\"classification\")\n",
    "#         val_loss = criterion(output_val,label_val)\n",
    "#         val_losses.append(val_loss)\n",
    "\n",
    "        message = \"epoch={}/{} iteration={}/{} train_loss={:.4f} took {:.4f} secs\" \\\n",
    "            .format(epoch+1,n_epochs,i+1,len(train_dataloader),loss.detach().numpy(),time.time()-start)\n",
    "\n",
    "        print(message)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    PATH = f\"models/classification_regression_epoch_{epoch+1}.pt\"\n",
    "    torch.save(sbert.state_dict(), PATH)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"models/classification_epoch_5.pt\"\n",
    "\n",
    "sbert = SBERT()\n",
    "sbert.load_state_dict(torch.load(PATH))\n",
    "# sbert.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = []\n",
    "# for idx in range(5):\n",
    "#     pred = torch.argmax(sbert(df_train.iloc[idx].sentence1,df_train.iloc[idx].sentence2,\"classification\")).numpy().item()\n",
    "#     true = df_train.iloc[idx]['gold_label']\n",
    "#     print(pred==true)\n",
    "#     res.append(pred==true)\n",
    "# acc = sum(res)/len(res)\n",
    "# print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3338ed 4999/5000 got 1669/5000\n"
     ]
    }
   ],
   "source": [
    "sbert.eval()\n",
    "res = []\n",
    "# for idx in range(5):\n",
    "for i, row in df_train.head(5000).iterrows():\n",
    "    pred = torch.argmax(sbert(row.sentence1,row.sentence2,\"classification\")).numpy().item()\n",
    "    true = row['gold_label']\n",
    "    res.append(pred==true)\n",
    "    print(f\"Finished {i}/{len(df_train.head(5000))} got {sum(res)}/{len(res)}\",end=\"\\r\")\n",
    "acc = sum(res)/len(res)\n",
    "print()\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Use the model trained for classification and fine-tuning it using the regression objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"models/classification_epoch_5.pt\"\n",
    "\n",
    "sbert = SBERT()\n",
    "sbert.load_state_dict(torch.load(PATH))\n",
    "# sbert.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.80</td>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.80</td>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.60</td>\n",
       "      <td>Three men are playing chess.</td>\n",
       "      <td>Two men are playing chess.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.25</td>\n",
       "      <td>A man is playing the cello.</td>\n",
       "      <td>A man seated is playing the cello.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.25</td>\n",
       "      <td>Some men are fighting.</td>\n",
       "      <td>Two men are fighting.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                      sentence1  \\\n",
       "0   3.80                A man is playing a large flute.   \n",
       "1   3.80  A man is spreading shreded cheese on a pizza.   \n",
       "2   2.60                   Three men are playing chess.   \n",
       "3   4.25                    A man is playing the cello.   \n",
       "4   4.25                         Some men are fighting.   \n",
       "\n",
       "                                           sentence2  \n",
       "0                          A man is playing a flute.  \n",
       "1  A man is spreading shredded cheese on an uncoo...  \n",
       "2                         Two men are playing chess.  \n",
       "3                 A man seated is playing the cello.  \n",
       "4                              Two men are fighting.  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"datasets/Stsbenchmark/sts-train.csv\",header=0,names=[\"main-caption\",\"genre\",\"filename\",\"year\",\"score\",\"sentence1\",\"sentence2\"])#,usecols=['score','sentence1','sentence2'])\n",
    "df_test = pd.read_csv(\"datasets/Stsbenchmark/sts-test.csv\",header=0,names=[\"main-caption\",\"genre\",\"filename\",\"year\",\"score\",\"sentence1\",\"sentence2\"])#,usecols=['score','sentence1','sentence2'])\n",
    "\n",
    "df_train = df_train[['score','sentence1','sentence2']]\n",
    "df_test = df_test[['score','sentence1','sentence2']]\n",
    "\n",
    "df_train.head()\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score         0\n",
      "sentence1     0\n",
      "sentence2    11\n",
      "dtype: int64\n",
      "\n",
      "score        0\n",
      "sentence1    0\n",
      "sentence2    5\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.isna().sum())\n",
    "print()\n",
    "print(df_test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(inplace=True)\n",
    "df_train.reset_index(inplace=True)\n",
    "\n",
    "df_test.dropna(inplace=True)\n",
    "df_test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index        0\n",
      "score        0\n",
      "sentence1    0\n",
      "sentence2    0\n",
      "dtype: int64\n",
      "\n",
      "index        0\n",
      "score        0\n",
      "sentence1    0\n",
      "sentence2    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.isna().sum())\n",
    "print()\n",
    "print(df_test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 5.0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = list(df_train.score)\n",
    "minn = min(ls)\n",
    "maxx = max(ls)\n",
    "(minn,maxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_score(value, leftMin=0, leftMax=5, rightMin=-1, rightMax=1):\n",
    "    # Figure out how 'wide' each range is\n",
    "    leftSpan = leftMax - leftMin\n",
    "    rightSpan = rightMax - rightMin\n",
    "\n",
    "    # Convert the left range into a 0-1 range (float)\n",
    "    valueScaled = float(value - leftMin) / float(leftSpan)\n",
    "\n",
    "    # Convert the 0-1 range into a value in the right range.\n",
    "    return rightMin + (valueScaled * rightSpan)\n",
    "\n",
    "df_train['score'] = df_train['score'].apply(map_score)\n",
    "df_test['score'] = df_test['score'].apply(map_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = list(df_train.score)\n",
    "minn = min(ls)\n",
    "maxx = max(ls)\n",
    "(minn,maxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.sentence1 = df_train.sentence1.astype(str)\n",
    "df_train.sentence2 = df_train.sentence2.astype(str)\n",
    "\n",
    "df_test.sentence1 = df_test.sentence1.astype(str)\n",
    "df_test.sentence2 = df_test.sentence2.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[~df_train.sentence1.str.contains(\"\\n\")]\n",
    "df_train = df_train[~df_train.sentence2.str.contains(\"\\n\")]\n",
    "\n",
    "df_test = df_test[~df_test.sentence1.str.contains(\"\\n\")]\n",
    "df_test = df_test[~df_test.sentence2.str.contains(\"\\n\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class STS_Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        sent1 = row['sentence1']\n",
    "        sent2 = row['sentence2']\n",
    "        label = row['score']\n",
    "        return (sent1, sent2), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9932], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert(\"I love pizza\",\"I love burger\",objective=\"regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(sbert.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_regression = STS_Dataset(df_train)\n",
    "train_dataloader_regression = DataLoader(training_data_regression, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3/3 iteration=1/245 loss=0.6318621635437012 took 0.3605921268463135 secs\n",
      "epoch=3/3 iteration=2/245 loss=0.40562236309051514 took 0.31027746200561523 secs\n",
      "epoch=3/3 iteration=3/245 loss=0.8065990209579468 took 0.23789548873901367 secs\n",
      "epoch=3/3 iteration=4/245 loss=0.6440738439559937 took 0.2286243438720703 secs\n",
      "epoch=3/3 iteration=5/245 loss=0.7480531930923462 took 0.22533774375915527 secs\n",
      "epoch=3/3 iteration=6/245 loss=1.1598297357559204 took 0.23276448249816895 secs\n",
      "epoch=3/3 iteration=7/245 loss=0.6883032917976379 took 0.20563983917236328 secs\n",
      "epoch=3/3 iteration=8/245 loss=1.2146214246749878 took 0.23572778701782227 secs\n",
      "epoch=3/3 iteration=9/245 loss=1.1725239753723145 took 0.3171517848968506 secs\n",
      "epoch=3/3 iteration=10/245 loss=0.4181896448135376 took 0.31582069396972656 secs\n",
      "epoch=3/3 iteration=11/245 loss=1.568471908569336 took 0.2533571720123291 secs\n",
      "epoch=3/3 iteration=12/245 loss=0.963043749332428 took 0.33654332160949707 secs\n",
      "epoch=3/3 iteration=13/245 loss=1.3176404237747192 took 0.2832834720611572 secs\n",
      "epoch=3/3 iteration=14/245 loss=2.1170144081115723 took 0.2642824649810791 secs\n",
      "epoch=3/3 iteration=15/245 loss=2.113760471343994 took 0.23930788040161133 secs\n",
      "epoch=3/3 iteration=16/245 loss=1.4550267457962036 took 0.27668190002441406 secs\n",
      "epoch=3/3 iteration=17/245 loss=1.5848286151885986 took 0.26754045486450195 secs\n",
      "epoch=3/3 iteration=18/245 loss=2.2359533309936523 took 0.25798916816711426 secs\n",
      "epoch=3/3 iteration=19/245 loss=1.2237523794174194 took 0.27786684036254883 secs\n",
      "epoch=3/3 iteration=20/245 loss=1.8180654048919678 took 0.24576783180236816 secs\n",
      "epoch=3/3 iteration=21/245 loss=2.461071729660034 took 0.2745950222015381 secs\n",
      "epoch=3/3 iteration=22/245 loss=1.9586834907531738 took 0.31129026412963867 secs\n",
      "epoch=3/3 iteration=23/245 loss=2.763369083404541 took 0.33904290199279785 secs\n",
      "epoch=3/3 iteration=24/245 loss=1.5633468627929688 took 0.25018787384033203 secs\n",
      "epoch=3/3 iteration=25/245 loss=1.9525811672210693 took 0.3270447254180908 secs\n",
      "epoch=3/3 iteration=26/245 loss=1.952845811843872 took 0.35396504402160645 secs\n",
      "epoch=3/3 iteration=27/245 loss=2.309659004211426 took 0.27315592765808105 secs\n",
      "epoch=3/3 iteration=28/245 loss=1.8531311750411987 took 0.35076379776000977 secs\n",
      "epoch=3/3 iteration=29/245 loss=2.639838933944702 took 0.3460848331451416 secs\n",
      "epoch=3/3 iteration=30/245 loss=2.367833137512207 took 0.3319077491760254 secs\n",
      "epoch=3/3 iteration=31/245 loss=2.1745355129241943 took 0.31119203567504883 secs\n",
      "epoch=3/3 iteration=32/245 loss=0.5062648057937622 took 0.2425696849822998 secs\n",
      "epoch=3/3 iteration=33/245 loss=0.6642733216285706 took 0.24860358238220215 secs\n",
      "epoch=3/3 iteration=34/245 loss=0.4830557703971863 took 0.21915411949157715 secs\n",
      "epoch=3/3 iteration=35/245 loss=0.9747359156608582 took 0.24855995178222656 secs\n",
      "epoch=3/3 iteration=36/245 loss=1.1681406497955322 took 0.33465099334716797 secs\n",
      "epoch=3/3 iteration=37/245 loss=1.6947811841964722 took 0.22939372062683105 secs\n",
      "epoch=3/3 iteration=38/245 loss=1.5662587881088257 took 0.2489919662475586 secs\n",
      "epoch=3/3 iteration=39/245 loss=1.1033029556274414 took 0.2746562957763672 secs\n",
      "epoch=3/3 iteration=40/245 loss=1.245246171951294 took 0.2310807704925537 secs\n",
      "epoch=3/3 iteration=41/245 loss=0.9000906348228455 took 0.2648963928222656 secs\n",
      "epoch=3/3 iteration=42/245 loss=1.0241001844406128 took 0.2525310516357422 secs\n",
      "epoch=3/3 iteration=43/245 loss=1.381223201751709 took 0.2492661476135254 secs\n",
      "epoch=3/3 iteration=44/245 loss=1.9362472295761108 took 0.2523038387298584 secs\n",
      "epoch=3/3 iteration=45/245 loss=1.6910569667816162 took 0.24329471588134766 secs\n",
      "epoch=3/3 iteration=46/245 loss=1.8910324573516846 took 0.24696660041809082 secs\n",
      "epoch=3/3 iteration=47/245 loss=1.9964714050292969 took 0.25786900520324707 secs\n",
      "epoch=3/3 iteration=48/245 loss=2.491955518722534 took 0.22010254859924316 secs\n",
      "epoch=3/3 iteration=49/245 loss=1.0366425514221191 took 0.28946995735168457 secs\n",
      "epoch=3/3 iteration=50/245 loss=1.6308833360671997 took 0.2965099811553955 secs\n",
      "epoch=3/3 iteration=51/245 loss=1.626291036605835 took 0.29248785972595215 secs\n",
      "epoch=3/3 iteration=52/245 loss=2.5037710666656494 took 0.2696349620819092 secs\n",
      "epoch=3/3 iteration=53/245 loss=1.371306300163269 took 0.4659578800201416 secs\n",
      "epoch=3/3 iteration=54/245 loss=2.6243648529052734 took 0.2878131866455078 secs\n",
      "epoch=3/3 iteration=55/245 loss=2.486783266067505 took 0.3009357452392578 secs\n",
      "epoch=3/3 iteration=56/245 loss=1.5730503797531128 took 0.2695155143737793 secs\n",
      "epoch=3/3 iteration=57/245 loss=2.718578338623047 took 0.28563666343688965 secs\n",
      "epoch=3/3 iteration=58/245 loss=2.3809351921081543 took 0.23576927185058594 secs\n",
      "epoch=3/3 iteration=59/245 loss=2.7132675647735596 took 0.291095495223999 secs\n",
      "epoch=3/3 iteration=60/245 loss=2.103043556213379 took 0.2931501865386963 secs\n",
      "epoch=3/3 iteration=61/245 loss=1.5445361137390137 took 0.41195249557495117 secs\n",
      "epoch=3/3 iteration=62/245 loss=2.4563050270080566 took 0.3036985397338867 secs\n",
      "epoch=3/3 iteration=63/245 loss=1.6188552379608154 took 0.2646830081939697 secs\n",
      "epoch=3/3 iteration=64/245 loss=1.4564591646194458 took 0.3510165214538574 secs\n",
      "epoch=3/3 iteration=65/245 loss=1.2540364265441895 took 0.29790377616882324 secs\n",
      "epoch=3/3 iteration=66/245 loss=1.5407544374465942 took 0.31531381607055664 secs\n",
      "epoch=3/3 iteration=67/245 loss=0.9962762594223022 took 0.33191585540771484 secs\n",
      "epoch=3/3 iteration=68/245 loss=1.2096065282821655 took 0.391460657119751 secs\n",
      "epoch=3/3 iteration=69/245 loss=1.3317677974700928 took 0.4092235565185547 secs\n",
      "epoch=3/3 iteration=70/245 loss=1.4156502485275269 took 0.37784600257873535 secs\n",
      "epoch=3/3 iteration=71/245 loss=1.2611066102981567 took 0.3407161235809326 secs\n",
      "epoch=3/3 iteration=72/245 loss=2.058314323425293 took 0.38695549964904785 secs\n",
      "epoch=3/3 iteration=73/245 loss=1.1367436647415161 took 0.3593106269836426 secs\n",
      "epoch=3/3 iteration=74/245 loss=1.2459981441497803 took 0.3966653347015381 secs\n",
      "epoch=3/3 iteration=75/245 loss=1.8463530540466309 took 0.3095438480377197 secs\n",
      "epoch=3/3 iteration=76/245 loss=0.9365397095680237 took 0.37420177459716797 secs\n",
      "epoch=3/3 iteration=77/245 loss=1.5974795818328857 took 0.35057973861694336 secs\n",
      "epoch=3/3 iteration=78/245 loss=1.2712297439575195 took 0.3077824115753174 secs\n",
      "epoch=3/3 iteration=79/245 loss=1.308359980583191 took 0.39401817321777344 secs\n",
      "epoch=3/3 iteration=80/245 loss=1.6789135932922363 took 0.35845327377319336 secs\n",
      "epoch=3/3 iteration=81/245 loss=1.337242603302002 took 0.3291139602661133 secs\n",
      "epoch=3/3 iteration=82/245 loss=0.9328470230102539 took 0.3481473922729492 secs\n",
      "epoch=3/3 iteration=83/245 loss=1.0472716093063354 took 0.4767787456512451 secs\n",
      "epoch=3/3 iteration=84/245 loss=1.0030980110168457 took 0.3148987293243408 secs\n",
      "epoch=3/3 iteration=85/245 loss=1.2283686399459839 took 0.35118913650512695 secs\n",
      "epoch=3/3 iteration=86/245 loss=0.7890948057174683 took 0.3837850093841553 secs\n",
      "epoch=3/3 iteration=87/245 loss=1.603326678276062 took 0.4468657970428467 secs\n",
      "epoch=3/3 iteration=88/245 loss=0.7464424967765808 took 0.3558993339538574 secs\n",
      "epoch=3/3 iteration=89/245 loss=1.311295509338379 took 0.34286022186279297 secs\n",
      "epoch=3/3 iteration=90/245 loss=0.4184778332710266 took 0.3769042491912842 secs\n",
      "epoch=3/3 iteration=91/245 loss=1.000396490097046 took 0.316436767578125 secs\n",
      "epoch=3/3 iteration=92/245 loss=0.5316572785377502 took 0.4214651584625244 secs\n",
      "epoch=3/3 iteration=93/245 loss=1.5241953134536743 took 0.341890811920166 secs\n",
      "epoch=3/3 iteration=94/245 loss=1.2667485475540161 took 0.47832202911376953 secs\n",
      "epoch=3/3 iteration=95/245 loss=1.5390899181365967 took 0.355104923248291 secs\n",
      "epoch=3/3 iteration=96/245 loss=0.9872480630874634 took 0.3329784870147705 secs\n",
      "epoch=3/3 iteration=97/245 loss=1.3251755237579346 took 0.47085070610046387 secs\n",
      "epoch=3/3 iteration=98/245 loss=1.5527290105819702 took 0.3542919158935547 secs\n",
      "epoch=3/3 iteration=99/245 loss=1.7306636571884155 took 0.4954681396484375 secs\n",
      "epoch=3/3 iteration=100/245 loss=1.0741467475891113 took 0.4117865562438965 secs\n",
      "epoch=3/3 iteration=101/245 loss=1.5015894174575806 took 0.38101816177368164 secs\n",
      "epoch=3/3 iteration=102/245 loss=1.2913850545883179 took 0.38073062896728516 secs\n",
      "epoch=3/3 iteration=103/245 loss=1.4208722114562988 took 0.3980116844177246 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3/3 iteration=104/245 loss=1.8507169485092163 took 0.40380382537841797 secs\n",
      "epoch=3/3 iteration=105/245 loss=0.5341312289237976 took 0.281658411026001 secs\n",
      "epoch=3/3 iteration=106/245 loss=1.7584996223449707 took 0.39264345169067383 secs\n",
      "epoch=3/3 iteration=107/245 loss=1.7386915683746338 took 0.3236367702484131 secs\n",
      "epoch=3/3 iteration=108/245 loss=1.4523295164108276 took 0.46636152267456055 secs\n",
      "epoch=3/3 iteration=109/245 loss=1.3675284385681152 took 0.3431675434112549 secs\n",
      "epoch=3/3 iteration=110/245 loss=1.5072845220565796 took 0.46570897102355957 secs\n",
      "epoch=3/3 iteration=111/245 loss=1.477246642112732 took 0.3639953136444092 secs\n",
      "epoch=3/3 iteration=112/245 loss=1.9153281450271606 took 0.496049165725708 secs\n",
      "epoch=3/3 iteration=113/245 loss=1.9780460596084595 took 0.40122342109680176 secs\n",
      "epoch=3/3 iteration=114/245 loss=0.8913781642913818 took 0.362440824508667 secs\n",
      "epoch=3/3 iteration=115/245 loss=1.189072847366333 took 0.38720059394836426 secs\n",
      "epoch=3/3 iteration=116/245 loss=1.4673941135406494 took 0.37930798530578613 secs\n",
      "epoch=3/3 iteration=117/245 loss=1.1363110542297363 took 0.3719024658203125 secs\n",
      "epoch=3/3 iteration=118/245 loss=1.2505393028259277 took 0.3461496829986572 secs\n",
      "epoch=3/3 iteration=119/245 loss=1.1993844509124756 took 0.3983161449432373 secs\n",
      "epoch=3/3 iteration=120/245 loss=1.27702796459198 took 0.4122958183288574 secs\n",
      "epoch=3/3 iteration=121/245 loss=1.6706771850585938 took 0.34468507766723633 secs\n",
      "epoch=3/3 iteration=122/245 loss=1.6189388036727905 took 0.34464311599731445 secs\n",
      "epoch=3/3 iteration=123/245 loss=1.5641659498214722 took 0.33324766159057617 secs\n",
      "epoch=3/3 iteration=124/245 loss=1.0652254819869995 took 0.37320542335510254 secs\n",
      "epoch=3/3 iteration=125/245 loss=2.2285265922546387 took 0.375307559967041 secs\n",
      "epoch=3/3 iteration=126/245 loss=1.388413429260254 took 0.3834567070007324 secs\n",
      "epoch=3/3 iteration=127/245 loss=1.404205322265625 took 0.48751068115234375 secs\n",
      "epoch=3/3 iteration=128/245 loss=1.0084136724472046 took 0.5342772006988525 secs\n",
      "epoch=3/3 iteration=129/245 loss=1.0050958395004272 took 0.3399631977081299 secs\n",
      "epoch=3/3 iteration=130/245 loss=1.1207984685897827 took 0.3846287727355957 secs\n",
      "epoch=3/3 iteration=131/245 loss=0.9409871101379395 took 0.3467576503753662 secs\n",
      "epoch=3/3 iteration=132/245 loss=0.6179242730140686 took 0.34954166412353516 secs\n",
      "epoch=3/3 iteration=133/245 loss=0.8418902158737183 took 0.38870954513549805 secs\n",
      "epoch=3/3 iteration=134/245 loss=1.0859664678573608 took 0.3515052795410156 secs\n",
      "epoch=3/3 iteration=135/245 loss=0.7880040407180786 took 0.37153172492980957 secs\n",
      "epoch=3/3 iteration=136/245 loss=1.0719983577728271 took 0.3413999080657959 secs\n",
      "epoch=3/3 iteration=137/245 loss=0.9338485598564148 took 0.39543628692626953 secs\n",
      "epoch=3/3 iteration=138/245 loss=1.0440945625305176 took 0.3655412197113037 secs\n",
      "epoch=3/3 iteration=139/245 loss=0.7184641361236572 took 0.3409738540649414 secs\n",
      "epoch=3/3 iteration=140/245 loss=1.229184865951538 took 0.3624091148376465 secs\n",
      "epoch=3/3 iteration=141/245 loss=1.4902652502059937 took 0.33800435066223145 secs\n",
      "epoch=3/3 iteration=142/245 loss=1.1481513977050781 took 0.37770915031433105 secs\n",
      "epoch=3/3 iteration=143/245 loss=1.0654873847961426 took 0.5920493602752686 secs\n",
      "epoch=3/3 iteration=144/245 loss=0.26179981231689453 took 0.5592007637023926 secs\n",
      "epoch=3/3 iteration=145/245 loss=0.5326132774353027 took 0.5107266902923584 secs\n",
      "epoch=3/3 iteration=146/245 loss=0.6671057939529419 took 0.6684043407440186 secs\n",
      "epoch=3/3 iteration=147/245 loss=0.5104815363883972 took 0.5834176540374756 secs\n",
      "epoch=3/3 iteration=148/245 loss=0.42143720388412476 took 0.5395195484161377 secs\n",
      "epoch=3/3 iteration=149/245 loss=0.8098065853118896 took 0.5822038650512695 secs\n",
      "epoch=3/3 iteration=150/245 loss=0.6004278659820557 took 0.9223661422729492 secs\n",
      "epoch=3/3 iteration=151/245 loss=0.7913246750831604 took 0.7018537521362305 secs\n",
      "epoch=3/3 iteration=152/245 loss=0.2709542512893677 took 0.6688973903656006 secs\n",
      "epoch=3/3 iteration=153/245 loss=0.48944056034088135 took 0.6730389595031738 secs\n",
      "epoch=3/3 iteration=154/245 loss=0.5896191596984863 took 0.6137235164642334 secs\n",
      "epoch=3/3 iteration=155/245 loss=0.6142138242721558 took 0.7324225902557373 secs\n",
      "epoch=3/3 iteration=156/245 loss=0.8631542325019836 took 0.6111905574798584 secs\n",
      "epoch=3/3 iteration=157/245 loss=0.6209844946861267 took 0.5468101501464844 secs\n",
      "epoch=3/3 iteration=158/245 loss=0.4657716155052185 took 0.6034598350524902 secs\n",
      "epoch=3/3 iteration=159/245 loss=0.3181367516517639 took 0.665919303894043 secs\n",
      "epoch=3/3 iteration=160/245 loss=0.5007990598678589 took 0.6875853538513184 secs\n",
      "epoch=3/3 iteration=161/245 loss=0.5645443201065063 took 0.7206387519836426 secs\n",
      "epoch=3/3 iteration=162/245 loss=0.8996108174324036 took 0.4842650890350342 secs\n",
      "epoch=3/3 iteration=163/245 loss=1.1406266689300537 took 0.30759096145629883 secs\n",
      "epoch=3/3 iteration=164/245 loss=1.3978182077407837 took 0.37497758865356445 secs\n",
      "epoch=3/3 iteration=165/245 loss=1.1797575950622559 took 0.7082970142364502 secs\n",
      "epoch=3/3 iteration=166/245 loss=0.6671737432479858 took 0.372286319732666 secs\n",
      "epoch=3/3 iteration=167/245 loss=1.118744134902954 took 0.29309868812561035 secs\n",
      "epoch=3/3 iteration=168/245 loss=1.1637299060821533 took 0.3739657402038574 secs\n",
      "epoch=3/3 iteration=169/245 loss=0.6363143920898438 took 0.31929588317871094 secs\n",
      "epoch=3/3 iteration=170/245 loss=0.9816732406616211 took 0.3062450885772705 secs\n",
      "epoch=3/3 iteration=171/245 loss=1.0372692346572876 took 0.33936166763305664 secs\n",
      "epoch=3/3 iteration=172/245 loss=1.115505337715149 took 0.4855778217315674 secs\n",
      "epoch=3/3 iteration=173/245 loss=1.459077000617981 took 0.3417489528656006 secs\n",
      "epoch=3/3 iteration=174/245 loss=1.2970213890075684 took 0.2902247905731201 secs\n",
      "epoch=3/3 iteration=175/245 loss=1.4558113813400269 took 0.2796037197113037 secs\n",
      "epoch=3/3 iteration=176/245 loss=1.148843765258789 took 0.33695197105407715 secs\n",
      "epoch=3/3 iteration=177/245 loss=1.0706413984298706 took 0.44159412384033203 secs\n",
      "epoch=3/3 iteration=178/245 loss=0.5829481482505798 took 0.28682589530944824 secs\n",
      "epoch=3/3 iteration=179/245 loss=1.3588157892227173 took 0.3341684341430664 secs\n",
      "epoch=3/3 iteration=180/245 loss=0.8943777680397034 took 0.2987091541290283 secs\n",
      "epoch=3/3 iteration=181/245 loss=1.18816339969635 took 0.3166372776031494 secs\n",
      "epoch=3/3 iteration=182/245 loss=1.4208509922027588 took 0.306396484375 secs\n",
      "epoch=3/3 iteration=183/245 loss=0.6462451219558716 took 0.4285392761230469 secs\n",
      "epoch=3/3 iteration=184/245 loss=1.0894724130630493 took 0.28533387184143066 secs\n",
      "epoch=3/3 iteration=185/245 loss=1.0233333110809326 took 0.2765920162200928 secs\n",
      "epoch=3/3 iteration=186/245 loss=1.6446095705032349 took 0.3008434772491455 secs\n",
      "epoch=3/3 iteration=187/245 loss=0.9561251401901245 took 0.3705868721008301 secs\n",
      "epoch=3/3 iteration=188/245 loss=0.742845892906189 took 0.3116593360900879 secs\n",
      "epoch=3/3 iteration=189/245 loss=1.1175068616867065 took 0.33115053176879883 secs\n",
      "epoch=3/3 iteration=190/245 loss=0.7513534426689148 took 0.3238060474395752 secs\n",
      "epoch=3/3 iteration=191/245 loss=1.3140416145324707 took 0.3163774013519287 secs\n",
      "epoch=3/3 iteration=192/245 loss=1.1708966493606567 took 0.30297088623046875 secs\n",
      "epoch=3/3 iteration=193/245 loss=1.6891168355941772 took 0.3865323066711426 secs\n",
      "epoch=3/3 iteration=194/245 loss=0.6548908352851868 took 0.3039357662200928 secs\n",
      "epoch=3/3 iteration=195/245 loss=0.8219928741455078 took 0.2820124626159668 secs\n",
      "epoch=3/3 iteration=196/245 loss=1.3258111476898193 took 0.3048427104949951 secs\n",
      "epoch=3/3 iteration=197/245 loss=1.1051021814346313 took 0.29515910148620605 secs\n",
      "epoch=3/3 iteration=198/245 loss=1.482466220855713 took 0.35074591636657715 secs\n",
      "epoch=3/3 iteration=199/245 loss=1.394439697265625 took 0.43208789825439453 secs\n",
      "epoch=3/3 iteration=200/245 loss=0.7716354131698608 took 0.31195950508117676 secs\n",
      "epoch=3/3 iteration=201/245 loss=0.9618585705757141 took 0.30581188201904297 secs\n",
      "epoch=3/3 iteration=202/245 loss=1.053972601890564 took 0.3224024772644043 secs\n",
      "epoch=3/3 iteration=203/245 loss=0.9005081653594971 took 0.32837724685668945 secs\n",
      "epoch=3/3 iteration=204/245 loss=1.2454297542572021 took 0.34668540954589844 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3/3 iteration=205/245 loss=1.8633840084075928 took 0.2967853546142578 secs\n",
      "epoch=3/3 iteration=206/245 loss=1.663746953010559 took 0.391430139541626 secs\n",
      "epoch=3/3 iteration=207/245 loss=1.3507640361785889 took 0.29930734634399414 secs\n",
      "epoch=3/3 iteration=208/245 loss=1.3213552236557007 took 0.27230000495910645 secs\n",
      "epoch=3/3 iteration=209/245 loss=1.6973134279251099 took 0.40332984924316406 secs\n",
      "epoch=3/3 iteration=210/245 loss=1.343172311782837 took 0.37693357467651367 secs\n",
      "epoch=3/3 iteration=211/245 loss=0.8760389089584351 took 0.5765392780303955 secs\n",
      "epoch=3/3 iteration=212/245 loss=1.4025315046310425 took 0.2912123203277588 secs\n",
      "epoch=3/3 iteration=213/245 loss=1.35648512840271 took 0.309220552444458 secs\n",
      "epoch=3/3 iteration=214/245 loss=1.7692828178405762 took 0.33842039108276367 secs\n",
      "epoch=3/3 iteration=215/245 loss=0.8352620601654053 took 0.3434717655181885 secs\n",
      "epoch=3/3 iteration=216/245 loss=1.2677700519561768 took 0.2976102828979492 secs\n",
      "epoch=3/3 iteration=217/245 loss=1.1387391090393066 took 0.32904553413391113 secs\n",
      "epoch=3/3 iteration=218/245 loss=1.505990982055664 took 0.3089437484741211 secs\n",
      "epoch=3/3 iteration=219/245 loss=2.0511412620544434 took 0.2909388542175293 secs\n",
      "epoch=3/3 iteration=220/245 loss=1.3903948068618774 took 0.33403539657592773 secs\n",
      "epoch=3/3 iteration=221/245 loss=0.7693547010421753 took 0.30246567726135254 secs\n",
      "epoch=3/3 iteration=222/245 loss=1.379050850868225 took 0.34169578552246094 secs\n",
      "epoch=3/3 iteration=223/245 loss=1.410735845565796 took 0.32923388481140137 secs\n",
      "epoch=3/3 iteration=224/245 loss=1.8494372367858887 took 0.2896997928619385 secs\n",
      "epoch=3/3 iteration=225/245 loss=1.620600938796997 took 0.2964627742767334 secs\n",
      "epoch=3/3 iteration=226/245 loss=0.9418888092041016 took 0.3061032295227051 secs\n",
      "epoch=3/3 iteration=227/245 loss=0.9205689430236816 took 0.34311962127685547 secs\n",
      "epoch=3/3 iteration=228/245 loss=1.56023371219635 took 0.34049057960510254 secs\n",
      "epoch=3/3 iteration=229/245 loss=1.6695210933685303 took 0.29982709884643555 secs\n",
      "epoch=3/3 iteration=230/245 loss=1.225379467010498 took 0.43790197372436523 secs\n",
      "epoch=3/3 iteration=231/245 loss=1.352595567703247 took 0.3207590579986572 secs\n",
      "epoch=3/3 iteration=232/245 loss=1.1370713710784912 took 0.30530714988708496 secs\n",
      "epoch=3/3 iteration=233/245 loss=0.9919671416282654 took 0.3117859363555908 secs\n",
      "epoch=3/3 iteration=234/245 loss=0.1456693857908249 took 0.3164024353027344 secs\n",
      "epoch=3/3 iteration=235/245 loss=0.22335313260555267 took 0.28046536445617676 secs\n",
      "epoch=3/3 iteration=236/245 loss=0.3526710867881775 took 0.30559825897216797 secs\n",
      "epoch=3/3 iteration=237/245 loss=0.5901209115982056 took 0.28192877769470215 secs\n",
      "epoch=3/3 iteration=238/245 loss=1.5307799577713013 took 0.29517412185668945 secs\n",
      "epoch=3/3 iteration=239/245 loss=1.4528740644454956 took 0.32660412788391113 secs\n",
      "epoch=3/3 iteration=240/245 loss=1.1030312776565552 took 0.29410600662231445 secs\n",
      "epoch=3/3 iteration=241/245 loss=1.4509236812591553 took 0.3178718090057373 secs\n",
      "epoch=3/3 iteration=242/245 loss=1.983812928199768 took 0.3668487071990967 secs\n",
      "epoch=3/3 iteration=243/245 loss=2.6546695232391357 took 0.3260643482208252 secs\n",
      "epoch=3/3 iteration=244/245 loss=3.110689163208008 took 0.37732410430908203 secs\n",
      "epoch=3/3 iteration=245/245 loss=3.5261690616607666 took 0.5030193328857422 secs\n",
      "epoch=4/3 iteration=1/245 loss=0.6317876577377319 took 0.32086920738220215 secs\n",
      "epoch=4/3 iteration=2/245 loss=0.40503430366516113 took 0.26546216011047363 secs\n",
      "epoch=4/3 iteration=3/245 loss=0.8074449300765991 took 0.2535109519958496 secs\n",
      "epoch=4/3 iteration=4/245 loss=0.6414063572883606 took 0.2707996368408203 secs\n",
      "epoch=4/3 iteration=5/245 loss=0.7490246295928955 took 0.2829101085662842 secs\n",
      "epoch=4/3 iteration=6/245 loss=1.1598474979400635 took 0.28077054023742676 secs\n",
      "epoch=4/3 iteration=7/245 loss=0.6878429651260376 took 0.22585058212280273 secs\n",
      "epoch=4/3 iteration=8/245 loss=1.2139747142791748 took 0.25775980949401855 secs\n",
      "epoch=4/3 iteration=9/245 loss=1.171829104423523 took 0.3306002616882324 secs\n",
      "epoch=4/3 iteration=10/245 loss=0.41787710785865784 took 0.3161029815673828 secs\n",
      "epoch=4/3 iteration=11/245 loss=1.5670967102050781 took 0.27321720123291016 secs\n",
      "epoch=4/3 iteration=12/245 loss=0.9616432189941406 took 0.2723417282104492 secs\n",
      "epoch=4/3 iteration=13/245 loss=1.3129150867462158 took 0.2988698482513428 secs\n",
      "epoch=4/3 iteration=14/245 loss=2.1157829761505127 took 0.2512168884277344 secs\n",
      "epoch=4/3 iteration=15/245 loss=2.1167104244232178 took 0.2652149200439453 secs\n",
      "epoch=4/3 iteration=16/245 loss=1.4512752294540405 took 0.26058340072631836 secs\n",
      "epoch=4/3 iteration=17/245 loss=1.5859370231628418 took 0.291820764541626 secs\n",
      "epoch=4/3 iteration=18/245 loss=2.2434401512145996 took 0.293079137802124 secs\n",
      "epoch=4/3 iteration=19/245 loss=1.2261936664581299 took 0.31502246856689453 secs\n",
      "epoch=4/3 iteration=20/245 loss=1.8164570331573486 took 0.2837541103363037 secs\n",
      "epoch=4/3 iteration=21/245 loss=2.459585666656494 took 0.3061645030975342 secs\n",
      "epoch=4/3 iteration=22/245 loss=1.9609317779541016 took 0.30919361114501953 secs\n",
      "epoch=4/3 iteration=23/245 loss=2.765601396560669 took 0.2773754596710205 secs\n",
      "epoch=4/3 iteration=24/245 loss=1.559478998184204 took 0.2803783416748047 secs\n",
      "epoch=4/3 iteration=25/245 loss=1.9537239074707031 took 0.3282966613769531 secs\n",
      "epoch=4/3 iteration=26/245 loss=1.9536402225494385 took 0.3398253917694092 secs\n",
      "epoch=4/3 iteration=27/245 loss=2.3111202716827393 took 0.30797815322875977 secs\n",
      "epoch=4/3 iteration=28/245 loss=1.8514723777770996 took 0.3352088928222656 secs\n",
      "epoch=4/3 iteration=29/245 loss=2.6450657844543457 took 0.3055102825164795 secs\n",
      "epoch=4/3 iteration=30/245 loss=2.3683416843414307 took 0.34221625328063965 secs\n",
      "epoch=4/3 iteration=31/245 loss=2.17350697517395 took 0.3529665470123291 secs\n",
      "epoch=4/3 iteration=32/245 loss=0.5071164965629578 took 0.30046653747558594 secs\n",
      "epoch=4/3 iteration=33/245 loss=0.6626562476158142 took 0.275667667388916 secs\n",
      "epoch=4/3 iteration=34/245 loss=0.4839055836200714 took 0.2621116638183594 secs\n",
      "epoch=4/3 iteration=35/245 loss=0.9740607738494873 took 0.2804107666015625 secs\n",
      "epoch=4/3 iteration=36/245 loss=1.1670842170715332 took 0.25992274284362793 secs\n",
      "epoch=4/3 iteration=37/245 loss=1.6963433027267456 took 0.24645590782165527 secs\n",
      "epoch=4/3 iteration=38/245 loss=1.562523365020752 took 0.2829594612121582 secs\n",
      "epoch=4/3 iteration=39/245 loss=1.1038051843643188 took 0.2962305545806885 secs\n",
      "epoch=4/3 iteration=40/245 loss=1.2439626455307007 took 0.26550745964050293 secs\n",
      "epoch=4/3 iteration=41/245 loss=0.9008985757827759 took 0.2766854763031006 secs\n",
      "epoch=4/3 iteration=42/245 loss=1.0236680507659912 took 0.2931520938873291 secs\n",
      "epoch=4/3 iteration=43/245 loss=1.3812085390090942 took 0.30472230911254883 secs\n",
      "epoch=4/3 iteration=44/245 loss=1.936957597732544 took 0.2715036869049072 secs\n",
      "epoch=4/3 iteration=45/245 loss=1.6888904571533203 took 0.27989721298217773 secs\n",
      "epoch=4/3 iteration=46/245 loss=1.8889294862747192 took 0.27960753440856934 secs\n",
      "epoch=4/3 iteration=47/245 loss=1.9985432624816895 took 0.30733323097229004 secs\n",
      "epoch=4/3 iteration=48/245 loss=2.4895668029785156 took 0.25452685356140137 secs\n",
      "epoch=4/3 iteration=49/245 loss=1.0340187549591064 took 0.3354659080505371 secs\n",
      "epoch=4/3 iteration=50/245 loss=1.626272201538086 took 0.3345210552215576 secs\n",
      "epoch=4/3 iteration=51/245 loss=1.6262328624725342 took 0.3253293037414551 secs\n",
      "epoch=4/3 iteration=52/245 loss=2.502931594848633 took 0.2943105697631836 secs\n",
      "epoch=4/3 iteration=53/245 loss=1.372360110282898 took 0.515855073928833 secs\n",
      "epoch=4/3 iteration=54/245 loss=2.624819278717041 took 0.3427417278289795 secs\n",
      "epoch=4/3 iteration=55/245 loss=2.488865375518799 took 0.3817424774169922 secs\n",
      "epoch=4/3 iteration=56/245 loss=1.5731076002120972 took 0.30054450035095215 secs\n",
      "epoch=4/3 iteration=57/245 loss=2.7185418605804443 took 0.3501875400543213 secs\n",
      "epoch=4/3 iteration=58/245 loss=2.380268096923828 took 0.27883148193359375 secs\n",
      "epoch=4/3 iteration=59/245 loss=2.715651750564575 took 0.4024841785430908 secs\n",
      "epoch=4/3 iteration=60/245 loss=2.1023926734924316 took 0.4061238765716553 secs\n",
      "epoch=4/3 iteration=61/245 loss=1.5417671203613281 took 0.48793864250183105 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4/3 iteration=62/245 loss=2.460096597671509 took 0.34749388694763184 secs\n",
      "epoch=4/3 iteration=63/245 loss=1.619706392288208 took 0.39972615242004395 secs\n",
      "epoch=4/3 iteration=64/245 loss=1.4590446949005127 took 0.5216257572174072 secs\n",
      "epoch=4/3 iteration=65/245 loss=1.2528035640716553 took 0.33443593978881836 secs\n",
      "epoch=4/3 iteration=66/245 loss=1.5406498908996582 took 0.41724061965942383 secs\n",
      "epoch=4/3 iteration=67/245 loss=0.9954672455787659 took 0.356950044631958 secs\n",
      "epoch=4/3 iteration=68/245 loss=1.2097232341766357 took 0.3294053077697754 secs\n",
      "epoch=4/3 iteration=69/245 loss=1.3291305303573608 took 0.39972782135009766 secs\n",
      "epoch=4/3 iteration=70/245 loss=1.4150139093399048 took 0.36209535598754883 secs\n",
      "epoch=4/3 iteration=71/245 loss=1.2583255767822266 took 0.35492801666259766 secs\n",
      "epoch=4/3 iteration=72/245 loss=2.0546653270721436 took 0.3408234119415283 secs\n",
      "epoch=4/3 iteration=73/245 loss=1.1334202289581299 took 0.3529696464538574 secs\n",
      "epoch=4/3 iteration=74/245 loss=1.2434066534042358 took 0.36426830291748047 secs\n",
      "epoch=4/3 iteration=75/245 loss=1.8451199531555176 took 0.33687257766723633 secs\n",
      "epoch=4/3 iteration=76/245 loss=0.9360297322273254 took 0.39299798011779785 secs\n",
      "epoch=4/3 iteration=77/245 loss=1.5961761474609375 took 0.34896278381347656 secs\n",
      "epoch=4/3 iteration=78/245 loss=1.268425464630127 took 0.32566332817077637 secs\n",
      "epoch=4/3 iteration=79/245 loss=1.3041671514511108 took 0.38437843322753906 secs\n",
      "epoch=4/3 iteration=80/245 loss=1.68022882938385 took 0.35783863067626953 secs\n",
      "epoch=4/3 iteration=81/245 loss=1.3352150917053223 took 0.3181331157684326 secs\n",
      "epoch=4/3 iteration=82/245 loss=0.9329524636268616 took 0.349963903427124 secs\n",
      "epoch=4/3 iteration=83/245 loss=1.0506402254104614 took 0.43334126472473145 secs\n",
      "epoch=4/3 iteration=84/245 loss=1.0056240558624268 took 0.3501260280609131 secs\n",
      "epoch=4/3 iteration=85/245 loss=1.2278717756271362 took 0.3302440643310547 secs\n",
      "epoch=4/3 iteration=86/245 loss=0.7896965742111206 took 0.3802340030670166 secs\n",
      "epoch=4/3 iteration=87/245 loss=1.6048855781555176 took 0.40679144859313965 secs\n",
      "epoch=4/3 iteration=88/245 loss=0.7454159259796143 took 0.3576667308807373 secs\n",
      "epoch=4/3 iteration=89/245 loss=1.3128525018692017 took 0.33157801628112793 secs\n",
      "epoch=4/3 iteration=90/245 loss=0.41705232858657837 took 0.35577988624572754 secs\n",
      "epoch=4/3 iteration=91/245 loss=0.9978143572807312 took 0.31160902976989746 secs\n",
      "epoch=4/3 iteration=92/245 loss=0.5317227244377136 took 0.38702845573425293 secs\n",
      "epoch=4/3 iteration=93/245 loss=1.524050235748291 took 0.35611987113952637 secs\n",
      "epoch=4/3 iteration=94/245 loss=1.2654587030410767 took 0.3495309352874756 secs\n",
      "epoch=4/3 iteration=95/245 loss=1.5363738536834717 took 0.3628356456756592 secs\n",
      "epoch=4/3 iteration=96/245 loss=0.9857749342918396 took 0.33870577812194824 secs\n",
      "epoch=4/3 iteration=97/245 loss=1.3299429416656494 took 0.502528190612793 secs\n",
      "epoch=4/3 iteration=98/245 loss=1.5523643493652344 took 0.4260244369506836 secs\n",
      "epoch=4/3 iteration=99/245 loss=1.7294671535491943 took 0.35669994354248047 secs\n",
      "epoch=4/3 iteration=100/245 loss=1.072890043258667 took 0.3539302349090576 secs\n",
      "epoch=4/3 iteration=101/245 loss=1.500576138496399 took 0.3826935291290283 secs\n",
      "epoch=4/3 iteration=102/245 loss=1.2924796342849731 took 0.344942569732666 secs\n",
      "epoch=4/3 iteration=103/245 loss=1.4185011386871338 took 0.4043605327606201 secs\n",
      "epoch=4/3 iteration=104/245 loss=1.8541018962860107 took 0.3993351459503174 secs\n",
      "epoch=4/3 iteration=105/245 loss=0.5333935618400574 took 0.30675625801086426 secs\n",
      "epoch=4/3 iteration=106/245 loss=1.7568429708480835 took 0.40398669242858887 secs\n",
      "epoch=4/3 iteration=107/245 loss=1.7404077053070068 took 0.3525519371032715 secs\n",
      "epoch=4/3 iteration=108/245 loss=1.4541065692901611 took 0.4760880470275879 secs\n",
      "epoch=4/3 iteration=109/245 loss=1.3685332536697388 took 0.3583664894104004 secs\n",
      "epoch=4/3 iteration=110/245 loss=1.5064393281936646 took 0.4635906219482422 secs\n",
      "epoch=4/3 iteration=111/245 loss=1.4782792329788208 took 0.41082334518432617 secs\n",
      "epoch=4/3 iteration=112/245 loss=1.9168812036514282 took 0.4765141010284424 secs\n",
      "epoch=4/3 iteration=113/245 loss=1.9804006814956665 took 0.4129006862640381 secs\n",
      "epoch=4/3 iteration=114/245 loss=0.8919795155525208 took 0.3698709011077881 secs\n",
      "epoch=4/3 iteration=115/245 loss=1.190278172492981 took 0.3754277229309082 secs\n",
      "epoch=4/3 iteration=116/245 loss=1.4631164073944092 took 0.37082862854003906 secs\n",
      "epoch=4/3 iteration=117/245 loss=1.1344376802444458 took 0.35268616676330566 secs\n",
      "epoch=4/3 iteration=118/245 loss=1.2500687837600708 took 0.3488795757293701 secs\n",
      "epoch=4/3 iteration=119/245 loss=1.195204257965088 took 0.39337873458862305 secs\n",
      "epoch=4/3 iteration=120/245 loss=1.2785017490386963 took 0.42474818229675293 secs\n",
      "epoch=4/3 iteration=121/245 loss=1.6705735921859741 took 0.32726526260375977 secs\n",
      "epoch=4/3 iteration=122/245 loss=1.6183596849441528 took 0.36777687072753906 secs\n",
      "epoch=4/3 iteration=123/245 loss=1.564657211303711 took 0.35354018211364746 secs\n",
      "epoch=4/3 iteration=124/245 loss=1.0654399394989014 took 0.38657069206237793 secs\n",
      "epoch=4/3 iteration=125/245 loss=2.228980779647827 took 0.368654727935791 secs\n",
      "epoch=4/3 iteration=126/245 loss=1.3891050815582275 took 0.3829810619354248 secs\n",
      "epoch=4/3 iteration=127/245 loss=1.4037238359451294 took 0.6523458957672119 secs\n",
      "epoch=4/3 iteration=128/245 loss=1.0090500116348267 took 0.3571653366088867 secs\n",
      "epoch=4/3 iteration=129/245 loss=1.0057554244995117 took 0.35373997688293457 secs\n",
      "epoch=4/3 iteration=130/245 loss=1.1170727014541626 took 0.40256786346435547 secs\n",
      "epoch=4/3 iteration=131/245 loss=0.9416845440864563 took 0.32726001739501953 secs\n",
      "epoch=4/3 iteration=132/245 loss=0.6196304559707642 took 0.4482917785644531 secs\n",
      "epoch=4/3 iteration=133/245 loss=0.8406251072883606 took 0.38447093963623047 secs\n",
      "epoch=4/3 iteration=134/245 loss=1.0844532251358032 took 0.3578517436981201 secs\n",
      "epoch=4/3 iteration=135/245 loss=0.7883985638618469 took 0.3567368984222412 secs\n",
      "epoch=4/3 iteration=136/245 loss=1.0732084512710571 took 0.34685206413269043 secs\n",
      "epoch=4/3 iteration=137/245 loss=0.9334791302680969 took 0.40906381607055664 secs\n",
      "epoch=4/3 iteration=138/245 loss=1.0416408777236938 took 0.35901403427124023 secs\n",
      "epoch=4/3 iteration=139/245 loss=0.7196376919746399 took 0.373807430267334 secs\n",
      "epoch=4/3 iteration=140/245 loss=1.2280837297439575 took 0.37427616119384766 secs\n",
      "epoch=4/3 iteration=141/245 loss=1.4889887571334839 took 0.33650946617126465 secs\n",
      "epoch=4/3 iteration=142/245 loss=1.148133635520935 took 0.3882145881652832 secs\n",
      "epoch=4/3 iteration=143/245 loss=1.064730167388916 took 0.5703907012939453 secs\n",
      "epoch=4/3 iteration=144/245 loss=0.2620825171470642 took 0.43738794326782227 secs\n",
      "epoch=4/3 iteration=145/245 loss=0.5327253937721252 took 0.5054459571838379 secs\n",
      "epoch=4/3 iteration=146/245 loss=0.6674529314041138 took 0.6650686264038086 secs\n",
      "epoch=4/3 iteration=147/245 loss=0.5104494690895081 took 0.5930578708648682 secs\n",
      "epoch=4/3 iteration=148/245 loss=0.4219609797000885 took 0.5508537292480469 secs\n",
      "epoch=4/3 iteration=149/245 loss=0.8111733794212341 took 0.6030876636505127 secs\n",
      "epoch=4/3 iteration=150/245 loss=0.600227952003479 took 0.7396831512451172 secs\n",
      "epoch=4/3 iteration=151/245 loss=0.7899258732795715 took 0.7104318141937256 secs\n",
      "epoch=4/3 iteration=152/245 loss=0.2720393240451813 took 0.7771124839782715 secs\n",
      "epoch=4/3 iteration=153/245 loss=0.48941969871520996 took 0.5954244136810303 secs\n",
      "epoch=4/3 iteration=154/245 loss=0.5877820253372192 took 0.5998523235321045 secs\n",
      "epoch=4/3 iteration=155/245 loss=0.6148616075515747 took 0.8396933078765869 secs\n",
      "epoch=4/3 iteration=156/245 loss=0.8611909747123718 took 0.6328887939453125 secs\n",
      "epoch=4/3 iteration=157/245 loss=0.6206331849098206 took 0.6121912002563477 secs\n",
      "epoch=4/3 iteration=158/245 loss=0.4639469385147095 took 0.5748870372772217 secs\n",
      "epoch=4/3 iteration=159/245 loss=0.31811219453811646 took 0.6578104496002197 secs\n",
      "epoch=4/3 iteration=160/245 loss=0.5012112259864807 took 0.666245698928833 secs\n",
      "epoch=4/3 iteration=161/245 loss=0.5641347169876099 took 0.7151165008544922 secs\n",
      "epoch=4/3 iteration=162/245 loss=0.9003428816795349 took 0.5070974826812744 secs\n",
      "epoch=4/3 iteration=163/245 loss=1.1403781175613403 took 0.31200528144836426 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4/3 iteration=164/245 loss=1.4023500680923462 took 0.36707353591918945 secs\n",
      "epoch=4/3 iteration=165/245 loss=1.1764236688613892 took 0.6982388496398926 secs\n",
      "epoch=4/3 iteration=166/245 loss=0.6691382527351379 took 0.3607752323150635 secs\n",
      "epoch=4/3 iteration=167/245 loss=1.1201788187026978 took 0.2885754108428955 secs\n",
      "epoch=4/3 iteration=168/245 loss=1.1639530658721924 took 0.38405919075012207 secs\n",
      "epoch=4/3 iteration=169/245 loss=0.6361140608787537 took 0.31299757957458496 secs\n",
      "epoch=4/3 iteration=170/245 loss=0.982347846031189 took 0.29977965354919434 secs\n",
      "epoch=4/3 iteration=171/245 loss=1.0384398698806763 took 0.33462071418762207 secs\n",
      "epoch=4/3 iteration=172/245 loss=1.1163216829299927 took 0.3148176670074463 secs\n",
      "epoch=4/3 iteration=173/245 loss=1.4589141607284546 took 0.3712575435638428 secs\n",
      "epoch=4/3 iteration=174/245 loss=1.2983882427215576 took 0.2595553398132324 secs\n",
      "epoch=4/3 iteration=175/245 loss=1.4543100595474243 took 0.2944362163543701 secs\n",
      "epoch=4/3 iteration=176/245 loss=1.1476248502731323 took 0.400005578994751 secs\n",
      "epoch=4/3 iteration=177/245 loss=1.0709868669509888 took 0.40269970893859863 secs\n",
      "epoch=4/3 iteration=178/245 loss=0.5824937224388123 took 0.29752659797668457 secs\n",
      "epoch=4/3 iteration=179/245 loss=1.356589674949646 took 0.30788278579711914 secs\n",
      "epoch=4/3 iteration=180/245 loss=0.8940743803977966 took 0.30567264556884766 secs\n",
      "epoch=4/3 iteration=181/245 loss=1.1910529136657715 took 0.3066587448120117 secs\n",
      "epoch=4/3 iteration=182/245 loss=1.4201327562332153 took 0.3989875316619873 secs\n",
      "epoch=4/3 iteration=183/245 loss=0.6438472867012024 took 0.38292670249938965 secs\n",
      "epoch=4/3 iteration=184/245 loss=1.0881366729736328 took 0.28350305557250977 secs\n",
      "epoch=4/3 iteration=185/245 loss=1.023036241531372 took 0.2990572452545166 secs\n",
      "epoch=4/3 iteration=186/245 loss=1.6459814310073853 took 0.3092508316040039 secs\n",
      "epoch=4/3 iteration=187/245 loss=0.958496630191803 took 0.35210227966308594 secs\n",
      "epoch=4/3 iteration=188/245 loss=0.746211051940918 took 0.316373348236084 secs\n",
      "epoch=4/3 iteration=189/245 loss=1.1174006462097168 took 0.3277013301849365 secs\n",
      "epoch=4/3 iteration=190/245 loss=0.7503498196601868 took 0.32137393951416016 secs\n",
      "epoch=4/3 iteration=191/245 loss=1.3128701448440552 took 0.2969207763671875 secs\n",
      "epoch=4/3 iteration=192/245 loss=1.1720167398452759 took 0.30028796195983887 secs\n",
      "epoch=4/3 iteration=193/245 loss=1.689087986946106 took 0.3853163719177246 secs\n",
      "epoch=4/3 iteration=194/245 loss=0.6558959484100342 took 0.3154451847076416 secs\n",
      "epoch=4/3 iteration=195/245 loss=0.8231474161148071 took 0.2788577079772949 secs\n",
      "epoch=4/3 iteration=196/245 loss=1.3240042924880981 took 0.28939080238342285 secs\n",
      "epoch=4/3 iteration=197/245 loss=1.105299472808838 took 0.2989201545715332 secs\n",
      "epoch=4/3 iteration=198/245 loss=1.482738971710205 took 0.3350064754486084 secs\n",
      "epoch=4/3 iteration=199/245 loss=1.3944898843765259 took 0.4256563186645508 secs\n",
      "epoch=4/3 iteration=200/245 loss=0.7729284763336182 took 0.3019561767578125 secs\n",
      "epoch=4/3 iteration=201/245 loss=0.9599959850311279 took 0.29871296882629395 secs\n",
      "epoch=4/3 iteration=202/245 loss=1.0547430515289307 took 0.3192753791809082 secs\n",
      "epoch=4/3 iteration=203/245 loss=0.8991845846176147 took 0.3277456760406494 secs\n",
      "epoch=4/3 iteration=204/245 loss=1.2459501028060913 took 0.3516428470611572 secs\n",
      "epoch=4/3 iteration=205/245 loss=1.8629512786865234 took 0.2978987693786621 secs\n",
      "epoch=4/3 iteration=206/245 loss=1.667520523071289 took 0.3170158863067627 secs\n",
      "epoch=4/3 iteration=207/245 loss=1.3478224277496338 took 0.3167133331298828 secs\n",
      "epoch=4/3 iteration=208/245 loss=1.320055603981018 took 0.29653143882751465 secs\n",
      "epoch=4/3 iteration=209/245 loss=1.6969304084777832 took 0.40924906730651855 secs\n",
      "epoch=4/3 iteration=210/245 loss=1.3391627073287964 took 0.4264342784881592 secs\n",
      "epoch=4/3 iteration=211/245 loss=0.8776711225509644 took 0.3546431064605713 secs\n",
      "epoch=4/3 iteration=212/245 loss=1.402194619178772 took 0.28708744049072266 secs\n",
      "epoch=4/3 iteration=213/245 loss=1.3562853336334229 took 0.2922959327697754 secs\n",
      "epoch=4/3 iteration=214/245 loss=1.7675153017044067 took 0.34273242950439453 secs\n",
      "epoch=4/3 iteration=215/245 loss=0.8352062106132507 took 0.5706257820129395 secs\n",
      "epoch=4/3 iteration=216/245 loss=1.2699370384216309 took 0.30898571014404297 secs\n",
      "epoch=4/3 iteration=217/245 loss=1.1401114463806152 took 0.32389092445373535 secs\n",
      "epoch=4/3 iteration=218/245 loss=1.5055878162384033 took 0.31539368629455566 secs\n",
      "epoch=4/3 iteration=219/245 loss=2.0531463623046875 took 0.3019981384277344 secs\n",
      "epoch=4/3 iteration=220/245 loss=1.3918160200119019 took 0.3353140354156494 secs\n",
      "epoch=4/3 iteration=221/245 loss=0.7671836614608765 took 0.3007321357727051 secs\n",
      "epoch=4/3 iteration=222/245 loss=1.3786396980285645 took 0.40396928787231445 secs\n",
      "epoch=4/3 iteration=223/245 loss=1.413184404373169 took 0.3237614631652832 secs\n",
      "epoch=4/3 iteration=224/245 loss=1.849916934967041 took 0.29995155334472656 secs\n",
      "epoch=4/3 iteration=225/245 loss=1.6196765899658203 took 0.3057262897491455 secs\n",
      "epoch=4/3 iteration=226/245 loss=0.9408489465713501 took 0.31142687797546387 secs\n",
      "epoch=4/3 iteration=227/245 loss=0.9224714040756226 took 0.33740878105163574 secs\n",
      "epoch=4/3 iteration=228/245 loss=1.5562126636505127 took 0.35842156410217285 secs\n",
      "epoch=4/3 iteration=229/245 loss=1.6680690050125122 took 0.3092799186706543 secs\n",
      "epoch=4/3 iteration=230/245 loss=1.2244513034820557 took 0.4239921569824219 secs\n",
      "epoch=4/3 iteration=231/245 loss=1.3510375022888184 took 0.3262643814086914 secs\n",
      "epoch=4/3 iteration=232/245 loss=1.135271430015564 took 0.3183934688568115 secs\n",
      "epoch=4/3 iteration=233/245 loss=0.9909095168113708 took 0.30530881881713867 secs\n",
      "epoch=4/3 iteration=234/245 loss=0.14446887373924255 took 0.3176276683807373 secs\n",
      "epoch=4/3 iteration=235/245 loss=0.22320541739463806 took 0.2769043445587158 secs\n",
      "epoch=4/3 iteration=236/245 loss=0.35163840651512146 took 0.3261902332305908 secs\n",
      "epoch=4/3 iteration=237/245 loss=0.5879765748977661 took 0.3039257526397705 secs\n",
      "epoch=4/3 iteration=238/245 loss=1.5286754369735718 took 0.29732227325439453 secs\n",
      "epoch=4/3 iteration=239/245 loss=1.4527915716171265 took 0.35695433616638184 secs\n",
      "epoch=4/3 iteration=240/245 loss=1.1058433055877686 took 0.29505228996276855 secs\n",
      "epoch=4/3 iteration=241/245 loss=1.4508812427520752 took 0.32244420051574707 secs\n",
      "epoch=4/3 iteration=242/245 loss=1.9868190288543701 took 0.33341431617736816 secs\n",
      "epoch=4/3 iteration=243/245 loss=2.652534008026123 took 0.40802597999572754 secs\n",
      "epoch=4/3 iteration=244/245 loss=3.1088452339172363 took 0.3316059112548828 secs\n",
      "epoch=4/3 iteration=245/245 loss=3.52993106842041 took 0.3359401226043701 secs\n",
      "epoch=5/3 iteration=1/245 loss=0.631627082824707 took 0.31048035621643066 secs\n",
      "epoch=5/3 iteration=2/245 loss=0.4048941433429718 took 0.2765157222747803 secs\n",
      "epoch=5/3 iteration=3/245 loss=0.8065428733825684 took 0.2614576816558838 secs\n",
      "epoch=5/3 iteration=4/245 loss=0.6427209973335266 took 0.33358311653137207 secs\n",
      "epoch=5/3 iteration=5/245 loss=0.7478134632110596 took 0.24226903915405273 secs\n",
      "epoch=5/3 iteration=6/245 loss=1.158258080482483 took 0.26244425773620605 secs\n",
      "epoch=5/3 iteration=7/245 loss=0.6878591775894165 took 0.2460470199584961 secs\n",
      "epoch=5/3 iteration=8/245 loss=1.2183388471603394 took 0.2539985179901123 secs\n",
      "epoch=5/3 iteration=9/245 loss=1.1713675260543823 took 0.33907365798950195 secs\n",
      "epoch=5/3 iteration=10/245 loss=0.4180722236633301 took 0.3006572723388672 secs\n",
      "epoch=5/3 iteration=11/245 loss=1.566678762435913 took 0.26657533645629883 secs\n",
      "epoch=5/3 iteration=12/245 loss=0.9645407199859619 took 0.2851541042327881 secs\n",
      "epoch=5/3 iteration=13/245 loss=1.315896987915039 took 0.2800633907318115 secs\n",
      "epoch=5/3 iteration=14/245 loss=2.118011951446533 took 0.2541525363922119 secs\n",
      "epoch=5/3 iteration=15/245 loss=2.1107659339904785 took 0.25861239433288574 secs\n",
      "epoch=5/3 iteration=16/245 loss=1.4506034851074219 took 0.2501235008239746 secs\n",
      "epoch=5/3 iteration=17/245 loss=1.586504578590393 took 0.2827939987182617 secs\n",
      "epoch=5/3 iteration=18/245 loss=2.2426412105560303 took 0.28169798851013184 secs\n",
      "epoch=5/3 iteration=19/245 loss=1.2231926918029785 took 0.31142592430114746 secs\n",
      "epoch=5/3 iteration=20/245 loss=1.8150737285614014 took 0.28939175605773926 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5/3 iteration=21/245 loss=2.4582080841064453 took 0.2923130989074707 secs\n",
      "epoch=5/3 iteration=22/245 loss=1.9621002674102783 took 0.33000826835632324 secs\n",
      "epoch=5/3 iteration=23/245 loss=2.7649903297424316 took 0.26053357124328613 secs\n",
      "epoch=5/3 iteration=24/245 loss=1.5590500831604004 took 0.28804492950439453 secs\n",
      "epoch=5/3 iteration=25/245 loss=1.9557619094848633 took 0.32326388359069824 secs\n",
      "epoch=5/3 iteration=26/245 loss=1.951329231262207 took 0.3515346050262451 secs\n",
      "epoch=5/3 iteration=27/245 loss=2.3096039295196533 took 0.3036689758300781 secs\n",
      "epoch=5/3 iteration=28/245 loss=1.8527941703796387 took 0.3492581844329834 secs\n",
      "epoch=5/3 iteration=29/245 loss=2.6459755897521973 took 0.29360508918762207 secs\n",
      "epoch=5/3 iteration=30/245 loss=2.366961717605591 took 0.3466796875 secs\n",
      "epoch=5/3 iteration=31/245 loss=2.1738088130950928 took 0.3424363136291504 secs\n",
      "epoch=5/3 iteration=32/245 loss=0.5084947347640991 took 0.2595398426055908 secs\n",
      "epoch=5/3 iteration=33/245 loss=0.663967490196228 took 0.2844860553741455 secs\n",
      "epoch=5/3 iteration=34/245 loss=0.48260262608528137 took 0.2602510452270508 secs\n",
      "epoch=5/3 iteration=35/245 loss=0.9744162559509277 took 0.40638113021850586 secs\n",
      "epoch=5/3 iteration=36/245 loss=1.1683645248413086 took 0.2736506462097168 secs\n",
      "epoch=5/3 iteration=37/245 loss=1.6945983171463013 took 0.2623863220214844 secs\n",
      "epoch=5/3 iteration=38/245 loss=1.5660862922668457 took 0.2662534713745117 secs\n",
      "epoch=5/3 iteration=39/245 loss=1.1030621528625488 took 0.29744744300842285 secs\n",
      "epoch=5/3 iteration=40/245 loss=1.2455142736434937 took 0.2436656951904297 secs\n",
      "epoch=5/3 iteration=41/245 loss=0.9007142782211304 took 0.30910778045654297 secs\n",
      "epoch=5/3 iteration=42/245 loss=1.0228928327560425 took 0.2824900150299072 secs\n",
      "epoch=5/3 iteration=43/245 loss=1.3798294067382812 took 0.2930324077606201 secs\n",
      "epoch=5/3 iteration=44/245 loss=1.9393922090530396 took 0.26813316345214844 secs\n",
      "epoch=5/3 iteration=45/245 loss=1.6884573698043823 took 0.2770354747772217 secs\n",
      "epoch=5/3 iteration=46/245 loss=1.8923488855361938 took 0.2681694030761719 secs\n",
      "epoch=5/3 iteration=47/245 loss=1.9989237785339355 took 0.29361414909362793 secs\n",
      "epoch=5/3 iteration=48/245 loss=2.4900174140930176 took 0.2698688507080078 secs\n",
      "epoch=5/3 iteration=49/245 loss=1.0365022420883179 took 0.32325053215026855 secs\n",
      "epoch=5/3 iteration=50/245 loss=1.6293339729309082 took 0.3464696407318115 secs\n",
      "epoch=5/3 iteration=51/245 loss=1.6269842386245728 took 0.3205416202545166 secs\n",
      "epoch=5/3 iteration=52/245 loss=2.5045166015625 took 0.30922985076904297 secs\n",
      "epoch=5/3 iteration=53/245 loss=1.3756343126296997 took 0.51499342918396 secs\n",
      "epoch=5/3 iteration=54/245 loss=2.625467538833618 took 0.30376172065734863 secs\n",
      "epoch=5/3 iteration=55/245 loss=2.4883151054382324 took 0.32418346405029297 secs\n",
      "epoch=5/3 iteration=56/245 loss=1.5734663009643555 took 0.29859209060668945 secs\n",
      "epoch=5/3 iteration=57/245 loss=2.7146787643432617 took 0.3153953552246094 secs\n",
      "epoch=5/3 iteration=58/245 loss=2.378756523132324 took 0.2658884525299072 secs\n",
      "epoch=5/3 iteration=59/245 loss=2.714979410171509 took 0.30660486221313477 secs\n",
      "epoch=5/3 iteration=60/245 loss=2.1011106967926025 took 0.33887386322021484 secs\n",
      "epoch=5/3 iteration=61/245 loss=1.5448399782180786 took 0.45436859130859375 secs\n",
      "epoch=5/3 iteration=62/245 loss=2.458406686782837 took 0.3260066509246826 secs\n",
      "epoch=5/3 iteration=63/245 loss=1.6183902025222778 took 0.2702493667602539 secs\n",
      "epoch=5/3 iteration=64/245 loss=1.4607149362564087 took 0.395479679107666 secs\n",
      "epoch=5/3 iteration=65/245 loss=1.254876732826233 took 0.32623767852783203 secs\n",
      "epoch=5/3 iteration=66/245 loss=1.540879249572754 took 0.3536524772644043 secs\n",
      "epoch=5/3 iteration=67/245 loss=0.9957912564277649 took 0.3644287586212158 secs\n",
      "epoch=5/3 iteration=68/245 loss=1.2093981504440308 took 0.35429835319519043 secs\n",
      "epoch=5/3 iteration=69/245 loss=1.332815170288086 took 0.41357946395874023 secs\n",
      "epoch=5/3 iteration=70/245 loss=1.4131885766983032 took 0.3656430244445801 secs\n",
      "epoch=5/3 iteration=71/245 loss=1.2595558166503906 took 0.33962392807006836 secs\n",
      "epoch=5/3 iteration=72/245 loss=2.053481101989746 took 0.34800171852111816 secs\n",
      "epoch=5/3 iteration=73/245 loss=1.1358479261398315 took 0.35373830795288086 secs\n",
      "epoch=5/3 iteration=74/245 loss=1.2459261417388916 took 0.3946068286895752 secs\n",
      "epoch=5/3 iteration=75/245 loss=1.846084713935852 took 0.3174874782562256 secs\n",
      "epoch=5/3 iteration=76/245 loss=0.9360300898551941 took 0.42843103408813477 secs\n",
      "epoch=5/3 iteration=77/245 loss=1.5948399305343628 took 0.3464236259460449 secs\n",
      "epoch=5/3 iteration=78/245 loss=1.2702150344848633 took 0.3603026866912842 secs\n",
      "epoch=5/3 iteration=79/245 loss=1.3084790706634521 took 0.3938169479370117 secs\n",
      "epoch=5/3 iteration=80/245 loss=1.6798956394195557 took 0.3792088031768799 secs\n",
      "epoch=5/3 iteration=81/245 loss=1.3371808528900146 took 0.30739784240722656 secs\n",
      "epoch=5/3 iteration=82/245 loss=0.9331194162368774 took 0.37738919258117676 secs\n",
      "epoch=5/3 iteration=83/245 loss=1.0484471321105957 took 0.4811716079711914 secs\n",
      "epoch=5/3 iteration=84/245 loss=1.0063941478729248 took 0.35605645179748535 secs\n",
      "epoch=5/3 iteration=85/245 loss=1.2284950017929077 took 0.35550665855407715 secs\n",
      "epoch=5/3 iteration=86/245 loss=0.7892373204231262 took 0.39298200607299805 secs\n",
      "epoch=5/3 iteration=87/245 loss=1.6032439470291138 took 0.4248228073120117 secs\n",
      "epoch=5/3 iteration=88/245 loss=0.7451130151748657 took 0.36010241508483887 secs\n",
      "epoch=5/3 iteration=89/245 loss=1.3125874996185303 took 0.33409810066223145 secs\n",
      "epoch=5/3 iteration=90/245 loss=0.4188488721847534 took 0.40917253494262695 secs\n",
      "epoch=5/3 iteration=91/245 loss=0.9968923330307007 took 0.38339734077453613 secs\n",
      "epoch=5/3 iteration=92/245 loss=0.5314074158668518 took 0.4671034812927246 secs\n",
      "epoch=5/3 iteration=93/245 loss=1.5228179693222046 took 0.3856160640716553 secs\n",
      "epoch=5/3 iteration=94/245 loss=1.2652021646499634 took 0.3316206932067871 secs\n",
      "epoch=5/3 iteration=95/245 loss=1.539228916168213 took 0.3640425205230713 secs\n",
      "epoch=5/3 iteration=96/245 loss=0.9869555830955505 took 0.3376500606536865 secs\n",
      "epoch=5/3 iteration=97/245 loss=1.3269140720367432 took 0.4877665042877197 secs\n",
      "epoch=5/3 iteration=98/245 loss=1.5492522716522217 took 0.3543696403503418 secs\n",
      "epoch=5/3 iteration=99/245 loss=1.7304694652557373 took 0.37564730644226074 secs\n",
      "epoch=5/3 iteration=100/245 loss=1.0710138082504272 took 0.36196064949035645 secs\n",
      "epoch=5/3 iteration=101/245 loss=1.497238039970398 took 0.3780708312988281 secs\n",
      "epoch=5/3 iteration=102/245 loss=1.2910006046295166 took 0.3502357006072998 secs\n",
      "epoch=5/3 iteration=103/245 loss=1.4206526279449463 took 0.39870691299438477 secs\n",
      "epoch=5/3 iteration=104/245 loss=1.8549858331680298 took 0.5204432010650635 secs\n",
      "epoch=5/3 iteration=105/245 loss=0.5339365601539612 took 0.3378937244415283 secs\n",
      "epoch=5/3 iteration=106/245 loss=1.7572920322418213 took 0.420640230178833 secs\n",
      "epoch=5/3 iteration=107/245 loss=1.7406015396118164 took 0.3536653518676758 secs\n",
      "epoch=5/3 iteration=108/245 loss=1.4534132480621338 took 0.45360875129699707 secs\n",
      "epoch=5/3 iteration=109/245 loss=1.3659547567367554 took 0.35086536407470703 secs\n",
      "epoch=5/3 iteration=110/245 loss=1.5077705383300781 took 0.459749698638916 secs\n",
      "epoch=5/3 iteration=111/245 loss=1.4772950410842896 took 0.3531982898712158 secs\n",
      "epoch=5/3 iteration=112/245 loss=1.9176387786865234 took 0.48740506172180176 secs\n",
      "epoch=5/3 iteration=113/245 loss=1.974622368812561 took 0.39731717109680176 secs\n",
      "epoch=5/3 iteration=114/245 loss=0.8924521803855896 took 0.3614773750305176 secs\n",
      "epoch=5/3 iteration=115/245 loss=1.1878607273101807 took 0.39220690727233887 secs\n",
      "epoch=5/3 iteration=116/245 loss=1.4658831357955933 took 0.3693995475769043 secs\n",
      "epoch=5/3 iteration=117/245 loss=1.133208155632019 took 0.37930727005004883 secs\n",
      "epoch=5/3 iteration=118/245 loss=1.2495524883270264 took 0.34035301208496094 secs\n",
      "epoch=5/3 iteration=119/245 loss=1.198333740234375 took 0.38884806632995605 secs\n",
      "epoch=5/3 iteration=120/245 loss=1.276599407196045 took 0.41939616203308105 secs\n",
      "epoch=5/3 iteration=121/245 loss=1.6724036931991577 took 0.3619203567504883 secs\n",
      "epoch=5/3 iteration=122/245 loss=1.617539644241333 took 0.35105252265930176 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5/3 iteration=123/245 loss=1.5662037134170532 took 0.34417176246643066 secs\n",
      "epoch=5/3 iteration=124/245 loss=1.0649387836456299 took 0.37200236320495605 secs\n",
      "epoch=5/3 iteration=125/245 loss=2.2273178100585938 took 0.36645030975341797 secs\n",
      "epoch=5/3 iteration=126/245 loss=1.388604760169983 took 0.3906404972076416 secs\n",
      "epoch=5/3 iteration=127/245 loss=1.405300498008728 took 0.47908449172973633 secs\n",
      "epoch=5/3 iteration=128/245 loss=1.0094449520111084 took 0.39264798164367676 secs\n",
      "epoch=5/3 iteration=129/245 loss=1.0041084289550781 took 0.34269070625305176 secs\n",
      "epoch=5/3 iteration=130/245 loss=1.1191412210464478 took 0.4011244773864746 secs\n",
      "epoch=5/3 iteration=131/245 loss=0.9417852759361267 took 0.33696508407592773 secs\n",
      "epoch=5/3 iteration=132/245 loss=0.6198779344558716 took 0.35736656188964844 secs\n",
      "epoch=5/3 iteration=133/245 loss=0.8394812345504761 took 0.44800877571105957 secs\n",
      "epoch=5/3 iteration=134/245 loss=1.0861519575119019 took 0.36858177185058594 secs\n",
      "epoch=5/3 iteration=135/245 loss=0.7879664897918701 took 0.38861703872680664 secs\n",
      "epoch=5/3 iteration=136/245 loss=1.0739566087722778 took 0.37840867042541504 secs\n",
      "epoch=5/3 iteration=137/245 loss=0.9322986006736755 took 0.41914868354797363 secs\n",
      "epoch=5/3 iteration=138/245 loss=1.0423858165740967 took 0.3635985851287842 secs\n",
      "epoch=5/3 iteration=139/245 loss=0.7202281951904297 took 0.35789918899536133 secs\n",
      "epoch=5/3 iteration=140/245 loss=1.2281965017318726 took 0.37821221351623535 secs\n",
      "epoch=5/3 iteration=141/245 loss=1.48963463306427 took 0.34980225563049316 secs\n",
      "epoch=5/3 iteration=142/245 loss=1.1490391492843628 took 0.3820765018463135 secs\n",
      "epoch=5/3 iteration=143/245 loss=1.063649296760559 took 0.5872602462768555 secs\n",
      "epoch=5/3 iteration=144/245 loss=0.2609097361564636 took 0.4423966407775879 secs\n",
      "epoch=5/3 iteration=145/245 loss=0.532491147518158 took 0.5233438014984131 secs\n",
      "epoch=5/3 iteration=146/245 loss=0.6680342555046082 took 0.6491575241088867 secs\n",
      "epoch=5/3 iteration=147/245 loss=0.5107125639915466 took 0.5561561584472656 secs\n",
      "epoch=5/3 iteration=148/245 loss=0.4231499135494232 took 0.532261848449707 secs\n",
      "epoch=5/3 iteration=149/245 loss=0.8115178942680359 took 0.5847909450531006 secs\n",
      "epoch=5/3 iteration=150/245 loss=0.599953293800354 took 0.6980578899383545 secs\n",
      "epoch=5/3 iteration=151/245 loss=0.7889718413352966 took 0.7021384239196777 secs\n",
      "epoch=5/3 iteration=152/245 loss=0.27141499519348145 took 0.6369333267211914 secs\n",
      "epoch=5/3 iteration=153/245 loss=0.4899439215660095 took 0.5732696056365967 secs\n",
      "epoch=5/3 iteration=154/245 loss=0.5868897438049316 took 0.5897035598754883 secs\n",
      "epoch=5/3 iteration=155/245 loss=0.6147985458374023 took 0.7121660709381104 secs\n",
      "epoch=5/3 iteration=156/245 loss=0.8609851598739624 took 0.6193568706512451 secs\n",
      "epoch=5/3 iteration=157/245 loss=0.6221951246261597 took 0.6003241539001465 secs\n",
      "epoch=5/3 iteration=158/245 loss=0.4649149477481842 took 0.5533018112182617 secs\n",
      "epoch=5/3 iteration=159/245 loss=0.3181932866573334 took 0.6213407516479492 secs\n",
      "epoch=5/3 iteration=160/245 loss=0.5008968114852905 took 0.6905055046081543 secs\n",
      "epoch=5/3 iteration=161/245 loss=0.5644317865371704 took 0.7243402004241943 secs\n",
      "epoch=5/3 iteration=162/245 loss=0.8995754718780518 took 0.5428295135498047 secs\n",
      "epoch=5/3 iteration=163/245 loss=1.1384681463241577 took 0.29833269119262695 secs\n",
      "epoch=5/3 iteration=164/245 loss=1.397858738899231 took 0.3732326030731201 secs\n",
      "epoch=5/3 iteration=165/245 loss=1.179080843925476 took 0.7187550067901611 secs\n",
      "epoch=5/3 iteration=166/245 loss=0.6701613068580627 took 0.36324548721313477 secs\n",
      "epoch=5/3 iteration=167/245 loss=1.1197128295898438 took 0.28212761878967285 secs\n",
      "epoch=5/3 iteration=168/245 loss=1.164697527885437 took 0.36621832847595215 secs\n",
      "epoch=5/3 iteration=169/245 loss=0.6349022388458252 took 0.3121836185455322 secs\n",
      "epoch=5/3 iteration=170/245 loss=0.9819760918617249 took 0.30610084533691406 secs\n",
      "epoch=5/3 iteration=171/245 loss=1.039599061012268 took 0.32396626472473145 secs\n",
      "epoch=5/3 iteration=172/245 loss=1.117811918258667 took 0.30516529083251953 secs\n",
      "epoch=5/3 iteration=173/245 loss=1.4577059745788574 took 0.35421156883239746 secs\n",
      "epoch=5/3 iteration=174/245 loss=1.298797845840454 took 0.28130292892456055 secs\n",
      "epoch=5/3 iteration=175/245 loss=1.4562278985977173 took 0.2911362648010254 secs\n",
      "epoch=5/3 iteration=176/245 loss=1.1471707820892334 took 0.35317111015319824 secs\n",
      "epoch=5/3 iteration=177/245 loss=1.071853518486023 took 0.46319079399108887 secs\n",
      "epoch=5/3 iteration=178/245 loss=0.5832637548446655 took 0.28471875190734863 secs\n",
      "epoch=5/3 iteration=179/245 loss=1.355655550956726 took 0.31165194511413574 secs\n",
      "epoch=5/3 iteration=180/245 loss=0.8939288854598999 took 0.2937295436859131 secs\n",
      "epoch=5/3 iteration=181/245 loss=1.189527153968811 took 0.29100942611694336 secs\n",
      "epoch=5/3 iteration=182/245 loss=1.4182028770446777 took 0.3965027332305908 secs\n",
      "epoch=5/3 iteration=183/245 loss=0.6437215209007263 took 0.3795945644378662 secs\n",
      "epoch=5/3 iteration=184/245 loss=1.0872596502304077 took 0.2774319648742676 secs\n",
      "epoch=5/3 iteration=185/245 loss=1.0223212242126465 took 0.2875685691833496 secs\n",
      "epoch=5/3 iteration=186/245 loss=1.6459341049194336 took 0.30507636070251465 secs\n",
      "epoch=5/3 iteration=187/245 loss=0.9568948745727539 took 0.35005927085876465 secs\n",
      "epoch=5/3 iteration=188/245 loss=0.7445087432861328 took 0.29482412338256836 secs\n",
      "epoch=5/3 iteration=189/245 loss=1.1173137426376343 took 0.3242146968841553 secs\n",
      "epoch=5/3 iteration=190/245 loss=0.7522869110107422 took 0.32237815856933594 secs\n",
      "epoch=5/3 iteration=191/245 loss=1.3138600587844849 took 0.315474271774292 secs\n",
      "epoch=5/3 iteration=192/245 loss=1.1712099313735962 took 0.3227040767669678 secs\n",
      "epoch=5/3 iteration=193/245 loss=1.6914795637130737 took 0.36896848678588867 secs\n",
      "epoch=5/3 iteration=194/245 loss=0.6548528671264648 took 0.3127875328063965 secs\n",
      "epoch=5/3 iteration=195/245 loss=0.8248484134674072 took 0.27515554428100586 secs\n",
      "epoch=5/3 iteration=196/245 loss=1.326569676399231 took 0.3168501853942871 secs\n",
      "epoch=5/3 iteration=197/245 loss=1.1058144569396973 took 0.31250977516174316 secs\n",
      "epoch=5/3 iteration=198/245 loss=1.485182285308838 took 0.34440064430236816 secs\n",
      "epoch=5/3 iteration=199/245 loss=1.394819974899292 took 0.4391002655029297 secs\n",
      "epoch=5/3 iteration=200/245 loss=0.7717793583869934 took 0.32822442054748535 secs\n",
      "epoch=5/3 iteration=201/245 loss=0.961341917514801 took 0.2778053283691406 secs\n",
      "epoch=5/3 iteration=202/245 loss=1.0534217357635498 took 0.3124880790710449 secs\n",
      "epoch=5/3 iteration=203/245 loss=0.901986300945282 took 0.3235609531402588 secs\n",
      "epoch=5/3 iteration=204/245 loss=1.2449166774749756 took 0.35064268112182617 secs\n",
      "epoch=5/3 iteration=205/245 loss=1.8635822534561157 took 0.29067373275756836 secs\n",
      "epoch=5/3 iteration=206/245 loss=1.663382887840271 took 0.3261988162994385 secs\n",
      "epoch=5/3 iteration=207/245 loss=1.3483617305755615 took 0.3273453712463379 secs\n",
      "epoch=5/3 iteration=208/245 loss=1.3215162754058838 took 0.27055954933166504 secs\n",
      "epoch=5/3 iteration=209/245 loss=1.6960337162017822 took 0.39711499214172363 secs\n",
      "epoch=5/3 iteration=210/245 loss=1.3437423706054688 took 0.3549654483795166 secs\n",
      "epoch=5/3 iteration=211/245 loss=0.879476010799408 took 0.33527064323425293 secs\n",
      "epoch=5/3 iteration=212/245 loss=1.402895450592041 took 0.27715229988098145 secs\n",
      "epoch=5/3 iteration=213/245 loss=1.3542898893356323 took 0.29093480110168457 secs\n",
      "epoch=5/3 iteration=214/245 loss=1.768953561782837 took 0.3422110080718994 secs\n",
      "epoch=5/3 iteration=215/245 loss=0.8319808840751648 took 0.3336033821105957 secs\n",
      "epoch=5/3 iteration=216/245 loss=1.2643251419067383 took 0.46796274185180664 secs\n",
      "epoch=5/3 iteration=217/245 loss=1.1377122402191162 took 0.3243536949157715 secs\n",
      "epoch=5/3 iteration=218/245 loss=1.505370020866394 took 0.3439042568206787 secs\n",
      "epoch=5/3 iteration=219/245 loss=2.0529510974884033 took 0.28211021423339844 secs\n",
      "epoch=5/3 iteration=220/245 loss=1.3907698392868042 took 0.3417651653289795 secs\n",
      "epoch=5/3 iteration=221/245 loss=0.7680377960205078 took 0.3413851261138916 secs\n",
      "epoch=5/3 iteration=222/245 loss=1.3779047727584839 took 0.34447193145751953 secs\n",
      "epoch=5/3 iteration=223/245 loss=1.411110520362854 took 0.30741405487060547 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5/3 iteration=224/245 loss=1.8485153913497925 took 0.2998363971710205 secs\n",
      "epoch=5/3 iteration=225/245 loss=1.6178762912750244 took 0.3291158676147461 secs\n",
      "epoch=5/3 iteration=226/245 loss=0.9430502653121948 took 0.30178022384643555 secs\n",
      "epoch=5/3 iteration=227/245 loss=0.9190624952316284 took 0.34763288497924805 secs\n",
      "epoch=5/3 iteration=228/245 loss=1.5588866472244263 took 0.32901430130004883 secs\n",
      "epoch=5/3 iteration=229/245 loss=1.666789174079895 took 0.3173096179962158 secs\n",
      "epoch=5/3 iteration=230/245 loss=1.2276209592819214 took 0.4131588935852051 secs\n",
      "epoch=5/3 iteration=231/245 loss=1.3496381044387817 took 0.32958030700683594 secs\n",
      "epoch=5/3 iteration=232/245 loss=1.1364827156066895 took 0.2991793155670166 secs\n",
      "epoch=5/3 iteration=233/245 loss=0.9926809668540955 took 0.314023494720459 secs\n",
      "epoch=5/3 iteration=234/245 loss=0.14526158571243286 took 0.31540346145629883 secs\n",
      "epoch=5/3 iteration=235/245 loss=0.2231157124042511 took 0.2711985111236572 secs\n",
      "epoch=5/3 iteration=236/245 loss=0.3523915410041809 took 0.3176906108856201 secs\n",
      "epoch=5/3 iteration=237/245 loss=0.5890105962753296 took 0.28562045097351074 secs\n",
      "epoch=5/3 iteration=238/245 loss=1.5265878438949585 took 0.30003786087036133 secs\n",
      "epoch=5/3 iteration=239/245 loss=1.4515650272369385 took 0.3360600471496582 secs\n",
      "epoch=5/3 iteration=240/245 loss=1.104111909866333 took 0.2876420021057129 secs\n",
      "epoch=5/3 iteration=241/245 loss=1.4523367881774902 took 0.3154749870300293 secs\n",
      "epoch=5/3 iteration=242/245 loss=1.98655366897583 took 0.3240194320678711 secs\n",
      "epoch=5/3 iteration=243/245 loss=2.6511495113372803 took 0.3362104892730713 secs\n",
      "epoch=5/3 iteration=244/245 loss=3.1088943481445312 took 0.4828648567199707 secs\n",
      "epoch=5/3 iteration=245/245 loss=3.5281424522399902 took 0.32343602180480957 secs\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "losses = []\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for i,((sent1,sent2),label) in enumerate(train_dataloader_regression):\n",
    "        start = time.time()\n",
    "\n",
    "        output = sbert(sent1,sent2,objective=\"regression\")\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        label = label.float()\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(f\"epoch={epoch+1}/{n_epochs} iteration={i+1}/{len(train_dataloader_regression)} loss={loss.detach().numpy()} took {time.time()-start} secs\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    PATH = f\"models/classification_regression_epoch_{epoch+1}.pt\"\n",
    "    torch.save(sbert.state_dict(), PATH)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f948202f700>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_losses = [l.detach().numpy() for l in losses]\n",
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Compare and evaluate the 2 new models on the STS benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. first with pure classification objective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 974/975\r"
     ]
    }
   ],
   "source": [
    "PATH = \"models/classification_epoch_5.pt\"\n",
    "sbert = SBERT()\n",
    "sbert.load_state_dict(torch.load(PATH))\n",
    "sbert.eval()\n",
    "\n",
    "import pandas as pd\n",
    "df_test = pd.read_csv(\"datasets/Stsbenchmark/sts-test.csv\",header=0,names=[\"main-caption\",\"genre\",\"filename\",\"year\",\"score\",\"sentence1\",\"sentence2\"])#,usecols=['score','sentence1','sentence2'])\n",
    "\n",
    "df_test = df_test[['score','sentence1','sentence2']]\n",
    "# df_test.head()\n",
    "\n",
    "ls = list(df_test.score)\n",
    "minn = min(ls)\n",
    "maxx = max(ls)\n",
    "# (minn,maxx)\n",
    "\n",
    "def map_score(value, leftMin=0, leftMax=5, rightMin=-1, rightMax=1):\n",
    "    # Figure out how 'wide' each range is\n",
    "    leftSpan = leftMax - leftMin\n",
    "    rightSpan = rightMax - rightMin\n",
    "\n",
    "    # Convert the left range into a 0-1 range (float)\n",
    "    valueScaled = float(value - leftMin) / float(leftSpan)\n",
    "\n",
    "    # Convert the 0-1 range into a value in the right range.\n",
    "    return rightMin + (valueScaled * rightSpan)\n",
    "\n",
    "df_test['score'] = df_test['score'].apply(map_score)\n",
    "\n",
    "ls = list(df_test.score)\n",
    "minn = min(ls)\n",
    "maxx = max(ls)\n",
    "# (minn,maxx)\n",
    "\n",
    "# df_test.isna().sum()\n",
    "\n",
    "df_test.dropna(inplace=True)\n",
    "df_test.reset_index(inplace=True)\n",
    "\n",
    "# df_test.isna().sum()\n",
    "\n",
    "cosine_scores = []\n",
    "for i,row in df_test.iterrows():\n",
    "    print(f\"Finished {i}/{len(df_test)}\",end=\"\\r\")\n",
    "    score = sbert(row.sentence1,row.sentence2,\"regression\").detach().numpy()[0]\n",
    "    cosine_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.03714025583281556, pvalue=0.24661266975901172)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "spearmanr(df_test.score.values.tolist(),cosine_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. second with classification-trained regression-fine-tuned objective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 974/975\r"
     ]
    }
   ],
   "source": [
    "PATH = \"models/classification_regression_epoch_5.pt\"\n",
    "sbert = SBERT()\n",
    "sbert.load_state_dict(torch.load(PATH))\n",
    "sbert.eval()\n",
    "\n",
    "import pandas as pd\n",
    "df_test = pd.read_csv(\"datasets/Stsbenchmark/sts-test.csv\",header=0,names=[\"main-caption\",\"genre\",\"filename\",\"year\",\"score\",\"sentence1\",\"sentence2\"])#,usecols=['score','sentence1','sentence2'])\n",
    "\n",
    "df_test = df_test[['score','sentence1','sentence2']]\n",
    "# df_test.head()\n",
    "\n",
    "ls = list(df_test.score)\n",
    "minn = min(ls)\n",
    "maxx = max(ls)\n",
    "# (minn,maxx)\n",
    "\n",
    "def map_score(value, leftMin=0, leftMax=5, rightMin=-1, rightMax=1):\n",
    "    # Figure out how 'wide' each range is\n",
    "    leftSpan = leftMax - leftMin\n",
    "    rightSpan = rightMax - rightMin\n",
    "\n",
    "    # Convert the left range into a 0-1 range (float)\n",
    "    valueScaled = float(value - leftMin) / float(leftSpan)\n",
    "\n",
    "    # Convert the 0-1 range into a value in the right range.\n",
    "    return rightMin + (valueScaled * rightSpan)\n",
    "\n",
    "df_test['score'] = df_test['score'].apply(map_score)\n",
    "\n",
    "ls = list(df_test.score)\n",
    "minn = min(ls)\n",
    "maxx = max(ls)\n",
    "# (minn,maxx)\n",
    "\n",
    "# df_test.isna().sum()\n",
    "\n",
    "df_test.dropna(inplace=True)\n",
    "df_test.reset_index(inplace=True)\n",
    "\n",
    "# df_test.isna().sum()\n",
    "\n",
    "cosine_scores = []\n",
    "for i,row in df_test.iterrows():\n",
    "    print(f\"Finished {i}/{len(df_test)}\",end=\"\\r\")\n",
    "    score = sbert(row.sentence1,row.sentence2,\"regression\").detach().numpy()[0]\n",
    "    cosine_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.02538929297721189, pvalue=0.42842418542383065)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df_test.score.values.tolist()\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "spearmanr(labels,cosine_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Use your best fine-tuned model and create a small semantic search system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We go for the first model a with ~slightly higher correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"models/classification_epoch_5.pt\"\n",
    "sbert = SBERT()\n",
    "sbert.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "list_ = []\n",
    "path = \"datasets/News_Category_Dataset_v2.json\"\n",
    "with open(path) as files:\n",
    "    for file in files:\n",
    "        list_.append(json.loads(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2       Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3       Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4       Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description        date  \n",
       "0  She left her husband. He killed their children...  2018-05-26  \n",
       "1                           Of course it has a song.  2018-05-26  \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  2018-05-26  \n",
       "3  The actor gives Dems an ass-kicking for not fi...  2018-05-26  \n",
       "4  The \"Dietland\" actress said using the bags is ...  2018-05-26  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_news = pd.DataFrame(list_)\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before splitting: 200853\n",
      "Length after splitting: 215043\n",
      "Data increased by 14190 sentences\n"
     ]
    }
   ],
   "source": [
    "sent1 = list(df_news.headline)\n",
    "print(\"Length before splitting:\",len(sent1))\n",
    "sent2 = [x for sentence in sent1 for x in sentence.split(\".\")]\n",
    "sent2 = [x for x in sent2 if x]\n",
    "print(\"Length after splitting:\",len(sent2))\n",
    "print(f\"Data increased by {len(sent2)-len(sent1)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2 = sent2[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV',\n",
       " \"Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song\",\n",
       " 'Hugh Grant Marries For The First Time At Age 57',\n",
       " \"Jim Carrey Blasts 'Castrato' Adam Schiff And Democrats In New Artwork\",\n",
       " 'Julianna Margulies Uses Donald Trump Poop Bags To Pick Up After Her Dog']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"Sweden is a very safe place.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(sentences):\n",
    "    embeddings = []\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        try:\n",
    "            print(f\"Finished iteration {i}/{len(sentences)}\",end=\"\\r\")\n",
    "\n",
    "            embeddings.append(sbert(sentence,objective=\"embedding\"))\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    embeddings = torch.cat(embeddings,0)\n",
    "    \n",
    "    embeddings = torch.FloatTensor(embeddings)\n",
    "    torch.save(embeddings, 'datasets/embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embeddings(sent2[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 766])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = torch.load('datasets/embeddings.pt')\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine(input_sentence, embeddings_database=\"embeddings\",K=5):\n",
    "    embedding = sbert(input_sentence,\"embedding\")\n",
    "    idx = torch.argsort(torch.cosine_similarity(embedding,embeddings),descending=True)[:5]\n",
    "    return [sent2[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['White House Preps Spending Cuts Bill That Could Set Up Another Shutdown Standoff',\n",
       " 'GOP Senator Reintroduces Bill To Protect Discrimination Against LGBTQ People',\n",
       " 'Group Warns Hundreds Of School Districts They May Be Putting Student Lives In Danger',\n",
       " ' Embassy In Jerusalem Opens Its Doors As Protests Rage Nearby',\n",
       " \"Steve Bannon Doesn't Deny Anything He Said In 'Fire And Fury'\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = \"I am the king of the jungle\"\n",
    "search_engine(input_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
