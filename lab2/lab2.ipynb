{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertModel, BertConfig, BertTokenizer\n",
    "from scipy import spatial\n",
    "\n",
    "# QUESTION: \n",
    "## SHOULD WE TRAIN FROM SCRATCH OR CAN WE START FROM bert-base-uncased or any other starting point?\n",
    "##### DONE: BertConfig() starts from random weights\n",
    "\n",
    "## TODO: Add validation set and test on testing set for SBERT classification\n",
    "##### keep the model running for a while and then test .....\n",
    "\n",
    "## TODO: After finishing training for classification, save the model please to stop training everytime <3\n",
    "##### sure.. but we can just run the model \n",
    "\n",
    "## TODO: After training on the classification data, check how to \"fine-tune\" on the STS regression data (maybe only the head without the bert itself?)\n",
    "## Check my question please <3 https://docs.google.com/document/d/1YeohuAr55fKF2nI1RiCgpq_Wa3Yn-CLAwfPoBmViNIM/edit\n",
    "\n",
    "class SBERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SBERT, self).__init__()\n",
    "        \n",
    "        # self.model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        configuration = BertConfig()\n",
    "        self.model = BertModel(configuration)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        self.pooling = nn.AvgPool1d(kernel_size=3,stride=1)\n",
    "        # 768 / 3 -> 256\n",
    "        self.linear = nn.Linear(in_features=2298, out_features=3) # 2298=(768-2)*3; 153 is the embedding dimension after pooling and stuff..\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, sent1, sent2=None, objective=\"embedding\"):\n",
    "        encoded_input1 = self.tokenizer(sent1, padding=True, truncation=True, return_tensors='pt')\n",
    "        output1 = self.model(**encoded_input1)\n",
    "        output1 = self.pooling(output1[\"pooler_output\"])\n",
    "        \n",
    "        if objective==\"embedding\":\n",
    "            return output1\n",
    "\n",
    "        encoded_input2 = self.tokenizer(sent2, padding=True, truncation=True, return_tensors='pt')\n",
    "        output2 = self.model(**encoded_input2)\n",
    "        output2 = self.pooling(output2[\"pooler_output\"])\n",
    "                        \n",
    "        if objective == \"regression\":\n",
    "            return torch.cosine_similarity(output1, output2)\n",
    "\n",
    "        if objective == \"classification\":\n",
    "            diff = abs(torch.subtract(output1,output2))\n",
    "            concat = torch.cat([output1,output2,diff],axis=1)            \n",
    "            result = self.linear(concat)\n",
    "            out = self.softmax(result)\n",
    "            return out\n",
    "\n",
    "sbert = SBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Objective Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False in ex1\n",
    "# True in ex2\n",
    "comparing_models = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if comparing_models: \n",
    "    PATH = \"models/classification_regression.pt\"\n",
    "    sbert = SBERT()\n",
    "    sbert.load_state_dict(torch.load(PATH))\n",
    "    sbert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.6</td>\n",
       "      <td>A group of men play soccer on the beach.</td>\n",
       "      <td>A group of boys are playing soccer on the beach.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>One woman is measuring another woman's ankle.</td>\n",
       "      <td>A woman measures another woman's ankle.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.2</td>\n",
       "      <td>A man is cutting up a cucumber.</td>\n",
       "      <td>A man is slicing a cucumber.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.5</td>\n",
       "      <td>A man is playing a harp.</td>\n",
       "      <td>A man is playing a keyboard.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.8</td>\n",
       "      <td>A woman is cutting onions.</td>\n",
       "      <td>A woman is cutting tofu.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                      sentence1  \\\n",
       "0    3.6       A group of men play soccer on the beach.   \n",
       "1    5.0  One woman is measuring another woman's ankle.   \n",
       "2    4.2                A man is cutting up a cucumber.   \n",
       "3    1.5                       A man is playing a harp.   \n",
       "4    1.8                     A woman is cutting onions.   \n",
       "\n",
       "                                          sentence2  \n",
       "0  A group of boys are playing soccer on the beach.  \n",
       "1           A woman measures another woman's ankle.  \n",
       "2                      A man is slicing a cucumber.  \n",
       "3                      A man is playing a keyboard.  \n",
       "4                          A woman is cutting tofu.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_csv(\"datasets/Stsbenchmark/sts-test.csv\",header=0,names=[\"main-caption\",\"genre\",\"filename\",\"year\",\"score\",\"sentence1\",\"sentence2\"])#,usecols=['score','sentence1','sentence2'])\n",
    "\n",
    "df_test = df_test[['score','sentence1','sentence2']]\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 5.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = list(df_test.score)\n",
    "minn = min(ls)\n",
    "maxx = max(ls)\n",
    "(minn,maxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_score(value, leftMin=0, leftMax=5, rightMin=-1, rightMax=1):\n",
    "    # Figure out how 'wide' each range is\n",
    "    leftSpan = leftMax - leftMin\n",
    "    rightSpan = rightMax - rightMin\n",
    "\n",
    "    # Convert the left range into a 0-1 range (float)\n",
    "    valueScaled = float(value - leftMin) / float(leftSpan)\n",
    "\n",
    "    # Convert the 0-1 range into a value in the right range.\n",
    "    return rightMin + (valueScaled * rightSpan)\n",
    "\n",
    "df_test['score'] = df_test['score'].apply(map_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = list(df_test.score)\n",
    "minn = min(ls)\n",
    "maxx = max(ls)\n",
    "(minn,maxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score        0\n",
       "sentence1    0\n",
       "sentence2    5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.dropna(inplace=True)\n",
    "df_test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index        0\n",
       "score        0\n",
       "sentence1    0\n",
       "sentence2    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 978/975\r"
     ]
    }
   ],
   "source": [
    "cosine_scores = []\n",
    "for i,row in df_test.iterrows():\n",
    "    print(f\"Finished {i}/{len(df_test)}\",end=\"\\r\")\n",
    "    score = sbert(row.sentence1,row.sentence2,\"regression\").detach().numpy()[0]\n",
    "    cosine_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_test.score.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.057250966842595334, pvalue=0.07396443024495716)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "spearmanr(labels,cosine_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Objective Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_list_train = list(open(\"datasets/snli_1.0/snli_1.0_train.jsonl\",\"r\"))\n",
    "json_list_val = list(open(\"datasets/snli_1.0/snli_1.0_dev.jsonl\",\"r\"))\n",
    "\n",
    "label2int = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 549367\n",
      "Testing data: 9842\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_train = {'sentence1': [], 'sentence2': [], 'gold_label': []}\n",
    "for json_str in json_list_train:\n",
    "    try:\n",
    "        result = json.loads(json_str)\n",
    "        result['gold_label']=label2int[result['gold_label']]\n",
    "        for key in data_train:\n",
    "            data_train[key].append(result[key])\n",
    "    except:\n",
    "        pass\n",
    "df_train = pd.DataFrame.from_dict(data_train)#.head()\n",
    "\n",
    "data_val = {'sentence1': [], 'sentence2': [], 'gold_label': []}\n",
    "for json_str in json_list_val:\n",
    "    try:\n",
    "        result = json.loads(json_str)\n",
    "        result['gold_label']=label2int[result['gold_label']]\n",
    "        for key in data_val:\n",
    "            data_val[key].append(result[key])\n",
    "    except:\n",
    "        pass\n",
    "df_val = pd.DataFrame.from_dict(data_val)#.head()\n",
    "\n",
    "print(\"Training data:\",len(df_train))\n",
    "print(\"Testing data:\",len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is training his horse for a competition.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is at a diner, ordering an omelette.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is outdoors, on a horse.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>They are smiling at their parents</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>There are children present</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  A person on a horse jumps over a broken down a...   \n",
       "1  A person on a horse jumps over a broken down a...   \n",
       "2  A person on a horse jumps over a broken down a...   \n",
       "3              Children smiling and waving at camera   \n",
       "4              Children smiling and waving at camera   \n",
       "\n",
       "                                           sentence2  gold_label  \n",
       "0  A person is training his horse for a competition.           2  \n",
       "1      A person is at a diner, ordering an omelette.           0  \n",
       "2                  A person is outdoors, on a horse.           1  \n",
       "3                  They are smiling at their parents           2  \n",
       "4                         There are children present           1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class SNLI_Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        sent1 = row['sentence1']\n",
    "        sent2 = row['sentence2']\n",
    "        label = row['gold_label']\n",
    "        return (sent1, sent2), label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- outputs of shape (N,C) where N is the batch size C is the number of classes\n",
    "- target of shape (N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1/10 iteration=1/34336 train_loss=1.1059 took 0.7070 secs\n",
      "epoch=1/10 iteration=2/34336 train_loss=1.1709 took 0.4261 secs\n",
      "epoch=1/10 iteration=3/34336 train_loss=1.1881 took 0.3999 secs\n",
      "epoch=1/10 iteration=4/34336 train_loss=1.1084 took 0.4236 secs\n",
      "epoch=1/10 iteration=5/34336 train_loss=1.1597 took 0.6614 secs\n",
      "epoch=1/10 iteration=6/34336 train_loss=1.1511 took 0.7325 secs\n",
      "epoch=1/10 iteration=7/34336 train_loss=1.2218 took 0.5861 secs\n",
      "epoch=1/10 iteration=8/34336 train_loss=1.1032 took 0.3301 secs\n",
      "epoch=1/10 iteration=9/34336 train_loss=1.1626 took 0.4545 secs\n",
      "epoch=1/10 iteration=10/34336 train_loss=1.1426 took 0.3818 secs\n",
      "epoch=1/10 iteration=11/34336 train_loss=1.1036 took 0.3551 secs\n",
      "epoch=1/10 iteration=12/34336 train_loss=1.1771 took 0.3191 secs\n",
      "epoch=1/10 iteration=13/34336 train_loss=1.1839 took 0.3191 secs\n",
      "epoch=1/10 iteration=14/34336 train_loss=1.1054 took 0.4677 secs\n",
      "epoch=1/10 iteration=15/34336 train_loss=1.1008 took 0.4597 secs\n",
      "epoch=1/10 iteration=16/34336 train_loss=1.0938 took 0.4867 secs\n",
      "epoch=1/10 iteration=17/34336 train_loss=1.1163 took 0.5085 secs\n",
      "epoch=1/10 iteration=18/34336 train_loss=1.0948 took 0.7110 secs\n",
      "epoch=1/10 iteration=19/34336 train_loss=1.1251 took 0.3893 secs\n",
      "epoch=1/10 iteration=20/34336 train_loss=1.1237 took 0.4220 secs\n",
      "epoch=1/10 iteration=21/34336 train_loss=1.1063 took 0.6514 secs\n",
      "epoch=1/10 iteration=22/34336 train_loss=1.1172 took 0.6331 secs\n",
      "epoch=1/10 iteration=23/34336 train_loss=1.1047 took 0.5002 secs\n",
      "epoch=1/10 iteration=24/34336 train_loss=1.1029 took 0.3865 secs\n",
      "epoch=1/10 iteration=25/34336 train_loss=1.1065 took 0.3920 secs\n",
      "epoch=1/10 iteration=26/34336 train_loss=1.1068 took 0.4501 secs\n",
      "epoch=1/10 iteration=27/34336 train_loss=1.1154 took 0.4358 secs\n",
      "epoch=1/10 iteration=28/34336 train_loss=1.0941 took 0.6381 secs\n",
      "epoch=1/10 iteration=29/34336 train_loss=1.1040 took 0.4616 secs\n",
      "epoch=1/10 iteration=30/34336 train_loss=1.1006 took 0.5124 secs\n",
      "epoch=1/10 iteration=31/34336 train_loss=1.1027 took 0.4057 secs\n",
      "epoch=1/10 iteration=32/34336 train_loss=1.1053 took 0.4136 secs\n",
      "epoch=1/10 iteration=33/34336 train_loss=1.0924 took 0.3248 secs\n",
      "epoch=1/10 iteration=34/34336 train_loss=1.0997 took 0.4748 secs\n",
      "epoch=1/10 iteration=35/34336 train_loss=1.1010 took 0.3734 secs\n",
      "epoch=1/10 iteration=36/34336 train_loss=1.1055 took 0.4209 secs\n",
      "epoch=1/10 iteration=37/34336 train_loss=1.1101 took 0.6442 secs\n",
      "epoch=1/10 iteration=38/34336 train_loss=1.0979 took 0.3668 secs\n",
      "epoch=1/10 iteration=39/34336 train_loss=1.0990 took 0.3642 secs\n",
      "epoch=1/10 iteration=40/34336 train_loss=1.1113 took 0.5050 secs\n",
      "epoch=1/10 iteration=41/34336 train_loss=1.1032 took 0.3120 secs\n",
      "epoch=1/10 iteration=42/34336 train_loss=1.1036 took 0.4202 secs\n",
      "epoch=1/10 iteration=43/34336 train_loss=1.1005 took 0.5014 secs\n",
      "epoch=1/10 iteration=44/34336 train_loss=1.0983 took 0.4896 secs\n",
      "epoch=1/10 iteration=45/34336 train_loss=1.1009 took 0.3667 secs\n",
      "epoch=1/10 iteration=46/34336 train_loss=1.1072 took 0.4500 secs\n",
      "epoch=1/10 iteration=47/34336 train_loss=1.0990 took 0.4878 secs\n",
      "epoch=1/10 iteration=48/34336 train_loss=1.1043 took 0.4566 secs\n",
      "epoch=1/10 iteration=49/34336 train_loss=1.0981 took 0.3819 secs\n",
      "epoch=1/10 iteration=50/34336 train_loss=1.1015 took 0.3863 secs\n",
      "epoch=1/10 iteration=51/34336 train_loss=1.1025 took 0.3340 secs\n",
      "epoch=1/10 iteration=52/34336 train_loss=1.0991 took 0.3762 secs\n",
      "epoch=1/10 iteration=53/34336 train_loss=1.1000 took 0.3546 secs\n",
      "epoch=1/10 iteration=54/34336 train_loss=1.1082 took 0.6747 secs\n",
      "epoch=1/10 iteration=55/34336 train_loss=1.0993 took 0.3639 secs\n",
      "epoch=1/10 iteration=56/34336 train_loss=1.0905 took 0.3699 secs\n",
      "epoch=1/10 iteration=57/34336 train_loss=1.1058 took 0.4304 secs\n",
      "epoch=1/10 iteration=58/34336 train_loss=1.0945 took 0.4950 secs\n",
      "epoch=1/10 iteration=59/34336 train_loss=1.0968 took 0.4853 secs\n",
      "epoch=1/10 iteration=60/34336 train_loss=1.1027 took 0.4014 secs\n",
      "epoch=1/10 iteration=61/34336 train_loss=1.0972 took 0.4225 secs\n",
      "epoch=1/10 iteration=62/34336 train_loss=1.1053 took 0.5388 secs\n",
      "epoch=1/10 iteration=63/34336 train_loss=1.1085 took 0.4611 secs\n",
      "epoch=1/10 iteration=64/34336 train_loss=1.0944 took 0.3436 secs\n",
      "epoch=1/10 iteration=65/34336 train_loss=1.1159 took 0.4626 secs\n",
      "epoch=1/10 iteration=66/34336 train_loss=1.1014 took 0.4158 secs\n",
      "epoch=1/10 iteration=67/34336 train_loss=1.0933 took 0.4030 secs\n",
      "epoch=1/10 iteration=68/34336 train_loss=1.1066 took 0.5124 secs\n",
      "epoch=1/10 iteration=69/34336 train_loss=1.0908 took 0.5298 secs\n",
      "epoch=1/10 iteration=70/34336 train_loss=1.1038 took 0.4883 secs\n",
      "epoch=1/10 iteration=71/34336 train_loss=1.1097 took 0.5107 secs\n",
      "epoch=1/10 iteration=72/34336 train_loss=1.1045 took 0.4475 secs\n",
      "epoch=1/10 iteration=73/34336 train_loss=1.0949 took 0.4385 secs\n",
      "epoch=1/10 iteration=74/34336 train_loss=1.0886 took 0.5849 secs\n",
      "epoch=1/10 iteration=75/34336 train_loss=1.0991 took 0.4794 secs\n",
      "epoch=1/10 iteration=76/34336 train_loss=1.0986 took 0.6103 secs\n",
      "epoch=1/10 iteration=77/34336 train_loss=1.1042 took 0.3700 secs\n",
      "epoch=1/10 iteration=78/34336 train_loss=1.1067 took 0.4443 secs\n",
      "epoch=1/10 iteration=79/34336 train_loss=1.1054 took 0.6291 secs\n",
      "epoch=1/10 iteration=80/34336 train_loss=1.0968 took 0.4599 secs\n",
      "epoch=1/10 iteration=81/34336 train_loss=1.1045 took 0.4696 secs\n",
      "epoch=1/10 iteration=82/34336 train_loss=1.0912 took 0.4214 secs\n",
      "epoch=1/10 iteration=83/34336 train_loss=1.1119 took 0.3695 secs\n",
      "epoch=1/10 iteration=84/34336 train_loss=1.0995 took 0.4036 secs\n",
      "epoch=1/10 iteration=85/34336 train_loss=1.0984 took 0.3989 secs\n",
      "epoch=1/10 iteration=86/34336 train_loss=1.0922 took 0.4490 secs\n",
      "epoch=1/10 iteration=87/34336 train_loss=1.0952 took 0.4418 secs\n",
      "epoch=1/10 iteration=88/34336 train_loss=1.0997 took 0.3757 secs\n",
      "epoch=1/10 iteration=89/34336 train_loss=1.0954 took 0.3798 secs\n",
      "epoch=1/10 iteration=90/34336 train_loss=1.1003 took 0.4401 secs\n",
      "epoch=1/10 iteration=91/34336 train_loss=1.0931 took 0.5018 secs\n",
      "epoch=1/10 iteration=92/34336 train_loss=1.0906 took 0.3563 secs\n",
      "epoch=1/10 iteration=93/34336 train_loss=1.0947 took 0.3196 secs\n",
      "epoch=1/10 iteration=94/34336 train_loss=1.1033 took 0.3599 secs\n",
      "epoch=1/10 iteration=95/34336 train_loss=1.1160 took 0.4037 secs\n",
      "epoch=1/10 iteration=96/34336 train_loss=1.1000 took 0.4550 secs\n",
      "epoch=1/10 iteration=97/34336 train_loss=1.0998 took 0.5040 secs\n",
      "epoch=1/10 iteration=98/34336 train_loss=1.0979 took 0.3623 secs\n",
      "epoch=1/10 iteration=99/34336 train_loss=1.1038 took 0.4206 secs\n",
      "epoch=1/10 iteration=100/34336 train_loss=1.1016 took 0.4629 secs\n",
      "epoch=1/10 iteration=101/34336 train_loss=1.0977 took 0.4713 secs\n",
      "epoch=1/10 iteration=102/34336 train_loss=1.1036 took 0.4134 secs\n",
      "epoch=1/10 iteration=103/34336 train_loss=1.0957 took 0.3956 secs\n",
      "epoch=1/10 iteration=104/34336 train_loss=1.1114 took 0.4902 secs\n",
      "epoch=1/10 iteration=105/34336 train_loss=1.0993 took 0.4695 secs\n",
      "epoch=1/10 iteration=106/34336 train_loss=1.1048 took 0.4299 secs\n",
      "epoch=1/10 iteration=107/34336 train_loss=1.1028 took 0.4076 secs\n",
      "epoch=1/10 iteration=108/34336 train_loss=1.1123 took 0.4411 secs\n",
      "epoch=1/10 iteration=109/34336 train_loss=1.0881 took 0.4639 secs\n",
      "epoch=1/10 iteration=110/34336 train_loss=1.1073 took 0.4018 secs\n",
      "epoch=1/10 iteration=111/34336 train_loss=1.1112 took 0.5643 secs\n",
      "epoch=1/10 iteration=112/34336 train_loss=1.0874 took 0.4503 secs\n",
      "epoch=1/10 iteration=113/34336 train_loss=1.0989 took 0.3435 secs\n",
      "epoch=1/10 iteration=114/34336 train_loss=1.1064 took 0.5895 secs\n",
      "epoch=1/10 iteration=115/34336 train_loss=1.1050 took 0.5200 secs\n",
      "epoch=1/10 iteration=116/34336 train_loss=1.1037 took 0.4278 secs\n",
      "epoch=1/10 iteration=117/34336 train_loss=1.1020 took 0.3685 secs\n",
      "epoch=1/10 iteration=118/34336 train_loss=1.1135 took 0.4686 secs\n",
      "epoch=1/10 iteration=119/34336 train_loss=1.1049 took 0.5075 secs\n",
      "epoch=1/10 iteration=120/34336 train_loss=1.0938 took 0.3890 secs\n",
      "epoch=1/10 iteration=121/34336 train_loss=1.1019 took 0.4015 secs\n",
      "epoch=1/10 iteration=122/34336 train_loss=1.0926 took 0.3592 secs\n",
      "epoch=1/10 iteration=123/34336 train_loss=1.0897 took 0.3707 secs\n",
      "epoch=1/10 iteration=124/34336 train_loss=1.0992 took 0.3160 secs\n",
      "epoch=1/10 iteration=125/34336 train_loss=1.1052 took 0.4196 secs\n",
      "epoch=1/10 iteration=126/34336 train_loss=1.1022 took 0.3489 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1/10 iteration=127/34336 train_loss=1.0977 took 0.3690 secs\n",
      "epoch=1/10 iteration=128/34336 train_loss=1.1001 took 0.3332 secs\n",
      "epoch=1/10 iteration=129/34336 train_loss=1.1000 took 0.3713 secs\n",
      "epoch=1/10 iteration=130/34336 train_loss=1.1171 took 0.4331 secs\n",
      "epoch=1/10 iteration=131/34336 train_loss=1.1060 took 0.3523 secs\n",
      "epoch=1/10 iteration=132/34336 train_loss=1.0820 took 0.3680 secs\n",
      "epoch=1/10 iteration=133/34336 train_loss=1.1085 took 0.3665 secs\n",
      "epoch=1/10 iteration=134/34336 train_loss=1.0986 took 0.4781 secs\n",
      "epoch=1/10 iteration=135/34336 train_loss=1.1000 took 0.4283 secs\n",
      "epoch=1/10 iteration=136/34336 train_loss=1.1067 took 0.3226 secs\n",
      "epoch=1/10 iteration=137/34336 train_loss=1.1103 took 0.3258 secs\n",
      "epoch=1/10 iteration=138/34336 train_loss=1.0962 took 0.4284 secs\n",
      "epoch=1/10 iteration=139/34336 train_loss=1.1038 took 0.3075 secs\n",
      "epoch=1/10 iteration=140/34336 train_loss=1.0924 took 0.5742 secs\n",
      "epoch=1/10 iteration=141/34336 train_loss=1.0968 took 0.3720 secs\n",
      "epoch=1/10 iteration=142/34336 train_loss=1.1088 took 0.4559 secs\n",
      "epoch=1/10 iteration=143/34336 train_loss=1.1121 took 0.4446 secs\n",
      "epoch=1/10 iteration=144/34336 train_loss=1.1004 took 0.4191 secs\n",
      "epoch=1/10 iteration=145/34336 train_loss=1.0921 took 0.3738 secs\n",
      "epoch=1/10 iteration=146/34336 train_loss=1.0988 took 0.4490 secs\n",
      "epoch=1/10 iteration=147/34336 train_loss=1.1148 took 0.4731 secs\n",
      "epoch=1/10 iteration=148/34336 train_loss=1.0956 took 0.4257 secs\n",
      "epoch=1/10 iteration=149/34336 train_loss=1.0939 took 0.3619 secs\n",
      "epoch=1/10 iteration=150/34336 train_loss=1.1070 took 0.4864 secs\n",
      "epoch=1/10 iteration=151/34336 train_loss=1.1120 took 0.8314 secs\n",
      "epoch=1/10 iteration=152/34336 train_loss=1.0891 took 1.0219 secs\n",
      "epoch=1/10 iteration=153/34336 train_loss=1.1096 took 0.5619 secs\n",
      "epoch=1/10 iteration=154/34336 train_loss=1.0957 took 0.5976 secs\n",
      "epoch=1/10 iteration=155/34336 train_loss=1.1203 took 0.4299 secs\n",
      "epoch=1/10 iteration=156/34336 train_loss=1.0929 took 0.3459 secs\n",
      "epoch=1/10 iteration=157/34336 train_loss=1.0953 took 0.3814 secs\n",
      "epoch=1/10 iteration=158/34336 train_loss=1.1002 took 0.3871 secs\n",
      "epoch=1/10 iteration=159/34336 train_loss=1.1134 took 0.3645 secs\n",
      "epoch=1/10 iteration=160/34336 train_loss=1.0944 took 0.4631 secs\n",
      "epoch=1/10 iteration=161/34336 train_loss=1.1052 took 0.4739 secs\n",
      "epoch=1/10 iteration=162/34336 train_loss=1.0946 took 0.3923 secs\n",
      "epoch=1/10 iteration=163/34336 train_loss=1.0988 took 0.3150 secs\n",
      "epoch=1/10 iteration=164/34336 train_loss=1.1074 took 0.3969 secs\n",
      "epoch=1/10 iteration=165/34336 train_loss=1.1000 took 0.4340 secs\n",
      "epoch=1/10 iteration=166/34336 train_loss=1.1008 took 0.4090 secs\n",
      "epoch=1/10 iteration=167/34336 train_loss=1.0944 took 0.4698 secs\n",
      "epoch=1/10 iteration=168/34336 train_loss=1.1082 took 0.4224 secs\n",
      "epoch=1/10 iteration=169/34336 train_loss=1.0952 took 0.3698 secs\n",
      "epoch=1/10 iteration=170/34336 train_loss=1.1138 took 0.3590 secs\n",
      "epoch=1/10 iteration=171/34336 train_loss=1.0862 took 0.3599 secs\n",
      "epoch=1/10 iteration=172/34336 train_loss=1.1041 took 0.4292 secs\n",
      "epoch=1/10 iteration=173/34336 train_loss=1.0962 took 0.3958 secs\n",
      "epoch=1/10 iteration=174/34336 train_loss=1.0934 took 0.3984 secs\n",
      "epoch=1/10 iteration=175/34336 train_loss=1.1000 took 0.5009 secs\n",
      "epoch=1/10 iteration=176/34336 train_loss=1.1099 took 0.7023 secs\n",
      "epoch=1/10 iteration=177/34336 train_loss=1.0933 took 0.3957 secs\n",
      "epoch=1/10 iteration=178/34336 train_loss=1.0846 took 0.4439 secs\n",
      "epoch=1/10 iteration=179/34336 train_loss=1.1050 took 0.4772 secs\n",
      "epoch=1/10 iteration=180/34336 train_loss=1.0939 took 0.4112 secs\n",
      "epoch=1/10 iteration=181/34336 train_loss=1.1041 took 0.4534 secs\n",
      "epoch=1/10 iteration=182/34336 train_loss=1.1081 took 0.2977 secs\n",
      "epoch=1/10 iteration=183/34336 train_loss=1.1030 took 0.3391 secs\n",
      "epoch=1/10 iteration=184/34336 train_loss=1.1020 took 0.3900 secs\n",
      "epoch=1/10 iteration=185/34336 train_loss=1.1050 took 0.5116 secs\n",
      "epoch=1/10 iteration=186/34336 train_loss=1.0910 took 0.4526 secs\n",
      "epoch=1/10 iteration=187/34336 train_loss=1.1065 took 0.3490 secs\n",
      "epoch=1/10 iteration=188/34336 train_loss=1.1072 took 0.3207 secs\n",
      "epoch=1/10 iteration=189/34336 train_loss=1.1047 took 0.7703 secs\n",
      "epoch=1/10 iteration=190/34336 train_loss=1.1049 took 0.3632 secs\n",
      "epoch=1/10 iteration=191/34336 train_loss=1.1032 took 0.4395 secs\n",
      "epoch=1/10 iteration=192/34336 train_loss=1.0998 took 0.3319 secs\n",
      "epoch=1/10 iteration=193/34336 train_loss=1.1026 took 0.3687 secs\n",
      "epoch=1/10 iteration=194/34336 train_loss=1.0998 took 0.5562 secs\n",
      "epoch=1/10 iteration=195/34336 train_loss=1.1010 took 0.4586 secs\n",
      "epoch=1/10 iteration=196/34336 train_loss=1.1006 took 0.3886 secs\n",
      "epoch=1/10 iteration=197/34336 train_loss=1.0947 took 0.4005 secs\n",
      "epoch=1/10 iteration=198/34336 train_loss=1.0961 took 0.3665 secs\n",
      "epoch=1/10 iteration=199/34336 train_loss=1.1053 took 0.5345 secs\n",
      "epoch=1/10 iteration=200/34336 train_loss=1.0978 took 0.3852 secs\n",
      "epoch=1/10 iteration=201/34336 train_loss=1.1022 took 0.4984 secs\n",
      "epoch=1/10 iteration=202/34336 train_loss=1.0953 took 0.4563 secs\n",
      "epoch=1/10 iteration=203/34336 train_loss=1.0933 took 0.3481 secs\n",
      "epoch=1/10 iteration=204/34336 train_loss=1.1018 took 0.3470 secs\n",
      "epoch=1/10 iteration=205/34336 train_loss=1.0992 took 0.3983 secs\n",
      "epoch=1/10 iteration=206/34336 train_loss=1.1008 took 0.5261 secs\n",
      "epoch=1/10 iteration=207/34336 train_loss=1.1128 took 0.4260 secs\n",
      "epoch=1/10 iteration=208/34336 train_loss=1.0930 took 0.5135 secs\n",
      "epoch=1/10 iteration=209/34336 train_loss=1.1007 took 0.5676 secs\n",
      "epoch=1/10 iteration=210/34336 train_loss=1.1023 took 0.4180 secs\n",
      "epoch=1/10 iteration=211/34336 train_loss=1.0989 took 0.4150 secs\n",
      "epoch=1/10 iteration=212/34336 train_loss=1.0977 took 0.4599 secs\n",
      "epoch=1/10 iteration=213/34336 train_loss=1.1075 took 0.3030 secs\n",
      "epoch=1/10 iteration=214/34336 train_loss=1.0953 took 0.4299 secs\n",
      "epoch=1/10 iteration=215/34336 train_loss=1.0957 took 0.3507 secs\n",
      "epoch=1/10 iteration=216/34336 train_loss=1.1287 took 0.3557 secs\n",
      "epoch=1/10 iteration=217/34336 train_loss=1.0866 took 0.2658 secs\n",
      "epoch=1/10 iteration=218/34336 train_loss=1.1046 took 0.3103 secs\n",
      "epoch=1/10 iteration=219/34336 train_loss=1.0922 took 0.3199 secs\n",
      "epoch=1/10 iteration=220/34336 train_loss=1.0989 took 0.4503 secs\n",
      "epoch=1/10 iteration=221/34336 train_loss=1.0984 took 0.3124 secs\n",
      "epoch=1/10 iteration=222/34336 train_loss=1.1026 took 0.3042 secs\n",
      "epoch=1/10 iteration=223/34336 train_loss=1.0947 took 0.3735 secs\n",
      "epoch=1/10 iteration=224/34336 train_loss=1.1020 took 0.3546 secs\n",
      "epoch=1/10 iteration=225/34336 train_loss=1.0973 took 0.4304 secs\n",
      "epoch=1/10 iteration=226/34336 train_loss=1.0913 took 0.3574 secs\n",
      "epoch=1/10 iteration=227/34336 train_loss=1.1158 took 0.3495 secs\n",
      "epoch=1/10 iteration=228/34336 train_loss=1.1026 took 0.4499 secs\n",
      "epoch=1/10 iteration=229/34336 train_loss=1.1040 took 0.4554 secs\n",
      "epoch=1/10 iteration=230/34336 train_loss=1.0993 took 0.3436 secs\n",
      "epoch=1/10 iteration=231/34336 train_loss=1.1014 took 0.3038 secs\n",
      "epoch=1/10 iteration=232/34336 train_loss=1.0984 took 0.3752 secs\n",
      "epoch=1/10 iteration=233/34336 train_loss=1.1080 took 0.3630 secs\n",
      "epoch=1/10 iteration=234/34336 train_loss=1.0993 took 0.3805 secs\n",
      "epoch=1/10 iteration=235/34336 train_loss=1.0998 took 0.3895 secs\n",
      "epoch=1/10 iteration=236/34336 train_loss=1.0978 took 0.3260 secs\n",
      "epoch=1/10 iteration=237/34336 train_loss=1.0960 took 0.5323 secs\n",
      "epoch=1/10 iteration=238/34336 train_loss=1.0982 took 0.4673 secs\n",
      "epoch=1/10 iteration=239/34336 train_loss=1.0866 took 0.4086 secs\n",
      "epoch=1/10 iteration=240/34336 train_loss=1.1131 took 0.3623 secs\n",
      "epoch=1/10 iteration=241/34336 train_loss=1.0943 took 0.3509 secs\n",
      "epoch=1/10 iteration=242/34336 train_loss=1.1069 took 0.3771 secs\n",
      "epoch=1/10 iteration=243/34336 train_loss=1.0951 took 0.3883 secs\n",
      "epoch=1/10 iteration=244/34336 train_loss=1.1041 took 0.3703 secs\n",
      "epoch=1/10 iteration=245/34336 train_loss=1.0957 took 0.5340 secs\n",
      "epoch=1/10 iteration=246/34336 train_loss=1.1023 took 0.3159 secs\n",
      "epoch=1/10 iteration=247/34336 train_loss=1.0979 took 0.3706 secs\n",
      "epoch=1/10 iteration=248/34336 train_loss=1.0918 took 0.5184 secs\n",
      "epoch=1/10 iteration=249/34336 train_loss=1.1046 took 0.3661 secs\n",
      "epoch=1/10 iteration=250/34336 train_loss=1.1029 took 0.3499 secs\n",
      "epoch=1/10 iteration=251/34336 train_loss=1.0984 took 0.3130 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1/10 iteration=252/34336 train_loss=1.0936 took 0.3439 secs\n",
      "epoch=1/10 iteration=253/34336 train_loss=1.1012 took 0.3577 secs\n",
      "epoch=1/10 iteration=254/34336 train_loss=1.0926 took 0.3493 secs\n",
      "epoch=1/10 iteration=255/34336 train_loss=1.1052 took 0.4273 secs\n",
      "epoch=1/10 iteration=256/34336 train_loss=1.1002 took 0.4254 secs\n",
      "epoch=1/10 iteration=257/34336 train_loss=1.1050 took 0.5002 secs\n",
      "epoch=1/10 iteration=258/34336 train_loss=1.0996 took 0.5125 secs\n",
      "epoch=1/10 iteration=259/34336 train_loss=1.0950 took 0.4493 secs\n",
      "epoch=1/10 iteration=260/34336 train_loss=1.1405 took 0.4415 secs\n",
      "epoch=1/10 iteration=261/34336 train_loss=1.0980 took 0.4050 secs\n",
      "epoch=1/10 iteration=262/34336 train_loss=1.0996 took 0.4618 secs\n",
      "epoch=1/10 iteration=263/34336 train_loss=1.1056 took 0.4704 secs\n",
      "epoch=1/10 iteration=264/34336 train_loss=1.0934 took 0.3020 secs\n",
      "epoch=1/10 iteration=265/34336 train_loss=1.0994 took 0.4040 secs\n",
      "epoch=1/10 iteration=266/34336 train_loss=1.0997 took 0.3608 secs\n",
      "epoch=1/10 iteration=267/34336 train_loss=1.1128 took 0.4224 secs\n",
      "epoch=1/10 iteration=268/34336 train_loss=1.0976 took 0.3924 secs\n",
      "epoch=1/10 iteration=269/34336 train_loss=1.0966 took 0.3681 secs\n",
      "epoch=1/10 iteration=270/34336 train_loss=1.1062 took 0.3367 secs\n",
      "epoch=1/10 iteration=271/34336 train_loss=1.1017 took 0.3535 secs\n",
      "epoch=1/10 iteration=272/34336 train_loss=1.0965 took 0.4700 secs\n",
      "epoch=1/10 iteration=273/34336 train_loss=1.0992 took 0.3503 secs\n",
      "epoch=1/10 iteration=274/34336 train_loss=1.0864 took 0.4993 secs\n",
      "epoch=1/10 iteration=275/34336 train_loss=1.1041 took 0.4562 secs\n",
      "epoch=1/10 iteration=276/34336 train_loss=1.1073 took 0.4206 secs\n",
      "epoch=1/10 iteration=277/34336 train_loss=1.0925 took 0.3904 secs\n",
      "epoch=1/10 iteration=278/34336 train_loss=1.0967 took 0.3874 secs\n",
      "epoch=1/10 iteration=279/34336 train_loss=1.0988 took 0.3829 secs\n",
      "epoch=1/10 iteration=280/34336 train_loss=1.1008 took 0.3378 secs\n",
      "epoch=1/10 iteration=281/34336 train_loss=1.0979 took 0.4060 secs\n",
      "epoch=1/10 iteration=282/34336 train_loss=1.1077 took 0.3828 secs\n",
      "epoch=1/10 iteration=283/34336 train_loss=1.1054 took 0.4190 secs\n",
      "epoch=1/10 iteration=284/34336 train_loss=1.0980 took 0.5008 secs\n",
      "epoch=1/10 iteration=285/34336 train_loss=1.0951 took 0.4260 secs\n",
      "epoch=1/10 iteration=286/34336 train_loss=1.0916 took 0.3594 secs\n",
      "epoch=1/10 iteration=287/34336 train_loss=1.1120 took 0.5055 secs\n",
      "epoch=1/10 iteration=288/34336 train_loss=1.0909 took 0.3568 secs\n",
      "epoch=1/10 iteration=289/34336 train_loss=1.1043 took 0.3338 secs\n",
      "epoch=1/10 iteration=290/34336 train_loss=1.0959 took 0.3740 secs\n",
      "epoch=1/10 iteration=291/34336 train_loss=1.1007 took 0.3683 secs\n",
      "epoch=1/10 iteration=292/34336 train_loss=1.0963 took 0.3682 secs\n",
      "epoch=1/10 iteration=293/34336 train_loss=1.1103 took 0.3129 secs\n",
      "epoch=1/10 iteration=294/34336 train_loss=1.0918 took 0.3645 secs\n",
      "epoch=1/10 iteration=295/34336 train_loss=1.1074 took 0.3697 secs\n",
      "epoch=1/10 iteration=296/34336 train_loss=1.0977 took 0.3803 secs\n",
      "epoch=1/10 iteration=297/34336 train_loss=1.1016 took 0.3671 secs\n",
      "epoch=1/10 iteration=298/34336 train_loss=1.0981 took 0.4237 secs\n",
      "epoch=1/10 iteration=299/34336 train_loss=1.1030 took 0.3897 secs\n",
      "epoch=1/10 iteration=300/34336 train_loss=1.0983 took 0.3843 secs\n",
      "epoch=1/10 iteration=301/34336 train_loss=1.0955 took 0.3971 secs\n",
      "epoch=1/10 iteration=302/34336 train_loss=1.1045 took 0.3267 secs\n",
      "epoch=1/10 iteration=303/34336 train_loss=1.0947 took 0.4018 secs\n",
      "epoch=1/10 iteration=304/34336 train_loss=1.0978 took 0.3368 secs\n",
      "epoch=1/10 iteration=305/34336 train_loss=1.1009 took 0.4548 secs\n",
      "epoch=1/10 iteration=306/34336 train_loss=1.1002 took 0.3660 secs\n",
      "epoch=1/10 iteration=307/34336 train_loss=1.0977 took 0.3753 secs\n",
      "epoch=1/10 iteration=308/34336 train_loss=1.0938 took 0.3875 secs\n",
      "epoch=1/10 iteration=309/34336 train_loss=1.1001 took 0.3664 secs\n",
      "epoch=1/10 iteration=310/34336 train_loss=1.0957 took 0.3607 secs\n",
      "epoch=1/10 iteration=311/34336 train_loss=1.1071 took 0.3198 secs\n",
      "epoch=1/10 iteration=312/34336 train_loss=1.1007 took 0.3729 secs\n",
      "epoch=1/10 iteration=313/34336 train_loss=1.0949 took 0.3756 secs\n",
      "epoch=1/10 iteration=314/34336 train_loss=1.1008 took 0.4537 secs\n",
      "epoch=1/10 iteration=315/34336 train_loss=1.0997 took 0.4560 secs\n",
      "epoch=1/10 iteration=316/34336 train_loss=1.0973 took 0.4527 secs\n",
      "epoch=1/10 iteration=317/34336 train_loss=1.1096 took 0.4195 secs\n",
      "epoch=1/10 iteration=318/34336 train_loss=1.0912 took 0.4163 secs\n",
      "epoch=1/10 iteration=319/34336 train_loss=1.1359 took 0.3643 secs\n",
      "epoch=1/10 iteration=320/34336 train_loss=1.0955 took 0.3380 secs\n",
      "epoch=1/10 iteration=321/34336 train_loss=1.0996 took 0.3491 secs\n",
      "epoch=1/10 iteration=322/34336 train_loss=1.1046 took 0.3673 secs\n",
      "epoch=1/10 iteration=323/34336 train_loss=1.0993 took 0.2845 secs\n",
      "epoch=1/10 iteration=324/34336 train_loss=1.0964 took 0.4458 secs\n",
      "epoch=1/10 iteration=325/34336 train_loss=1.1039 took 0.3738 secs\n",
      "epoch=1/10 iteration=326/34336 train_loss=1.1010 took 0.4134 secs\n",
      "epoch=1/10 iteration=327/34336 train_loss=1.0997 took 0.4640 secs\n",
      "epoch=1/10 iteration=328/34336 train_loss=1.0924 took 0.4937 secs\n",
      "epoch=1/10 iteration=329/34336 train_loss=1.0990 took 0.3923 secs\n",
      "epoch=1/10 iteration=330/34336 train_loss=1.1056 took 0.4370 secs\n",
      "epoch=1/10 iteration=331/34336 train_loss=1.0965 took 0.3895 secs\n",
      "epoch=1/10 iteration=332/34336 train_loss=1.1022 took 0.5347 secs\n",
      "epoch=1/10 iteration=333/34336 train_loss=1.1195 took 0.4322 secs\n",
      "epoch=1/10 iteration=334/34336 train_loss=1.0993 took 0.3765 secs\n",
      "epoch=1/10 iteration=335/34336 train_loss=1.1035 took 0.7427 secs\n",
      "epoch=1/10 iteration=336/34336 train_loss=1.1000 took 0.3269 secs\n",
      "epoch=1/10 iteration=337/34336 train_loss=1.1034 took 0.4051 secs\n",
      "epoch=1/10 iteration=338/34336 train_loss=1.0927 took 0.5662 secs\n",
      "epoch=1/10 iteration=339/34336 train_loss=1.1293 took 0.7235 secs\n",
      "epoch=1/10 iteration=340/34336 train_loss=1.1029 took 0.7111 secs\n",
      "epoch=1/10 iteration=341/34336 train_loss=1.1031 took 0.4998 secs\n",
      "epoch=1/10 iteration=342/34336 train_loss=1.0996 took 0.4171 secs\n",
      "epoch=1/10 iteration=343/34336 train_loss=1.0979 took 0.4213 secs\n",
      "epoch=1/10 iteration=344/34336 train_loss=1.0959 took 0.4932 secs\n",
      "epoch=1/10 iteration=345/34336 train_loss=1.1061 took 0.3927 secs\n",
      "epoch=1/10 iteration=346/34336 train_loss=1.1016 took 0.3749 secs\n",
      "epoch=1/10 iteration=347/34336 train_loss=1.0960 took 0.3126 secs\n",
      "epoch=1/10 iteration=348/34336 train_loss=1.0923 took 0.3156 secs\n",
      "epoch=1/10 iteration=349/34336 train_loss=1.0986 took 0.3321 secs\n",
      "epoch=1/10 iteration=350/34336 train_loss=1.1043 took 0.5057 secs\n",
      "epoch=1/10 iteration=351/34336 train_loss=1.0923 took 0.3792 secs\n",
      "epoch=1/10 iteration=352/34336 train_loss=1.1068 took 0.3320 secs\n",
      "epoch=1/10 iteration=353/34336 train_loss=1.0857 took 0.3433 secs\n",
      "epoch=1/10 iteration=354/34336 train_loss=1.1144 took 0.5717 secs\n",
      "epoch=1/10 iteration=355/34336 train_loss=1.1091 took 0.4482 secs\n",
      "epoch=1/10 iteration=356/34336 train_loss=1.0883 took 0.4316 secs\n",
      "epoch=1/10 iteration=357/34336 train_loss=1.1136 took 0.4911 secs\n",
      "epoch=1/10 iteration=358/34336 train_loss=1.0997 took 0.6598 secs\n",
      "epoch=1/10 iteration=359/34336 train_loss=1.1048 took 0.5163 secs\n",
      "epoch=1/10 iteration=360/34336 train_loss=1.0966 took 0.5501 secs\n",
      "epoch=1/10 iteration=361/34336 train_loss=1.0977 took 0.6178 secs\n",
      "epoch=1/10 iteration=362/34336 train_loss=1.0972 took 0.6692 secs\n",
      "epoch=1/10 iteration=363/34336 train_loss=1.0954 took 0.4525 secs\n",
      "epoch=1/10 iteration=364/34336 train_loss=1.1147 took 0.8454 secs\n",
      "epoch=1/10 iteration=365/34336 train_loss=1.0980 took 0.5202 secs\n",
      "epoch=1/10 iteration=366/34336 train_loss=1.1066 took 0.4197 secs\n",
      "epoch=1/10 iteration=367/34336 train_loss=1.1159 took 0.4018 secs\n",
      "epoch=1/10 iteration=368/34336 train_loss=1.1117 took 0.3405 secs\n",
      "epoch=1/10 iteration=369/34336 train_loss=1.0970 took 0.2932 secs\n",
      "epoch=1/10 iteration=370/34336 train_loss=1.0979 took 0.3242 secs\n",
      "epoch=1/10 iteration=371/34336 train_loss=1.0988 took 0.4353 secs\n",
      "epoch=1/10 iteration=372/34336 train_loss=1.0986 took 0.4344 secs\n",
      "epoch=1/10 iteration=373/34336 train_loss=1.1012 took 0.3885 secs\n",
      "epoch=1/10 iteration=374/34336 train_loss=1.0940 took 0.4305 secs\n",
      "epoch=1/10 iteration=375/34336 train_loss=1.1131 took 0.3557 secs\n",
      "epoch=1/10 iteration=376/34336 train_loss=1.1002 took 0.3867 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1/10 iteration=377/34336 train_loss=1.1098 took 0.4717 secs\n",
      "epoch=1/10 iteration=378/34336 train_loss=1.0992 took 0.3339 secs\n",
      "epoch=1/10 iteration=379/34336 train_loss=1.1069 took 0.4697 secs\n",
      "epoch=1/10 iteration=380/34336 train_loss=1.0999 took 0.3949 secs\n",
      "epoch=1/10 iteration=381/34336 train_loss=1.0997 took 0.3452 secs\n",
      "epoch=1/10 iteration=382/34336 train_loss=1.0939 took 0.3781 secs\n",
      "epoch=1/10 iteration=383/34336 train_loss=1.1031 took 0.3377 secs\n",
      "epoch=1/10 iteration=384/34336 train_loss=1.1020 took 0.3799 secs\n",
      "epoch=1/10 iteration=385/34336 train_loss=1.1027 took 0.2858 secs\n",
      "epoch=1/10 iteration=386/34336 train_loss=1.1038 took 0.3774 secs\n",
      "epoch=1/10 iteration=387/34336 train_loss=1.1078 took 0.3635 secs\n",
      "epoch=1/10 iteration=388/34336 train_loss=1.0969 took 0.3824 secs\n",
      "epoch=1/10 iteration=389/34336 train_loss=1.1022 took 0.4305 secs\n",
      "epoch=1/10 iteration=390/34336 train_loss=1.1000 took 0.5288 secs\n",
      "epoch=1/10 iteration=391/34336 train_loss=1.1014 took 0.4507 secs\n",
      "epoch=1/10 iteration=392/34336 train_loss=1.1030 took 0.4375 secs\n",
      "epoch=1/10 iteration=393/34336 train_loss=1.0964 took 0.5660 secs\n",
      "epoch=1/10 iteration=394/34336 train_loss=1.1043 took 0.8007 secs\n",
      "epoch=1/10 iteration=395/34336 train_loss=1.1106 took 0.4205 secs\n",
      "epoch=1/10 iteration=396/34336 train_loss=1.0959 took 0.4303 secs\n",
      "epoch=1/10 iteration=397/34336 train_loss=1.1019 took 0.4737 secs\n",
      "epoch=1/10 iteration=398/34336 train_loss=1.0968 took 0.4551 secs\n",
      "epoch=1/10 iteration=399/34336 train_loss=1.0966 took 0.4295 secs\n",
      "epoch=1/10 iteration=400/34336 train_loss=1.1031 took 0.3727 secs\n",
      "epoch=1/10 iteration=401/34336 train_loss=1.0941 took 0.4060 secs\n",
      "epoch=1/10 iteration=402/34336 train_loss=1.1108 took 0.3301 secs\n",
      "epoch=1/10 iteration=403/34336 train_loss=1.0981 took 0.4253 secs\n",
      "epoch=1/10 iteration=404/34336 train_loss=1.0985 took 0.3493 secs\n",
      "epoch=1/10 iteration=405/34336 train_loss=1.1029 took 0.5357 secs\n",
      "epoch=1/10 iteration=406/34336 train_loss=1.0998 took 0.5299 secs\n",
      "epoch=1/10 iteration=407/34336 train_loss=1.1013 took 0.4156 secs\n",
      "epoch=1/10 iteration=408/34336 train_loss=1.0955 took 0.4299 secs\n",
      "epoch=1/10 iteration=409/34336 train_loss=1.1011 took 0.4389 secs\n",
      "epoch=1/10 iteration=410/34336 train_loss=1.1047 took 0.3640 secs\n",
      "epoch=1/10 iteration=411/34336 train_loss=1.1030 took 0.5361 secs\n",
      "epoch=1/10 iteration=412/34336 train_loss=1.1029 took 0.3663 secs\n",
      "epoch=1/10 iteration=413/34336 train_loss=1.1000 took 0.4458 secs\n",
      "epoch=1/10 iteration=414/34336 train_loss=1.1007 took 0.3741 secs\n",
      "epoch=1/10 iteration=415/34336 train_loss=1.1076 took 0.3493 secs\n",
      "epoch=1/10 iteration=416/34336 train_loss=1.1000 took 0.6370 secs\n",
      "epoch=1/10 iteration=417/34336 train_loss=1.0959 took 0.4293 secs\n",
      "epoch=1/10 iteration=418/34336 train_loss=1.1003 took 0.3280 secs\n",
      "epoch=1/10 iteration=419/34336 train_loss=1.1003 took 0.3254 secs\n",
      "epoch=1/10 iteration=420/34336 train_loss=1.1064 took 0.7562 secs\n",
      "epoch=1/10 iteration=421/34336 train_loss=1.1014 took 0.3258 secs\n",
      "epoch=1/10 iteration=422/34336 train_loss=1.1046 took 0.3771 secs\n",
      "epoch=1/10 iteration=423/34336 train_loss=1.1018 took 0.5114 secs\n",
      "epoch=1/10 iteration=424/34336 train_loss=1.0997 took 0.4093 secs\n",
      "epoch=1/10 iteration=425/34336 train_loss=1.0991 took 0.2785 secs\n",
      "epoch=1/10 iteration=426/34336 train_loss=1.1028 took 0.3252 secs\n",
      "epoch=1/10 iteration=427/34336 train_loss=1.0990 took 0.3787 secs\n",
      "epoch=1/10 iteration=428/34336 train_loss=1.0962 took 0.4409 secs\n",
      "epoch=1/10 iteration=429/34336 train_loss=1.1001 took 0.3220 secs\n",
      "epoch=1/10 iteration=430/34336 train_loss=1.1024 took 0.4457 secs\n",
      "epoch=1/10 iteration=431/34336 train_loss=1.0978 took 0.3431 secs\n",
      "epoch=1/10 iteration=432/34336 train_loss=1.0989 took 0.4972 secs\n",
      "epoch=1/10 iteration=433/34336 train_loss=1.0993 took 0.5310 secs\n",
      "epoch=1/10 iteration=434/34336 train_loss=1.1031 took 0.6418 secs\n",
      "epoch=1/10 iteration=435/34336 train_loss=1.0982 took 0.4264 secs\n",
      "epoch=1/10 iteration=436/34336 train_loss=1.0986 took 0.4585 secs\n",
      "epoch=1/10 iteration=437/34336 train_loss=1.1190 took 0.6567 secs\n",
      "epoch=1/10 iteration=438/34336 train_loss=1.1036 took 0.7256 secs\n",
      "epoch=1/10 iteration=439/34336 train_loss=1.0967 took 0.5054 secs\n",
      "epoch=1/10 iteration=440/34336 train_loss=1.1032 took 0.5009 secs\n",
      "epoch=1/10 iteration=441/34336 train_loss=1.0995 took 0.6535 secs\n",
      "epoch=1/10 iteration=442/34336 train_loss=1.0978 took 0.5762 secs\n",
      "epoch=1/10 iteration=443/34336 train_loss=1.1041 took 0.3965 secs\n",
      "epoch=1/10 iteration=444/34336 train_loss=1.0972 took 0.4002 secs\n",
      "epoch=1/10 iteration=445/34336 train_loss=1.1030 took 0.3830 secs\n",
      "epoch=1/10 iteration=446/34336 train_loss=1.0970 took 0.3815 secs\n",
      "epoch=1/10 iteration=447/34336 train_loss=1.0992 took 0.4526 secs\n",
      "epoch=1/10 iteration=448/34336 train_loss=1.1071 took 0.4418 secs\n",
      "epoch=1/10 iteration=449/34336 train_loss=1.1008 took 0.4069 secs\n",
      "epoch=1/10 iteration=450/34336 train_loss=1.1060 took 0.3892 secs\n",
      "epoch=1/10 iteration=451/34336 train_loss=1.0990 took 0.4351 secs\n",
      "epoch=1/10 iteration=452/34336 train_loss=1.0976 took 0.4161 secs\n",
      "epoch=1/10 iteration=453/34336 train_loss=1.1074 took 0.4988 secs\n",
      "epoch=1/10 iteration=454/34336 train_loss=1.0987 took 0.3899 secs\n",
      "epoch=1/10 iteration=455/34336 train_loss=1.0963 took 0.2858 secs\n",
      "epoch=1/10 iteration=456/34336 train_loss=1.1074 took 0.4572 secs\n",
      "epoch=1/10 iteration=457/34336 train_loss=1.1024 took 0.3466 secs\n",
      "epoch=1/10 iteration=458/34336 train_loss=1.0998 took 0.5279 secs\n",
      "epoch=1/10 iteration=459/34336 train_loss=1.1132 took 0.4840 secs\n",
      "epoch=1/10 iteration=460/34336 train_loss=1.0961 took 0.7385 secs\n",
      "epoch=1/10 iteration=461/34336 train_loss=1.1038 took 0.4187 secs\n",
      "epoch=1/10 iteration=462/34336 train_loss=1.0964 took 0.3231 secs\n",
      "epoch=1/10 iteration=463/34336 train_loss=1.0946 took 0.4429 secs\n",
      "epoch=1/10 iteration=464/34336 train_loss=1.1035 took 0.8130 secs\n",
      "epoch=1/10 iteration=465/34336 train_loss=1.1034 took 0.5787 secs\n",
      "epoch=1/10 iteration=466/34336 train_loss=1.0995 took 0.4562 secs\n",
      "epoch=1/10 iteration=467/34336 train_loss=1.0983 took 0.2873 secs\n",
      "epoch=1/10 iteration=468/34336 train_loss=1.0988 took 0.5454 secs\n",
      "epoch=1/10 iteration=469/34336 train_loss=1.1016 took 0.5204 secs\n",
      "epoch=1/10 iteration=470/34336 train_loss=1.0958 took 0.3731 secs\n",
      "epoch=1/10 iteration=471/34336 train_loss=1.1001 took 0.3458 secs\n",
      "epoch=1/10 iteration=472/34336 train_loss=1.0966 took 0.4669 secs\n",
      "epoch=1/10 iteration=473/34336 train_loss=1.0971 took 0.3302 secs\n",
      "epoch=1/10 iteration=474/34336 train_loss=1.1000 took 0.3960 secs\n",
      "epoch=1/10 iteration=475/34336 train_loss=1.1119 took 0.3650 secs\n",
      "epoch=1/10 iteration=476/34336 train_loss=1.0960 took 0.4360 secs\n",
      "epoch=1/10 iteration=477/34336 train_loss=1.0974 took 0.4585 secs\n",
      "epoch=1/10 iteration=478/34336 train_loss=1.1032 took 0.3417 secs\n",
      "epoch=1/10 iteration=479/34336 train_loss=1.1075 took 0.3194 secs\n",
      "epoch=1/10 iteration=480/34336 train_loss=1.1040 took 0.5890 secs\n",
      "epoch=1/10 iteration=481/34336 train_loss=1.0994 took 0.4850 secs\n",
      "epoch=1/10 iteration=482/34336 train_loss=1.0980 took 0.6583 secs\n",
      "epoch=1/10 iteration=483/34336 train_loss=1.0998 took 0.3350 secs\n",
      "epoch=1/10 iteration=484/34336 train_loss=1.1072 took 0.3400 secs\n",
      "epoch=1/10 iteration=485/34336 train_loss=1.1087 took 0.3263 secs\n",
      "epoch=1/10 iteration=486/34336 train_loss=1.0953 took 0.4727 secs\n",
      "epoch=1/10 iteration=487/34336 train_loss=1.0953 took 0.3884 secs\n",
      "epoch=1/10 iteration=488/34336 train_loss=1.0985 took 0.4280 secs\n",
      "epoch=1/10 iteration=489/34336 train_loss=1.0981 took 0.4472 secs\n",
      "epoch=1/10 iteration=490/34336 train_loss=1.1006 took 0.3597 secs\n",
      "epoch=1/10 iteration=491/34336 train_loss=1.1001 took 0.3749 secs\n",
      "epoch=1/10 iteration=492/34336 train_loss=1.0986 took 0.3339 secs\n",
      "epoch=1/10 iteration=493/34336 train_loss=1.1202 took 0.4421 secs\n",
      "epoch=1/10 iteration=494/34336 train_loss=1.0977 took 0.4115 secs\n",
      "epoch=1/10 iteration=495/34336 train_loss=1.0970 took 0.3340 secs\n",
      "epoch=1/10 iteration=496/34336 train_loss=1.1057 took 0.4078 secs\n",
      "epoch=1/10 iteration=497/34336 train_loss=1.1001 took 0.6424 secs\n",
      "epoch=1/10 iteration=498/34336 train_loss=1.0983 took 0.3123 secs\n",
      "epoch=1/10 iteration=499/34336 train_loss=1.1103 took 0.4373 secs\n",
      "epoch=1/10 iteration=500/34336 train_loss=1.0971 took 0.4405 secs\n",
      "epoch=1/10 iteration=501/34336 train_loss=1.0963 took 0.3663 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1/10 iteration=502/34336 train_loss=1.1015 took 0.3570 secs\n",
      "epoch=1/10 iteration=503/34336 train_loss=1.1027 took 0.3654 secs\n",
      "epoch=1/10 iteration=504/34336 train_loss=1.0992 took 0.3958 secs\n",
      "epoch=1/10 iteration=505/34336 train_loss=1.1005 took 0.5075 secs\n",
      "epoch=1/10 iteration=506/34336 train_loss=1.0999 took 0.4457 secs\n",
      "epoch=1/10 iteration=507/34336 train_loss=1.1068 took 0.4602 secs\n",
      "epoch=1/10 iteration=508/34336 train_loss=1.1041 took 0.3417 secs\n",
      "epoch=1/10 iteration=509/34336 train_loss=1.0911 took 0.5211 secs\n",
      "epoch=1/10 iteration=510/34336 train_loss=1.1035 took 0.3986 secs\n",
      "epoch=1/10 iteration=511/34336 train_loss=1.1097 took 0.3953 secs\n",
      "epoch=1/10 iteration=512/34336 train_loss=1.0980 took 0.3953 secs\n",
      "epoch=1/10 iteration=513/34336 train_loss=1.0989 took 0.3885 secs\n",
      "epoch=1/10 iteration=514/34336 train_loss=1.1009 took 0.4470 secs\n",
      "epoch=1/10 iteration=515/34336 train_loss=1.1113 took 0.4314 secs\n",
      "epoch=1/10 iteration=516/34336 train_loss=1.0976 took 0.4414 secs\n",
      "epoch=1/10 iteration=517/34336 train_loss=1.1036 took 0.3273 secs\n",
      "epoch=1/10 iteration=518/34336 train_loss=1.0982 took 0.3395 secs\n",
      "epoch=1/10 iteration=519/34336 train_loss=1.0984 took 0.4062 secs\n",
      "epoch=1/10 iteration=520/34336 train_loss=1.1007 took 0.3645 secs\n",
      "epoch=1/10 iteration=521/34336 train_loss=1.0998 took 0.4106 secs\n",
      "epoch=1/10 iteration=522/34336 train_loss=1.0994 took 0.3639 secs\n",
      "epoch=1/10 iteration=523/34336 train_loss=1.0941 took 0.5408 secs\n",
      "epoch=1/10 iteration=524/34336 train_loss=1.1068 took 0.4731 secs\n",
      "epoch=1/10 iteration=525/34336 train_loss=1.0944 took 0.3946 secs\n",
      "epoch=1/10 iteration=526/34336 train_loss=1.1138 took 0.4205 secs\n",
      "epoch=1/10 iteration=527/34336 train_loss=1.0975 took 0.4687 secs\n",
      "epoch=1/10 iteration=528/34336 train_loss=1.1057 took 0.4913 secs\n",
      "epoch=1/10 iteration=529/34336 train_loss=1.0965 took 0.4588 secs\n",
      "epoch=1/10 iteration=530/34336 train_loss=1.1010 took 0.4737 secs\n",
      "epoch=1/10 iteration=531/34336 train_loss=1.0995 took 0.4425 secs\n",
      "epoch=1/10 iteration=532/34336 train_loss=1.0997 took 0.5505 secs\n",
      "epoch=1/10 iteration=533/34336 train_loss=1.0942 took 0.3260 secs\n",
      "epoch=1/10 iteration=534/34336 train_loss=1.1035 took 0.3720 secs\n",
      "epoch=1/10 iteration=535/34336 train_loss=1.0955 took 0.5190 secs\n",
      "epoch=1/10 iteration=536/34336 train_loss=1.1050 took 0.4495 secs\n",
      "epoch=1/10 iteration=537/34336 train_loss=1.1117 took 0.4049 secs\n",
      "epoch=1/10 iteration=538/34336 train_loss=1.0986 took 0.4977 secs\n",
      "epoch=1/10 iteration=539/34336 train_loss=1.0989 took 0.3855 secs\n",
      "epoch=1/10 iteration=540/34336 train_loss=1.0977 took 0.3719 secs\n",
      "epoch=1/10 iteration=541/34336 train_loss=1.1070 took 0.3333 secs\n",
      "epoch=1/10 iteration=542/34336 train_loss=1.0997 took 0.3350 secs\n",
      "epoch=1/10 iteration=543/34336 train_loss=1.0994 took 0.3496 secs\n",
      "epoch=1/10 iteration=544/34336 train_loss=1.0991 took 0.3325 secs\n",
      "epoch=1/10 iteration=545/34336 train_loss=1.0916 took 0.4358 secs\n",
      "epoch=1/10 iteration=546/34336 train_loss=1.1084 took 0.3746 secs\n",
      "epoch=1/10 iteration=547/34336 train_loss=1.1063 took 0.4438 secs\n",
      "epoch=1/10 iteration=548/34336 train_loss=1.0965 took 0.3349 secs\n",
      "epoch=1/10 iteration=549/34336 train_loss=1.1079 took 0.3574 secs\n",
      "epoch=1/10 iteration=550/34336 train_loss=1.0995 took 0.4210 secs\n",
      "epoch=1/10 iteration=551/34336 train_loss=1.1046 took 0.4340 secs\n",
      "epoch=1/10 iteration=552/34336 train_loss=1.1041 took 0.4615 secs\n",
      "epoch=1/10 iteration=553/34336 train_loss=1.0934 took 0.5373 secs\n",
      "epoch=1/10 iteration=554/34336 train_loss=1.1049 took 0.4864 secs\n",
      "epoch=1/10 iteration=555/34336 train_loss=1.1037 took 0.4679 secs\n",
      "epoch=1/10 iteration=556/34336 train_loss=1.0951 took 0.4571 secs\n",
      "epoch=1/10 iteration=557/34336 train_loss=1.1024 took 0.4765 secs\n",
      "epoch=1/10 iteration=558/34336 train_loss=1.1019 took 0.3267 secs\n",
      "epoch=1/10 iteration=559/34336 train_loss=1.0995 took 0.3105 secs\n",
      "epoch=1/10 iteration=560/34336 train_loss=1.1088 took 0.3959 secs\n",
      "epoch=1/10 iteration=561/34336 train_loss=1.0973 took 0.3468 secs\n",
      "epoch=1/10 iteration=562/34336 train_loss=1.0964 took 0.4048 secs\n",
      "epoch=1/10 iteration=563/34336 train_loss=1.1051 took 0.4258 secs\n",
      "epoch=1/10 iteration=564/34336 train_loss=1.0953 took 0.3968 secs\n",
      "epoch=1/10 iteration=565/34336 train_loss=1.1016 took 0.5119 secs\n",
      "epoch=1/10 iteration=566/34336 train_loss=1.1008 took 0.5074 secs\n",
      "epoch=1/10 iteration=567/34336 train_loss=1.0991 took 0.4110 secs\n",
      "epoch=1/10 iteration=568/34336 train_loss=1.1010 took 0.4349 secs\n",
      "epoch=1/10 iteration=569/34336 train_loss=1.0989 took 0.3721 secs\n",
      "epoch=1/10 iteration=570/34336 train_loss=1.1005 took 0.4115 secs\n",
      "epoch=1/10 iteration=571/34336 train_loss=1.0960 took 0.3899 secs\n",
      "epoch=1/10 iteration=572/34336 train_loss=1.0974 took 0.3884 secs\n",
      "epoch=1/10 iteration=573/34336 train_loss=1.0986 took 0.4096 secs\n",
      "epoch=1/10 iteration=574/34336 train_loss=1.1030 took 0.4606 secs\n",
      "epoch=1/10 iteration=575/34336 train_loss=1.1014 took 0.4207 secs\n",
      "epoch=1/10 iteration=576/34336 train_loss=1.0981 took 0.3619 secs\n",
      "epoch=1/10 iteration=577/34336 train_loss=1.0997 took 0.3557 secs\n",
      "epoch=1/10 iteration=578/34336 train_loss=1.1032 took 0.3544 secs\n",
      "epoch=1/10 iteration=579/34336 train_loss=1.1005 took 0.3512 secs\n",
      "epoch=1/10 iteration=580/34336 train_loss=1.1035 took 0.3564 secs\n",
      "epoch=1/10 iteration=581/34336 train_loss=1.0981 took 0.3946 secs\n",
      "epoch=1/10 iteration=582/34336 train_loss=1.0997 took 0.4615 secs\n",
      "epoch=1/10 iteration=583/34336 train_loss=1.1011 took 0.4829 secs\n",
      "epoch=1/10 iteration=584/34336 train_loss=1.1007 took 0.4510 secs\n",
      "epoch=1/10 iteration=585/34336 train_loss=1.0977 took 0.4411 secs\n",
      "epoch=1/10 iteration=586/34336 train_loss=1.1001 took 0.3786 secs\n",
      "epoch=1/10 iteration=587/34336 train_loss=1.0987 took 0.3638 secs\n",
      "epoch=1/10 iteration=588/34336 train_loss=1.0996 took 0.3511 secs\n",
      "epoch=1/10 iteration=589/34336 train_loss=1.0991 took 0.4184 secs\n",
      "epoch=1/10 iteration=590/34336 train_loss=1.0997 took 0.3705 secs\n",
      "epoch=1/10 iteration=591/34336 train_loss=1.0993 took 0.4423 secs\n",
      "epoch=1/10 iteration=592/34336 train_loss=1.1013 took 0.3342 secs\n",
      "epoch=1/10 iteration=593/34336 train_loss=1.1015 took 0.2972 secs\n",
      "epoch=1/10 iteration=594/34336 train_loss=1.0978 took 0.3143 secs\n",
      "epoch=1/10 iteration=595/34336 train_loss=1.0962 took 0.3962 secs\n",
      "epoch=1/10 iteration=596/34336 train_loss=1.1121 took 0.3407 secs\n",
      "epoch=1/10 iteration=597/34336 train_loss=1.1010 took 0.4154 secs\n",
      "epoch=1/10 iteration=598/34336 train_loss=1.1005 took 0.3872 secs\n",
      "epoch=1/10 iteration=599/34336 train_loss=1.1033 took 0.4927 secs\n",
      "epoch=1/10 iteration=600/34336 train_loss=1.0935 took 0.3881 secs\n",
      "epoch=1/10 iteration=601/34336 train_loss=1.0996 took 0.3262 secs\n",
      "epoch=1/10 iteration=602/34336 train_loss=1.0981 took 0.3503 secs\n",
      "epoch=1/10 iteration=603/34336 train_loss=1.0960 took 0.5605 secs\n",
      "epoch=1/10 iteration=604/34336 train_loss=1.0992 took 0.7132 secs\n",
      "epoch=1/10 iteration=605/34336 train_loss=1.1019 took 0.3769 secs\n",
      "epoch=1/10 iteration=606/34336 train_loss=1.1024 took 0.3574 secs\n",
      "epoch=1/10 iteration=607/34336 train_loss=1.0958 took 0.3561 secs\n",
      "epoch=1/10 iteration=608/34336 train_loss=1.1008 took 0.4818 secs\n",
      "epoch=1/10 iteration=609/34336 train_loss=1.1009 took 0.3563 secs\n",
      "epoch=1/10 iteration=610/34336 train_loss=1.0970 took 0.4634 secs\n",
      "epoch=1/10 iteration=611/34336 train_loss=1.1070 took 0.4577 secs\n",
      "epoch=1/10 iteration=612/34336 train_loss=1.1089 took 0.4857 secs\n",
      "epoch=1/10 iteration=613/34336 train_loss=1.0989 took 0.4321 secs\n",
      "epoch=1/10 iteration=614/34336 train_loss=1.1099 took 0.3496 secs\n",
      "epoch=1/10 iteration=615/34336 train_loss=1.0978 took 0.6922 secs\n",
      "epoch=1/10 iteration=616/34336 train_loss=1.0991 took 0.4140 secs\n",
      "epoch=1/10 iteration=617/34336 train_loss=1.0989 took 0.3508 secs\n",
      "epoch=1/10 iteration=618/34336 train_loss=1.1035 took 0.4165 secs\n",
      "epoch=1/10 iteration=619/34336 train_loss=1.1058 took 0.4152 secs\n",
      "epoch=1/10 iteration=620/34336 train_loss=1.0992 took 0.3510 secs\n",
      "epoch=1/10 iteration=621/34336 train_loss=1.0959 took 0.3195 secs\n",
      "epoch=1/10 iteration=622/34336 train_loss=1.0982 took 0.3829 secs\n",
      "epoch=1/10 iteration=623/34336 train_loss=1.1050 took 0.6140 secs\n",
      "epoch=1/10 iteration=624/34336 train_loss=1.0987 took 0.6260 secs\n",
      "epoch=1/10 iteration=625/34336 train_loss=1.0966 took 0.3622 secs\n",
      "epoch=1/10 iteration=626/34336 train_loss=1.0983 took 0.2620 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1/10 iteration=627/34336 train_loss=1.1021 took 0.3714 secs\n",
      "epoch=1/10 iteration=628/34336 train_loss=1.0923 took 0.3818 secs\n",
      "epoch=1/10 iteration=629/34336 train_loss=1.1048 took 0.3221 secs\n",
      "epoch=1/10 iteration=630/34336 train_loss=1.1056 took 0.3449 secs\n",
      "epoch=1/10 iteration=631/34336 train_loss=1.1040 took 0.3434 secs\n",
      "epoch=1/10 iteration=632/34336 train_loss=1.1053 took 0.3193 secs\n",
      "epoch=1/10 iteration=633/34336 train_loss=1.0978 took 0.4865 secs\n",
      "epoch=1/10 iteration=634/34336 train_loss=1.0998 took 0.5117 secs\n",
      "epoch=1/10 iteration=635/34336 train_loss=1.0987 took 0.6334 secs\n",
      "epoch=1/10 iteration=636/34336 train_loss=1.0984 took 0.4702 secs\n",
      "epoch=1/10 iteration=637/34336 train_loss=1.1033 took 0.4426 secs\n",
      "epoch=1/10 iteration=638/34336 train_loss=1.0990 took 0.4426 secs\n",
      "epoch=1/10 iteration=639/34336 train_loss=1.0990 took 0.4420 secs\n",
      "epoch=1/10 iteration=640/34336 train_loss=1.0969 took 0.3982 secs\n",
      "epoch=1/10 iteration=641/34336 train_loss=1.1014 took 0.4450 secs\n",
      "epoch=1/10 iteration=642/34336 train_loss=1.0993 took 0.3504 secs\n",
      "epoch=1/10 iteration=643/34336 train_loss=1.0991 took 0.3256 secs\n",
      "epoch=1/10 iteration=644/34336 train_loss=1.1036 took 0.4988 secs\n",
      "epoch=1/10 iteration=645/34336 train_loss=1.0975 took 0.5179 secs\n",
      "epoch=1/10 iteration=646/34336 train_loss=1.1012 took 0.3558 secs\n",
      "epoch=1/10 iteration=647/34336 train_loss=1.0994 took 0.4346 secs\n",
      "epoch=1/10 iteration=648/34336 train_loss=1.1008 took 0.3622 secs\n",
      "epoch=1/10 iteration=649/34336 train_loss=1.0932 took 0.4319 secs\n",
      "epoch=1/10 iteration=650/34336 train_loss=1.1186 took 0.4479 secs\n",
      "epoch=1/10 iteration=651/34336 train_loss=1.1117 took 0.4104 secs\n",
      "epoch=1/10 iteration=652/34336 train_loss=1.0905 took 0.4293 secs\n",
      "epoch=1/10 iteration=653/34336 train_loss=1.1059 took 0.4514 secs\n",
      "epoch=1/10 iteration=654/34336 train_loss=1.1038 took 0.3720 secs\n",
      "epoch=1/10 iteration=655/34336 train_loss=1.1070 took 0.3573 secs\n",
      "epoch=1/10 iteration=656/34336 train_loss=1.1015 took 0.4538 secs\n",
      "epoch=1/10 iteration=657/34336 train_loss=1.0953 took 0.4529 secs\n",
      "epoch=1/10 iteration=658/34336 train_loss=1.1140 took 0.4927 secs\n",
      "epoch=1/10 iteration=659/34336 train_loss=1.1046 took 0.3879 secs\n",
      "epoch=1/10 iteration=660/34336 train_loss=1.1020 took 0.3329 secs\n",
      "epoch=1/10 iteration=661/34336 train_loss=1.1000 took 0.3200 secs\n",
      "epoch=1/10 iteration=662/34336 train_loss=1.1014 took 0.3039 secs\n",
      "epoch=1/10 iteration=663/34336 train_loss=1.0957 took 0.3517 secs\n",
      "epoch=1/10 iteration=664/34336 train_loss=1.0961 took 0.3564 secs\n",
      "epoch=1/10 iteration=665/34336 train_loss=1.1071 took 0.4163 secs\n",
      "epoch=1/10 iteration=666/34336 train_loss=1.1050 took 0.4208 secs\n",
      "epoch=1/10 iteration=667/34336 train_loss=1.0950 took 0.4943 secs\n",
      "epoch=1/10 iteration=668/34336 train_loss=1.1022 took 0.3687 secs\n",
      "epoch=1/10 iteration=669/34336 train_loss=1.1006 took 0.3804 secs\n",
      "epoch=1/10 iteration=670/34336 train_loss=1.0990 took 0.3792 secs\n",
      "epoch=1/10 iteration=671/34336 train_loss=1.1005 took 0.6116 secs\n",
      "epoch=1/10 iteration=672/34336 train_loss=1.0970 took 0.3878 secs\n",
      "epoch=1/10 iteration=673/34336 train_loss=1.1011 took 0.3173 secs\n",
      "epoch=1/10 iteration=674/34336 train_loss=1.1051 took 0.3427 secs\n",
      "epoch=1/10 iteration=675/34336 train_loss=1.1041 took 0.4132 secs\n",
      "epoch=1/10 iteration=676/34336 train_loss=1.0932 took 0.5229 secs\n",
      "epoch=1/10 iteration=677/34336 train_loss=1.1005 took 0.4582 secs\n",
      "epoch=1/10 iteration=678/34336 train_loss=1.0950 took 0.4681 secs\n",
      "epoch=1/10 iteration=679/34336 train_loss=1.1214 took 0.3810 secs\n",
      "epoch=1/10 iteration=680/34336 train_loss=1.0906 took 0.4177 secs\n",
      "epoch=1/10 iteration=681/34336 train_loss=1.0995 took 0.3969 secs\n",
      "epoch=1/10 iteration=682/34336 train_loss=1.0974 took 0.3977 secs\n",
      "epoch=1/10 iteration=683/34336 train_loss=1.1187 took 0.4051 secs\n",
      "epoch=1/10 iteration=684/34336 train_loss=1.1194 took 0.3722 secs\n",
      "epoch=1/10 iteration=685/34336 train_loss=1.0897 took 0.3303 secs\n",
      "epoch=1/10 iteration=686/34336 train_loss=1.0976 took 0.3659 secs\n",
      "epoch=1/10 iteration=687/34336 train_loss=1.1084 took 0.3801 secs\n",
      "epoch=1/10 iteration=688/34336 train_loss=1.1045 took 0.3014 secs\n",
      "epoch=1/10 iteration=689/34336 train_loss=1.0953 took 0.4475 secs\n",
      "epoch=1/10 iteration=690/34336 train_loss=1.0938 took 0.4608 secs\n",
      "epoch=1/10 iteration=691/34336 train_loss=1.1117 took 0.4048 secs\n",
      "epoch=1/10 iteration=692/34336 train_loss=1.1020 took 0.2513 secs\n",
      "epoch=1/10 iteration=693/34336 train_loss=1.1011 took 0.2944 secs\n",
      "epoch=1/10 iteration=694/34336 train_loss=1.1087 took 0.3484 secs\n",
      "epoch=1/10 iteration=695/34336 train_loss=1.1056 took 0.6423 secs\n",
      "epoch=1/10 iteration=696/34336 train_loss=1.0996 took 0.3141 secs\n",
      "epoch=1/10 iteration=697/34336 train_loss=1.0999 took 0.3661 secs\n",
      "epoch=1/10 iteration=698/34336 train_loss=1.0985 took 0.3711 secs\n",
      "epoch=1/10 iteration=699/34336 train_loss=1.0982 took 0.6282 secs\n",
      "epoch=1/10 iteration=700/34336 train_loss=1.1042 took 0.4361 secs\n",
      "epoch=1/10 iteration=701/34336 train_loss=1.0980 took 0.4667 secs\n",
      "epoch=1/10 iteration=702/34336 train_loss=1.1062 took 0.4213 secs\n",
      "epoch=1/10 iteration=703/34336 train_loss=1.0975 took 0.4250 secs\n",
      "epoch=1/10 iteration=704/34336 train_loss=1.1018 took 0.3665 secs\n",
      "epoch=1/10 iteration=705/34336 train_loss=1.0972 took 0.4937 secs\n",
      "epoch=1/10 iteration=706/34336 train_loss=1.1047 took 0.5279 secs\n",
      "epoch=1/10 iteration=707/34336 train_loss=1.1003 took 0.3354 secs\n",
      "epoch=1/10 iteration=708/34336 train_loss=1.0971 took 0.4209 secs\n",
      "epoch=1/10 iteration=709/34336 train_loss=1.1108 took 0.4712 secs\n",
      "epoch=1/10 iteration=710/34336 train_loss=1.1052 took 0.3675 secs\n",
      "epoch=1/10 iteration=711/34336 train_loss=1.0957 took 0.4193 secs\n",
      "epoch=1/10 iteration=712/34336 train_loss=1.0991 took 0.4684 secs\n",
      "epoch=1/10 iteration=713/34336 train_loss=1.0967 took 0.3508 secs\n",
      "epoch=1/10 iteration=714/34336 train_loss=1.0993 took 0.3179 secs\n",
      "epoch=1/10 iteration=715/34336 train_loss=1.1001 took 0.3027 secs\n",
      "epoch=1/10 iteration=716/34336 train_loss=1.0990 took 0.5066 secs\n",
      "epoch=1/10 iteration=717/34336 train_loss=1.1036 took 0.3808 secs\n",
      "epoch=1/10 iteration=718/34336 train_loss=1.0980 took 0.4055 secs\n",
      "epoch=1/10 iteration=719/34336 train_loss=1.0985 took 0.6562 secs\n",
      "epoch=1/10 iteration=720/34336 train_loss=1.1020 took 0.4515 secs\n",
      "epoch=1/10 iteration=721/34336 train_loss=1.0986 took 0.3795 secs\n",
      "epoch=1/10 iteration=722/34336 train_loss=1.1008 took 0.4874 secs\n",
      "epoch=1/10 iteration=723/34336 train_loss=1.1017 took 0.5036 secs\n",
      "epoch=1/10 iteration=724/34336 train_loss=1.1002 took 0.4560 secs\n",
      "epoch=1/10 iteration=725/34336 train_loss=1.1021 took 0.4219 secs\n",
      "epoch=1/10 iteration=726/34336 train_loss=1.0987 took 0.3750 secs\n",
      "epoch=1/10 iteration=727/34336 train_loss=1.0996 took 0.3736 secs\n",
      "epoch=1/10 iteration=728/34336 train_loss=1.0965 took 0.3908 secs\n",
      "epoch=1/10 iteration=729/34336 train_loss=1.1114 took 0.4376 secs\n",
      "epoch=1/10 iteration=730/34336 train_loss=1.0916 took 0.4198 secs\n",
      "epoch=1/10 iteration=731/34336 train_loss=1.1128 took 0.4459 secs\n",
      "epoch=1/10 iteration=732/34336 train_loss=1.1065 took 0.3708 secs\n",
      "epoch=1/10 iteration=733/34336 train_loss=1.1006 took 0.3748 secs\n",
      "epoch=1/10 iteration=734/34336 train_loss=1.0968 took 0.4949 secs\n",
      "epoch=1/10 iteration=735/34336 train_loss=1.1032 took 0.4032 secs\n",
      "epoch=1/10 iteration=736/34336 train_loss=1.1025 took 0.3999 secs\n",
      "epoch=1/10 iteration=737/34336 train_loss=1.0987 took 0.3849 secs\n",
      "epoch=1/10 iteration=738/34336 train_loss=1.1144 took 0.3455 secs\n",
      "epoch=1/10 iteration=739/34336 train_loss=1.0944 took 0.5225 secs\n",
      "epoch=1/10 iteration=740/34336 train_loss=1.0959 took 0.6361 secs\n",
      "epoch=1/10 iteration=741/34336 train_loss=1.1043 took 0.4584 secs\n",
      "epoch=1/10 iteration=742/34336 train_loss=1.0974 took 0.3659 secs\n",
      "epoch=1/10 iteration=743/34336 train_loss=1.0996 took 0.3639 secs\n",
      "epoch=1/10 iteration=744/34336 train_loss=1.1104 took 0.4160 secs\n",
      "epoch=1/10 iteration=745/34336 train_loss=1.1098 took 0.5174 secs\n",
      "epoch=1/10 iteration=746/34336 train_loss=1.1109 took 0.5297 secs\n",
      "epoch=1/10 iteration=747/34336 train_loss=1.0982 took 0.5329 secs\n",
      "epoch=1/10 iteration=748/34336 train_loss=1.0990 took 0.4920 secs\n",
      "epoch=1/10 iteration=749/34336 train_loss=1.1011 took 0.4573 secs\n",
      "epoch=1/10 iteration=750/34336 train_loss=1.1020 took 0.3964 secs\n",
      "epoch=1/10 iteration=751/34336 train_loss=1.0955 took 0.4326 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1/10 iteration=752/34336 train_loss=1.1046 took 0.4105 secs\n",
      "epoch=1/10 iteration=753/34336 train_loss=1.1029 took 0.3185 secs\n",
      "epoch=1/10 iteration=754/34336 train_loss=1.0971 took 0.3870 secs\n",
      "epoch=1/10 iteration=755/34336 train_loss=1.0991 took 0.4435 secs\n",
      "epoch=1/10 iteration=756/34336 train_loss=1.0963 took 0.4850 secs\n",
      "epoch=1/10 iteration=757/34336 train_loss=1.0956 took 0.4180 secs\n",
      "epoch=1/10 iteration=758/34336 train_loss=1.1058 took 0.6471 secs\n",
      "epoch=1/10 iteration=759/34336 train_loss=1.1098 took 0.4610 secs\n",
      "epoch=1/10 iteration=760/34336 train_loss=1.0951 took 0.4518 secs\n",
      "epoch=1/10 iteration=761/34336 train_loss=1.1016 took 0.3948 secs\n",
      "epoch=1/10 iteration=762/34336 train_loss=1.1044 took 0.4513 secs\n",
      "epoch=1/10 iteration=763/34336 train_loss=1.1009 took 0.3583 secs\n",
      "epoch=1/10 iteration=764/34336 train_loss=1.1001 took 0.3250 secs\n",
      "epoch=1/10 iteration=765/34336 train_loss=1.1039 took 0.4266 secs\n",
      "epoch=1/10 iteration=766/34336 train_loss=1.1027 took 0.3304 secs\n",
      "epoch=1/10 iteration=767/34336 train_loss=1.1013 took 0.3555 secs\n",
      "epoch=1/10 iteration=768/34336 train_loss=1.1019 took 0.4835 secs\n",
      "epoch=1/10 iteration=769/34336 train_loss=1.0976 took 0.3339 secs\n",
      "epoch=1/10 iteration=770/34336 train_loss=1.1039 took 0.2796 secs\n",
      "epoch=1/10 iteration=771/34336 train_loss=1.0984 took 0.4883 secs\n",
      "epoch=1/10 iteration=772/34336 train_loss=1.0994 took 0.3775 secs\n",
      "epoch=1/10 iteration=773/34336 train_loss=1.1006 took 0.5632 secs\n",
      "epoch=1/10 iteration=774/34336 train_loss=1.0987 took 0.4357 secs\n",
      "epoch=1/10 iteration=775/34336 train_loss=1.0999 took 0.3664 secs\n",
      "epoch=1/10 iteration=776/34336 train_loss=1.0985 took 0.5600 secs\n",
      "epoch=1/10 iteration=777/34336 train_loss=1.1017 took 0.5920 secs\n",
      "epoch=1/10 iteration=778/34336 train_loss=1.1160 took 0.5340 secs\n",
      "epoch=1/10 iteration=779/34336 train_loss=1.0903 took 0.4022 secs\n",
      "epoch=1/10 iteration=780/34336 train_loss=1.1061 took 0.5104 secs\n",
      "epoch=1/10 iteration=781/34336 train_loss=1.1061 took 0.3635 secs\n",
      "epoch=1/10 iteration=782/34336 train_loss=1.1003 took 0.2926 secs\n",
      "epoch=1/10 iteration=783/34336 train_loss=1.1033 took 0.4426 secs\n",
      "epoch=1/10 iteration=784/34336 train_loss=1.1045 took 0.6031 secs\n",
      "epoch=1/10 iteration=785/34336 train_loss=1.1002 took 0.3462 secs\n",
      "epoch=1/10 iteration=786/34336 train_loss=1.0958 took 0.3075 secs\n",
      "epoch=1/10 iteration=787/34336 train_loss=1.1049 took 0.3535 secs\n",
      "epoch=1/10 iteration=788/34336 train_loss=1.0948 took 0.3964 secs\n",
      "epoch=1/10 iteration=789/34336 train_loss=1.1026 took 0.3035 secs\n",
      "epoch=1/10 iteration=790/34336 train_loss=1.0997 took 0.3532 secs\n",
      "epoch=1/10 iteration=791/34336 train_loss=1.0991 took 0.4662 secs\n",
      "epoch=1/10 iteration=792/34336 train_loss=1.1014 took 0.4391 secs\n",
      "epoch=1/10 iteration=793/34336 train_loss=1.0959 took 0.5977 secs\n",
      "epoch=1/10 iteration=794/34336 train_loss=1.1029 took 0.3495 secs\n",
      "epoch=1/10 iteration=795/34336 train_loss=1.0985 took 0.4127 secs\n",
      "epoch=1/10 iteration=796/34336 train_loss=1.1030 took 0.3424 secs\n",
      "epoch=1/10 iteration=797/34336 train_loss=1.1010 took 0.3604 secs\n",
      "epoch=1/10 iteration=798/34336 train_loss=1.1028 took 0.3662 secs\n",
      "epoch=1/10 iteration=799/34336 train_loss=1.0954 took 0.3914 secs\n",
      "epoch=1/10 iteration=800/34336 train_loss=1.1016 took 0.3669 secs\n",
      "epoch=1/10 iteration=801/34336 train_loss=1.0982 took 0.3133 secs\n",
      "epoch=1/10 iteration=802/34336 train_loss=1.1027 took 0.4699 secs\n",
      "epoch=1/10 iteration=803/34336 train_loss=1.1009 took 0.3808 secs\n",
      "epoch=1/10 iteration=804/34336 train_loss=1.0994 took 0.4174 secs\n",
      "epoch=1/10 iteration=805/34336 train_loss=1.1054 took 0.3603 secs\n",
      "epoch=1/10 iteration=806/34336 train_loss=1.0984 took 0.4215 secs\n",
      "epoch=1/10 iteration=807/34336 train_loss=1.0999 took 0.4105 secs\n",
      "epoch=1/10 iteration=808/34336 train_loss=1.0987 took 0.4264 secs\n",
      "epoch=1/10 iteration=809/34336 train_loss=1.1023 took 0.4116 secs\n",
      "epoch=1/10 iteration=810/34336 train_loss=1.1077 took 0.7411 secs\n",
      "epoch=1/10 iteration=811/34336 train_loss=1.0989 took 0.6933 secs\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# from torchsample.callbacks import EarlyStopping\n",
    "# callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "# model.set_callbacks(callbacks)\n",
    "\n",
    "\n",
    "n_epochs = 10\n",
    "n_batch = 16\n",
    "lr=0.001\n",
    "\n",
    "\n",
    "training_data = SNLI_Dataset(df_train)\n",
    "train_dataloader = DataLoader(training_data, batch_size=n_batch, shuffle=False)\n",
    "\n",
    "validation_data = SNLI_Dataset(df_val)\n",
    "val_dataloader = DataLoader(validation_data, batch_size=n_batch, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(sbert.parameters(), lr=lr, momentum=0.9)\n",
    "optimizer = optim.Adam(sbert.parameters(), lr=lr)\n",
    "\n",
    "sbert.train()\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for i,((sent1,sent2),label) in enumerate(train_dataloader):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = sbert(sent1,sent2,objective=\"classification\")\n",
    "        loss = criterion(output, label)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "#         (sent1_val,sent2_val),label_val = next(iter(val_dataloader))\n",
    "#         output_val = sbert(sent1_val,sent2_val,\"classification\")\n",
    "#         val_loss = criterion(output_val,label_val)\n",
    "#         val_losses.append(val_loss)\n",
    "        \n",
    "        message = \"epoch={}/{} iteration={}/{} train_loss={:.4f} took {:.4f} secs\" \\\n",
    "            .format(epoch+1,n_epochs,i+1,len(train_dataloader),loss.detach().numpy(),time.time()-start)\n",
    "        \n",
    "        print(message)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_102496/2642549563.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for idx in range(5):\n",
    "    pred = torch.argmax(sbert(df_train.iloc[idx].sentence1,df_train.iloc[idx].sentence2,\"classification\")).numpy().item()\n",
    "    true = df_train.iloc[idx]['gold_label']\n",
    "    print(pred==true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"models/classification.pt\"\n",
    "torch.save(sbert.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fine-tuning using the regression objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pooling): AvgPool1d(kernel_size=(3,), stride=(1,), padding=(0,))\n",
       "  (linear): Linear(in_features=2298, out_features=3, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"models/classification.pt\"\n",
    "\n",
    "sbert = SBERT()\n",
    "sbert.load_state_dict(torch.load(PATH))\n",
    "sbert.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.80</td>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.80</td>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.60</td>\n",
       "      <td>Three men are playing chess.</td>\n",
       "      <td>Two men are playing chess.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.25</td>\n",
       "      <td>A man is playing the cello.</td>\n",
       "      <td>A man seated is playing the cello.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.25</td>\n",
       "      <td>Some men are fighting.</td>\n",
       "      <td>Two men are fighting.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                      sentence1  \\\n",
       "0   3.80                A man is playing a large flute.   \n",
       "1   3.80  A man is spreading shreded cheese on a pizza.   \n",
       "2   2.60                   Three men are playing chess.   \n",
       "3   4.25                    A man is playing the cello.   \n",
       "4   4.25                         Some men are fighting.   \n",
       "\n",
       "                                           sentence2  \n",
       "0                          A man is playing a flute.  \n",
       "1  A man is spreading shredded cheese on an uncoo...  \n",
       "2                         Two men are playing chess.  \n",
       "3                 A man seated is playing the cello.  \n",
       "4                              Two men are fighting.  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"Stsbenchmark/sts-train.csv\",header=0,names=[\"main-caption\",\"genre\",\"filename\",\"year\",\"score\",\"sentence1\",\"sentence2\"])#,usecols=['score','sentence1','sentence2'])\n",
    "df_test = pd.read_csv(\"Stsbenchmark/sts-test.csv\",header=0,names=[\"main-caption\",\"genre\",\"filename\",\"year\",\"score\",\"sentence1\",\"sentence2\"])#,usecols=['score','sentence1','sentence2'])\n",
    "\n",
    "df_train = df_train[['score','sentence1','sentence2']]\n",
    "df_test = df_test[['score','sentence1','sentence2']]\n",
    "\n",
    "df_train.head()\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 5.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = list(df_train.score)\n",
    "minn = min(ls)\n",
    "maxx = max(ls)\n",
    "(minn,maxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_score(value, leftMin=0, leftMax=5, rightMin=-1, rightMax=1):\n",
    "    # Figure out how 'wide' each range is\n",
    "    leftSpan = leftMax - leftMin\n",
    "    rightSpan = rightMax - rightMin\n",
    "\n",
    "    # Convert the left range into a 0-1 range (float)\n",
    "    valueScaled = float(value - leftMin) / float(leftSpan)\n",
    "\n",
    "    # Convert the 0-1 range into a value in the right range.\n",
    "    return rightMin + (valueScaled * rightSpan)\n",
    "\n",
    "df_train['score'] = df_train['score'].apply(map_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, 1.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = list(df_train.score)\n",
    "minn = min(ls)\n",
    "maxx = max(ls)\n",
    "(minn,maxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "class STS_Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        sent1 = row['sentence1']\n",
    "        sent2 = row['sentence2']\n",
    "        label = row['score']\n",
    "        return (sent1, sent2), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6167], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert(\"I love pizza\",\"I love burger\",objective=\"regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(sbert.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_regression = STS_Dataset(df_train)\n",
    "train_dataloader_regression = DataLoader(training_data_regression, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 iteration=1/62 loss=0.3414393365383148\n",
      "epoch=1 iteration=2/62 loss=0.5225234627723694\n",
      "epoch=1 iteration=3/62 loss=0.5447043776512146\n",
      "epoch=1 iteration=4/62 loss=0.9775974750518799\n",
      "epoch=1 iteration=5/62 loss=0.8285077214241028\n",
      "epoch=1 iteration=6/62 loss=0.9737005829811096\n",
      "epoch=1 iteration=7/62 loss=0.7615697383880615\n",
      "epoch=1 iteration=8/62 loss=0.7637732625007629\n",
      "epoch=1 iteration=9/62 loss=0.27583539485931396\n",
      "epoch=1 iteration=10/62 loss=0.4577804207801819\n",
      "epoch=1 iteration=11/62 loss=0.42774084210395813\n",
      "epoch=1 iteration=12/62 loss=0.6824129223823547\n",
      "epoch=1 iteration=13/62 loss=0.5674241185188293\n",
      "epoch=1 iteration=14/62 loss=0.6364807486534119\n",
      "epoch=1 iteration=15/62 loss=0.7143394947052002\n",
      "epoch=1 iteration=16/62 loss=0.5169401168823242\n",
      "epoch=1 iteration=17/62 loss=0.39554280042648315\n",
      "epoch=1 iteration=18/62 loss=0.48140937089920044\n",
      "epoch=1 iteration=19/62 loss=0.4177089333534241\n",
      "epoch=1 iteration=20/62 loss=0.4531996548175812\n",
      "epoch=1 iteration=21/62 loss=0.3222440481185913\n",
      "epoch=1 iteration=22/62 loss=0.33722788095474243\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "\n",
    "losses = []\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for i,((sent1,sent2),label) in enumerate(train_dataloader_regression):\n",
    "        \n",
    "        output = sbert(sent1,sent2,objective=\"regression\")\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        label = label.float()\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "        print(f\"epoch={epoch+1} iteration={i+1}/{len(train_dataloader_regression)} loss={loss.detach().numpy()}\")\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # break\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"models/classification_regression.pt\"\n",
    "torch.save(sbert.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use your best fine-tuned model and create a small semantic search system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pooling): AvgPool1d(kernel_size=(3,), stride=(1,), padding=(0,))\n",
       "  (linear): Linear(in_features=2298, out_features=3, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"models/classification_regression.pt\"\n",
    "sbert = SBERT()\n",
    "sbert.load_state_dict(torch.load(PATH))\n",
    "sbert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "list_ = []\n",
    "path = \"datasets/News_Category_Dataset_v2.json\"\n",
    "with open(path) as files:\n",
    "    for file in files:\n",
    "        list_.append(json.loads(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                           headline  \\\n",
       "0          CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1  ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2  ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3  ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4  ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "\n",
       "           authors                                               link  \\\n",
       "0  Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1    Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2       Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3       Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4       Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "\n",
       "                                   short_description        date  \n",
       "0  She left her husband. He killed their children...  2018-05-26  \n",
       "1                           Of course it has a song.  2018-05-26  \n",
       "2  The actor and his longtime girlfriend Anna Ebe...  2018-05-26  \n",
       "3  The actor gives Dems an ass-kicking for not fi...  2018-05-26  \n",
       "4  The \"Dietland\" actress said using the bags is ...  2018-05-26  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_news = pd.DataFrame(list_)\n",
    "df_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before splitting: 200853\n",
      "Length after splitting: 215043\n",
      "Data increased by 14190 sentences\n"
     ]
    }
   ],
   "source": [
    "sent1 = list(df_news.headline)\n",
    "print(\"Length before splitting:\",len(sent1))\n",
    "sent2 = [x for sentence in sent1 for x in sentence.split(\".\")]\n",
    "sent2 = [x for x in sent2 if x]\n",
    "print(\"Length after splitting:\",len(sent2))\n",
    "print(f\"Data increased by {len(sent2)-len(sent1)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV',\n",
       " \"Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song\",\n",
       " 'Hugh Grant Marries For The First Time At Age 57',\n",
       " \"Jim Carrey Blasts 'Castrato' Adam Schiff And Democrats In New Artwork\",\n",
       " 'Julianna Margulies Uses Donald Trump Poop Bags To Pick Up After Her Dog']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"Sweden is a very safe place.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(sentences):\n",
    "    embeddings = []\n",
    "    for i,sentence in enumerate(sentences):\n",
    "        print(f\"Finished iteration {i}/{len(sentences)}\",end=\"\\r\")\n",
    "        \n",
    "        embeddings.append(sbert(sentence,objective=\"embedding\"))\n",
    "\n",
    "    embeddings = torch.FloatTensor(embeddings)\n",
    "    torch.save(x, 'datasets/embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished iteration 1061/215043\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_89163/908439655.py\", line 1, in <module>\n",
      "    create_embeddings(sent2)\n",
      "  File \"/tmp/ipykernel_89163/686597705.py\", line 6, in create_embeddings\n",
      "    embeddings.append(sbert(sentence,objective=\"embedding\"))\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tmp/ipykernel_89163/255337989.py\", line 37, in forward\n",
      "    output1 = self.model(**encoded_input1)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 991, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 582, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 510, in forward\n",
      "    layer_output = apply_chunking_to_forward(\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 2186, in apply_chunking_to_forward\n",
      "    return forward_fn(*input_tensors)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 523, in feed_forward_chunk\n",
      "    layer_output = self.output(intermediate_output, attention_output)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 438, in forward\n",
      "    hidden_states = self.dense(hidden_states)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 103, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py\", line 1848, in linear\n",
      "    return torch._C._nn.linear(input, weight, bias)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 751, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 721, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/posixpath.py\", line 381, in abspath\n",
      "    return normpath(path)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/posixpath.py\", line 355, in normpath\n",
      "    comps = path.split(sep)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_89163/908439655.py\", line 1, in <module>\n",
      "    create_embeddings(sent2)\n",
      "  File \"/tmp/ipykernel_89163/686597705.py\", line 6, in create_embeddings\n",
      "    embeddings.append(sbert(sentence,objective=\"embedding\"))\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tmp/ipykernel_89163/255337989.py\", line 37, in forward\n",
      "    output1 = self.model(**encoded_input1)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 991, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 582, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 510, in forward\n",
      "    layer_output = apply_chunking_to_forward(\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 2186, in apply_chunking_to_forward\n",
      "    return forward_fn(*input_tensors)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 523, in feed_forward_chunk\n",
      "    layer_output = self.output(intermediate_output, attention_output)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 438, in forward\n",
      "    hidden_states = self.dense(hidden_states)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 103, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py\", line 1848, in linear\n",
      "    return torch._C._nn.linear(input, weight, bias)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3461, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2066, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/posixpath.py\", line 411, in _joinrealpath\n",
      "    name, _, rest = rest.partition(sep)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_89163/908439655.py\", line 1, in <module>\n",
      "    create_embeddings(sent2)\n",
      "  File \"/tmp/ipykernel_89163/686597705.py\", line 6, in create_embeddings\n",
      "    embeddings.append(sbert(sentence,objective=\"embedding\"))\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/tmp/ipykernel_89163/255337989.py\", line 37, in forward\n",
      "    output1 = self.model(**encoded_input1)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 991, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 582, in forward\n",
      "    layer_outputs = layer_module(\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 510, in forward\n",
      "    layer_output = apply_chunking_to_forward(\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/modeling_utils.py\", line 2186, in apply_chunking_to_forward\n",
      "    return forward_fn(*input_tensors)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 523, in feed_forward_chunk\n",
      "    layer_output = self.output(intermediate_output, attention_output)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py\", line 438, in forward\n",
      "    hidden_states = self.dense(hidden_states)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 103, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/functional.py\", line 1848, in linear\n",
      "    return torch._C._nn.linear(input, weight, bias)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3461, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2066, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2947, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3172, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3383, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2066, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
      "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 751, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 720, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 705, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/ali/anaconda3/envs/py38/lib/python3.8/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "create_embeddings(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
